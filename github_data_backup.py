#!/usr/bin/env python3
"""
è­¦å‘Šæ­·å²æ•¸æ“š GitHub è‡ªå‹•å‚™ä»½ç³»çµ±
è‡ªå‹•å°‡ SQLite æ•¸æ“šåº«å‚™ä»½åˆ° GitHub repository
"""

import os
import json
import sqlite3
import subprocess
from datetime import datetime
import shutil

class GitHubDataBackup:
    def __init__(self, db_path="warning_history.db"):
        self.db_path = db_path
        self.backup_dir = "data_backups"
        self.ensure_backup_directory()
        
    def ensure_backup_directory(self):
        """ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨"""
        if not os.path.exists(self.backup_dir):
            os.makedirs(self.backup_dir)
            
    def export_to_json(self):
        """å°‡ SQLite æ•¸æ“šåº«å°å‡ºç‚º JSON æ ¼å¼"""
        if not os.path.exists(self.db_path):
            print("âŒ è­¦å‘Šæ­·å²æ•¸æ“šåº«ä¸å­˜åœ¨")
            return None
            
        try:
            conn = sqlite3.connect(self.db_path)
            
            # å°å‡ºæ‰€æœ‰è¡¨çš„æ•¸æ“š
            tables = ['warning_records', 'prediction_records', 'warning_impact_analysis']
            backup_data = {
                'backup_timestamp': datetime.now().isoformat(),
                'database_info': {
                    'file_size_kb': os.path.getsize(self.db_path) / 1024,
                    'tables': tables
                },
                'data': {}
            }
            
            for table in tables:
                cursor = conn.cursor()
                cursor.execute(f"SELECT * FROM {table}")
                rows = cursor.fetchall()
                
                # ç²å–åˆ—å
                cursor.execute(f"PRAGMA table_info({table})")
                columns = [column[1] for column in cursor.fetchall()]
                
                # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
                table_data = []
                for row in rows:
                    row_dict = dict(zip(columns, row))
                    table_data.append(row_dict)
                    
                backup_data['data'][table] = {
                    'count': len(table_data),
                    'columns': columns,
                    'records': table_data
                }
                
            conn.close()
            
            # ä¿å­˜ JSON æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_filename = f"{self.backup_dir}/warning_history_backup_{timestamp}.json"
            
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… æ•¸æ“šæˆåŠŸå°å‡ºåˆ°: {json_filename}")
            return json_filename
            
        except Exception as e:
            print(f"âŒ å°å‡ºæ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def copy_database(self):
        """è¤‡è£½æ•¸æ“šåº«æ–‡ä»¶åˆ°å‚™ä»½ç›®éŒ„"""
        if not os.path.exists(self.db_path):
            print("âŒ è­¦å‘Šæ­·å²æ•¸æ“šåº«ä¸å­˜åœ¨")
            return None
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"{self.backup_dir}/warning_history_{timestamp}.db"
        
        try:
            shutil.copy2(self.db_path, backup_filename)
            print(f"âœ… æ•¸æ“šåº«å·²å‚™ä»½åˆ°: {backup_filename}")
            return backup_filename
        except Exception as e:
            print(f"âŒ å‚™ä»½æ•¸æ“šåº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def create_summary_report(self):
        """å‰µå»ºæ•¸æ“šæ‘˜è¦å ±å‘Š"""
        if not os.path.exists(self.db_path):
            return None
            
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # çµ±è¨ˆæ•¸æ“š
            cursor.execute("SELECT COUNT(*) FROM warning_records")
            warning_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM prediction_records") 
            prediction_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT category, COUNT(*) FROM warning_records GROUP BY category")
            category_stats = dict(cursor.fetchall())
            
            cursor.execute("SELECT severity, COUNT(*) FROM warning_records GROUP BY severity")
            severity_stats = dict(cursor.fetchall())
            
            cursor.execute("SELECT MIN(created_at), MAX(created_at) FROM warning_records")
            date_range = cursor.fetchone()
            
            conn.close()
            
            # å‰µå»ºå ±å‘Š
            report = {
                'generated_at': datetime.now().isoformat(),
                'database_summary': {
                    'total_warnings': warning_count,
                    'total_predictions': prediction_count,
                    'date_range': {
                        'first_record': date_range[0],
                        'last_record': date_range[1]
                    },
                    'category_distribution': category_stats,
                    'severity_distribution': severity_stats
                }
            }
            
            # ä¿å­˜å ±å‘Š
            report_filename = f"{self.backup_dir}/data_summary.json"
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… æ•¸æ“šæ‘˜è¦å ±å‘Šå·²ç”Ÿæˆ: {report_filename}")
            return report_filename
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ‘˜è¦å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def git_add_and_commit(self, message=None):
        """å°‡å‚™ä»½æ–‡ä»¶æ·»åŠ åˆ° git ä¸¦æäº¤"""
        try:
            if message is None:
                message = f"è‡ªå‹•å‚™ä»½è­¦å‘Šæ­·å²æ•¸æ“š - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            
            # æ·»åŠ å‚™ä»½ç›®éŒ„åˆ° git
            subprocess.run(['git', 'add', self.backup_dir], check=True)
            
            # æäº¤è®Šæ›´
            subprocess.run(['git', 'commit', '-m', message], check=True)
            
            print(f"âœ… Git æäº¤æˆåŠŸ: {message}")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Git æ“ä½œå¤±æ•—: {e}")
            return False
    
    def push_to_github(self):
        """æ¨é€åˆ° GitHub"""
        try:
            subprocess.run(['git', 'push', 'origin', 'main'], check=True)
            print("âœ… æˆåŠŸæ¨é€åˆ° GitHub!")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ æ¨é€åˆ° GitHub å¤±æ•—: {e}")
            return False
    
    def full_backup(self):
        """åŸ·è¡Œå®Œæ•´å‚™ä»½æµç¨‹"""
        print("ğŸš€ é–‹å§‹åŸ·è¡Œè­¦å‘Šæ­·å²æ•¸æ“š GitHub å‚™ä»½...")
        
        # 1. å°å‡º JSON æ ¼å¼
        json_file = self.export_to_json()
        
        # 2. è¤‡è£½æ•¸æ“šåº«æ–‡ä»¶
        db_file = self.copy_database()
        
        # 3. å‰µå»ºæ‘˜è¦å ±å‘Š
        summary_file = self.create_summary_report()
        
        if json_file or db_file or summary_file:
            # 4. Git æäº¤
            if self.git_add_and_commit():
                # 5. æ¨é€åˆ° GitHub
                self.push_to_github()
                print("ğŸ‰ å‚™ä»½å®Œæˆï¼æ•¸æ“šå·²ä¿å­˜åˆ° GitHub")
            else:
                print("âš ï¸ å‚™ä»½æ–‡ä»¶å·²å‰µå»ºï¼Œä½† Git æäº¤å¤±æ•—")
        else:
            print("âŒ å‚™ä»½å¤±æ•—ï¼Œæ²’æœ‰æ–‡ä»¶è¢«å‰µå»º")

def main():
    """ä¸»å‡½æ•¸"""
    backup_system = GitHubDataBackup()
    backup_system.full_backup()

if __name__ == "__main__":
    main()
