from flask import Flask, jsonify, render_template, request, send_from_directory, redirect
from flask_caching import Cache
from flask_cors import CORS
from hko_fetcher import fetch_weather_data, fetch_forecast_data, fetch_ninday_forecast, get_current_wind_data, fetch_warning_data
from unified_scorer import calculate_burnsky_score_unified
from forecast_extractor import forecast_extractor
from hko_webcam_fetcher import RealTimeWebcamMonitor, HKOWebcamFetcher, WebcamImageAnalyzer
from burnsky_case_analyzer import BurnskyCaseAnalyzer
import numpy as np
import os
import time
import schedule
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šé‡
load_dotenv()
import threading
from datetime import datetime, timedelta
from werkzeug.utils import secure_filename
import base64
import io
from PIL import Image
import uuid
import sqlite3
import json

# ========== æ¨¡å¡ŠåŒ–çµ„ä»¶å°å…¥ ==========
# å„ªå…ˆä½¿ç”¨æ¨¡å¡ŠåŒ–çµ„ä»¶ï¼Œå¦‚æœä¸å¯ç”¨å‰‡ä½¿ç”¨å…§åµŒå‡½æ•¸
try:
    from modules.config import (
        CACHE_DURATION, UPLOAD_FOLDER, ALLOWED_EXTENSIONS, 
        MAX_FILE_SIZE, AUTO_SAVE_PHOTOS, PHOTO_RETENTION_DAYS,
        PREDICTION_HISTORY_DB, HOURLY_SAVE_ENABLED,
        BURNSKY_PHOTO_CASES, LAST_CASE_UPDATE
    )
    from modules.cache import cache
    from modules.database import (
        init_prediction_history_db, save_prediction_to_history,
        get_season, get_time_category
    )
    from modules.cache import get_cached_data, clear_prediction_cache, trigger_prediction_update
    from modules.scheduler import auto_save_current_predictions, start_hourly_scheduler
    from modules.file_handler import (
        allowed_file, validate_image_content, cleanup_old_photos,
        save_uploaded_photo, get_photo_storage_info
    )
    from modules.utils import (
        convert_numpy_types, get_prediction_level,
        get_optimal_sunset_time, get_optimal_burnsky_time,
        get_historical_prediction_for_time, cross_check_photo_with_prediction
    )
    from modules.photo_analyzer import (
        analyze_photo_quality, record_burnsky_photo_case,
        analyze_photo_case_patterns, apply_burnsky_photo_corrections,
        is_similar_to_successful_cases, initialize_photo_cases
    )
    MODULES_LOADED = True
    print("âœ… æ¨¡å¡ŠåŒ–çµ„ä»¶å·²è¼‰å…¥")
except ImportError as e:
    print(f"âš ï¸ æ¨¡å¡ŠåŒ–çµ„ä»¶æœªå¯ç”¨ï¼Œä½¿ç”¨å…§åµŒå‡½æ•¸: {e}")
    MODULES_LOADED = False
    # å¦‚æœæ¨¡å¡Šä¸å¯ç”¨ï¼Œä¿ç•™åŸå§‹çš„è®Šæ•¸å®šç¾©
    cache = {}
    CACHE_DURATION = int(os.getenv('CACHE_DURATION', '300'))
    BURNSKY_PHOTO_CASES = {}
    LAST_CASE_UPDATE = None
    UPLOAD_FOLDER = os.getenv('UPLOAD_FOLDER', 'uploads')
    ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'webp'}
    MAX_FILE_SIZE = int(os.getenv('MAX_FILE_SIZE', str(16 * 1024 * 1024)))
    AUTO_SAVE_PHOTOS = os.getenv('AUTO_SAVE_PHOTOS', 'False').lower() == 'true'
    PHOTO_RETENTION_DAYS = int(os.getenv('PHOTO_RETENTION_DAYS', '30'))
    os.makedirs(UPLOAD_FOLDER, exist_ok=True)
    PREDICTION_HISTORY_DB = os.getenv('PREDICTION_HISTORY_DB', 'prediction_history.db')
    HOURLY_SAVE_ENABLED = os.getenv('HOURLY_SAVE_ENABLED', 'True').lower() == 'true'

# å³æ™‚æ”å½±æ©Ÿç›£æ§ç³»çµ±
webcam_monitor = RealTimeWebcamMonitor()

# ========== ä»¥ä¸‹æ˜¯åŸå§‹å‡½æ•¸å®šç¾©ï¼ˆä¿ç•™ç”¨æ–¼å‘å¾Œå…¼å®¹ï¼‰==========
# å¦‚æœæ¨¡å¡Šå·²è¼‰å…¥ï¼Œé€™äº›å‡½æ•¸å°‡è¢«æ¨¡å¡Šä¸­çš„ç‰ˆæœ¬è¦†è“‹

def init_prediction_history_db():
    """åˆå§‹åŒ–é æ¸¬æ­·å²æ•¸æ“šåº«"""
    conn = sqlite3.connect(PREDICTION_HISTORY_DB)
    cursor = conn.cursor()
    
    # å‰µå»ºé æ¸¬æ­·å²è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS prediction_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            prediction_type TEXT NOT NULL,
            advance_hours INTEGER,
            score REAL,
            factors TEXT,  -- JSONæ ¼å¼å„²å­˜æ‰€æœ‰å› å­
            weather_data TEXT,  -- JSONæ ¼å¼å„²å­˜å¤©æ°£æ•¸æ“š
            warnings TEXT,  -- JSONæ ¼å¼å„²å­˜è­¦å‘Šæ•¸æ“š
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # å‰µå»ºç´¢å¼•
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON prediction_history(timestamp)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_type ON prediction_history(prediction_type)')
    
    conn.commit()
    conn.close()
    print("ğŸ“Š é æ¸¬æ­·å²æ•¸æ“šåº«å·²åˆå§‹åŒ–")

def save_prediction_to_history(prediction_type, advance_hours, score, factors, weather_data, warnings):
    """ä¿å­˜é æ¸¬åˆ°æ­·å²æ•¸æ“šåº«"""
    try:
        conn = sqlite3.connect(PREDICTION_HISTORY_DB)
        cursor = conn.cursor()
        
        # å¢åŠ æ›´å¤šæ™‚é–“ç›¸é—œçš„å› å­
        enhanced_factors = factors.copy() if factors else {}
        current_time = datetime.now()
        
        # æ·»åŠ æ™‚é–“å› å­
        enhanced_factors.update({
            'time_factors': {
                'hour': current_time.hour,
                'day_of_week': current_time.weekday(),
                'day_of_month': current_time.day,
                'month': current_time.month,
                'season': get_season(current_time.month),
                'is_weekend': current_time.weekday() >= 5,
                'time_category': get_time_category(current_time.hour)
            },
            'weather_timing': {
                'prediction_datetime': current_time.isoformat(),
                'target_datetime': (current_time + timedelta(hours=advance_hours)).isoformat(),
                'advance_hours': advance_hours
            }
        })
        
        cursor.execute('''
            INSERT INTO prediction_history 
            (prediction_type, advance_hours, score, factors, weather_data, warnings)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            prediction_type,
            advance_hours,
            score,
            json.dumps(enhanced_factors, ensure_ascii=False),
            json.dumps(weather_data, ensure_ascii=False),
            json.dumps(warnings, ensure_ascii=False)
        ))
        
        conn.commit()
        conn.close()
        print(f"ğŸ’¾ å·²ä¿å­˜é æ¸¬æ­·å²: {prediction_type} (åˆ†æ•¸: {score:.1f}, {current_time.strftime('%H:%M')})")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜é æ¸¬æ­·å²å¤±æ•—: {e}")
        return False

def get_season(month):
    """æ ¹æ“šæœˆä»½åˆ¤æ–·å­£ç¯€"""
    if month in [12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5]:
        return 'spring'
    elif month in [6, 7, 8]:
        return 'summer'
    else:
        return 'autumn'

def get_time_category(hour):
    """æ ¹æ“šå°æ™‚åˆ¤æ–·æ™‚é–“é¡åˆ¥"""
    if 5 <= hour < 8:
        return 'early_morning'
    elif 8 <= hour < 12:
        return 'morning'
    elif 12 <= hour < 17:
        return 'afternoon'
    elif 17 <= hour < 20:
        return 'evening'
    elif 20 <= hour < 23:
        return 'night'
    else:
        return 'late_night'

def auto_save_current_predictions():
    """è‡ªå‹•ä¿å­˜ç•¶å‰æ™‚é–“çš„é æ¸¬"""
    try:
        print("ğŸ• é–‹å§‹è‡ªå‹•ä¿å­˜æ¯å°æ™‚é æ¸¬...")
        
        # æ¸…é™¤å¿«å–ç¢ºä¿ç²å–æœ€æ–°æ•¸æ“š
        global cache
        cache.clear()
        
        for prediction_type in ['sunset', 'sunrise']:
            for advance_hours in [0, 1, 2, 3, 6, 12]:
                try:
                    # é‡æ–°è¨ˆç®—é æ¸¬
                    result = predict_burnsky_core(prediction_type, advance_hours)
                    
                    if result.get('status') == 'success':
                        # ä¿å­˜åˆ°é æ¸¬æ­·å²æ•¸æ“šåº«
                        save_prediction_to_history(
                            prediction_type,
                            advance_hours,
                            result.get('burnsky_score', 0),
                            result.get('analysis_details', {}),
                            result.get('weather_data', {}),
                            result.get('warning_data', {})
                        )
                    
                    time.sleep(0.5)  # é¿å…è«‹æ±‚éå¿«
                    
                except Exception as e:
                    print(f"âŒ ä¿å­˜ {prediction_type} (æå‰{advance_hours}å°æ™‚) å¤±æ•—: {e}")
        
        print("âœ… æ¯å°æ™‚é æ¸¬ä¿å­˜å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ è‡ªå‹•ä¿å­˜é æ¸¬å¤±æ•—: {e}")

def get_historical_prediction_for_time(target_datetime, prediction_type, tolerance_hours=2):
    """ç²å–æŒ‡å®šæ™‚é–“é™„è¿‘çš„æ­·å²é æ¸¬æ•¸æ“š"""
    try:
        conn = sqlite3.connect(PREDICTION_HISTORY_DB)
        cursor = conn.cursor()
        
        # è¨ˆç®—æ™‚é–“ç¯„åœ
        start_time = target_datetime - timedelta(hours=tolerance_hours)
        end_time = target_datetime + timedelta(hours=tolerance_hours)
        
        cursor.execute('''
            SELECT timestamp, advance_hours, score, factors, weather_data
            FROM prediction_history 
            WHERE prediction_type = ? 
            AND timestamp BETWEEN ? AND ?
            ORDER BY ABS(julianday(?) - julianday(timestamp)) ASC
            LIMIT 5
        ''', (prediction_type, start_time, end_time, target_datetime))
        
        results = cursor.fetchall()
        conn.close()
        
        historical_data = []
        for row in results:
            historical_data.append({
                'timestamp': row[0],
                'advance_hours': row[1],
                'score': row[2],
                'factors': json.loads(row[3]) if row[3] else {},
                'weather_data': json.loads(row[4]) if row[4] else {}
            })
        
        return historical_data
    except Exception as e:
        print(f"âŒ ç²å–æ­·å²é æ¸¬å¤±æ•—: {e}")
        return []

def cross_check_photo_with_prediction(photo_datetime, photo_location, photo_quality, prediction_type='sunset'):
    """äº¤å‰æª¢æŸ¥ç…§ç‰‡èˆ‡æ­·å²é æ¸¬çš„æº–ç¢ºæ€§"""
    try:
        # è§£æç…§ç‰‡æ™‚é–“ - æ”¯æŒå¤šç¨®æ ¼å¼
        if isinstance(photo_datetime, str):
            # å˜—è©¦ä¸åŒçš„æ™‚é–“æ ¼å¼
            time_formats = [
                "%Y-%m-%d_%H-%M",  # "2025-07-27_19-10"
                "%Y-%m-%d %H:%M:%S",  # "2025-07-27 17:02:18"
                "%Y-%m-%dT%H:%M:%S",  # ISOæ ¼å¼
                "%Y-%m-%d %H:%M",  # "2025-07-27 17:02"
            ]
            
            photo_dt = None
            for fmt in time_formats:
                try:
                    photo_dt = datetime.strptime(photo_datetime, fmt)
                    break
                except ValueError:
                    continue
            
            if photo_dt is None:
                return {
                    'status': 'error',
                    'message': f'ç„¡æ³•è§£ææ™‚é–“æ ¼å¼: {photo_datetime}ã€‚æ”¯æŒæ ¼å¼: YYYY-MM-DD_HH-MM æˆ– YYYY-MM-DD HH:MM:SS'
                }
        else:
            photo_dt = photo_datetime
        
        # ç²å–è©²æ™‚é–“çš„æ­·å²é æ¸¬
        historical_predictions = get_historical_prediction_for_time(photo_dt, prediction_type)
        
        if not historical_predictions:
            return {
                'status': 'no_data',
                'message': 'è©²æ™‚é–“æ²’æœ‰æ­·å²é æ¸¬æ•¸æ“š',
                'photo_quality': photo_quality,
                'searched_time': photo_dt.isoformat(),
                'suggestion': 'éœ€è¦ç­‰å¾…ç³»çµ±ç´¯ç©æ›´å¤šé æ¸¬æ•¸æ“šå¾Œå†é€²è¡Œæ¯”è¼ƒ'
            }
        
        # åˆ†ææº–ç¢ºæ€§
        accuracy_analysis = []
        for pred in historical_predictions:
            predicted_score = pred['score']
            actual_quality = photo_quality * 10  # è½‰æ›ç‚º0-100åˆ†åˆ¶
            
            accuracy = 100 - abs(predicted_score - actual_quality)
            accuracy = max(0, accuracy)  # ç¢ºä¿ä¸ç‚ºè² æ•¸
            
            accuracy_analysis.append({
                'prediction_time': pred['timestamp'],
                'advance_hours': pred['advance_hours'],
                'predicted_score': predicted_score,
                'actual_quality': actual_quality,
                'accuracy_percentage': accuracy,
                'factors': pred['factors']
            })
        
        # è¨ˆç®—å¹³å‡æº–ç¢ºæ€§
        avg_accuracy = sum(a['accuracy_percentage'] for a in accuracy_analysis) / len(accuracy_analysis)
        
        # ç”Ÿæˆå»ºè­°
        best_prediction = max(accuracy_analysis, key=lambda x: x['accuracy_percentage'])
        worst_prediction = min(accuracy_analysis, key=lambda x: x['accuracy_percentage'])
        
        return {
            'status': 'success',
            'photo_datetime': photo_dt.isoformat(),
            'photo_location': photo_location,
            'photo_quality': photo_quality,
            'average_accuracy': avg_accuracy,
            'predictions_analyzed': len(accuracy_analysis),
            'best_prediction': best_prediction,
            'worst_prediction': worst_prediction,
            'all_predictions': accuracy_analysis,
            'improvement_suggestions': generate_accuracy_suggestions(accuracy_analysis)
        }
        
    except Exception as e:
        print(f"âŒ äº¤å‰æª¢æŸ¥å¤±æ•—: {e}")
        return {
            'status': 'error',
            'message': str(e)
        }

def generate_accuracy_suggestions(accuracy_analysis):
    """åŸºæ–¼æº–ç¢ºæ€§åˆ†æç”Ÿæˆæ”¹é€²å»ºè­°"""
    suggestions = []
    
    avg_accuracy = sum(a['accuracy_percentage'] for a in accuracy_analysis) / len(accuracy_analysis)
    
    if avg_accuracy < 60:
        suggestions.append("é æ¸¬æº–ç¢ºæ€§åä½ï¼Œå»ºè­°æª¢æŸ¥å¤©æ°£æ•¸æ“šæºå’Œç®—æ³•åƒæ•¸")
    elif avg_accuracy < 75:
        suggestions.append("é æ¸¬æº–ç¢ºæ€§ä¸­ç­‰ï¼Œå¯ä»¥å„ªåŒ–æ¬Šé‡åˆ†é…")
    else:
        suggestions.append("é æ¸¬æº–ç¢ºæ€§è‰¯å¥½ï¼Œç¹¼çºŒç¶­æŒç•¶å‰ç®—æ³•")
    
    # åˆ†ææå‰æ™‚é–“çš„å½±éŸ¿
    advance_accuracies = {}
    for a in accuracy_analysis:
        hours = a['advance_hours']
        if hours not in advance_accuracies:
            advance_accuracies[hours] = []
        advance_accuracies[hours].append(a['accuracy_percentage'])
    
    for hours, accuracies in advance_accuracies.items():
        avg_acc = sum(accuracies) / len(accuracies)
        if avg_acc < 60:
            suggestions.append(f"æå‰{hours}å°æ™‚çš„é æ¸¬æº–ç¢ºæ€§è¼ƒä½ ({avg_acc:.1f}%)")
    
    return suggestions

def start_hourly_scheduler():
    """å•Ÿå‹•æ¯å°æ™‚ä¿å­˜æ’ç¨‹"""
    if not HOURLY_SAVE_ENABLED:
        return
    
    # è¨­å®šæ¯å°æ™‚çš„ç¬¬5åˆ†é˜åŸ·è¡Œ
    schedule.every().hour.at(":05").do(auto_save_current_predictions)
    
    def run_scheduler():
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
    scheduler_thread.start()
    print("â° æ¯å°æ™‚é æ¸¬ä¿å­˜æ’ç¨‹å·²å•Ÿå‹•")

# åˆå§‹åŒ–é æ¸¬æ­·å²æ•¸æ“šåº«
if MODULES_LOADED:
    print("ğŸ”§ ä½¿ç”¨æ¨¡å¡ŠåŒ–çµ„ä»¶åˆå§‹åŒ–ç³»çµ±...")
    initialize_photo_cases()  # åˆå§‹åŒ–ç…§ç‰‡æ¡ˆä¾‹ç³»çµ±
    start_hourly_scheduler()  # å•Ÿå‹•èª¿åº¦å™¨
else:
    print("ğŸ”§ ä½¿ç”¨å…§åµŒå‡½æ•¸åˆå§‹åŒ–ç³»çµ±...")
    init_prediction_history_db()

# ä»¥ä¸‹å‡½æ•¸å®šç¾©ä¿ç•™ç”¨æ–¼å‘å¾Œå…¼å®¹ï¼ˆç•¶æ¨¡å¡Šæœªè¼‰å…¥æ™‚ï¼‰

def allowed_file(filename):
    """æª¢æŸ¥æª”æ¡ˆé¡å‹æ˜¯å¦å…è¨±"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def validate_image_content(image_data):
    """é©—è­‰æª”æ¡ˆç¢ºå¯¦æ˜¯åœ–ç‰‡"""
    try:
        image = Image.open(io.BytesIO(image_data))
        image.verify()  # é©—è­‰åœ–ç‰‡å®Œæ•´æ€§
        return True
    except Exception:
        return False

def cleanup_old_photos():
    """æ¸…ç†èˆŠç…§ç‰‡"""
    if not os.path.exists(UPLOAD_FOLDER):
        return
        
    cutoff_time = time.time() - (PHOTO_RETENTION_DAYS * 24 * 60 * 60)
    cleaned_count = 0
    
    for filename in os.listdir(UPLOAD_FOLDER):
        file_path = os.path.join(UPLOAD_FOLDER, filename)
        if os.path.isfile(file_path) and os.path.getmtime(file_path) < cutoff_time:
            try:
                os.remove(file_path)
                cleaned_count += 1
            except OSError:
                pass
    
    if cleaned_count > 0:
        print(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} å€‹èˆŠç…§ç‰‡")

def get_cached_data(key, fetch_function, *args):
    """ç²å–å¿«å–æ•¸æ“šæˆ–é‡æ–°ç²å–"""
    current_time = time.time()
    
    if key in cache:
        cached_time, cached_data = cache[key]
        if current_time - cached_time < CACHE_DURATION:
            print(f"âœ… ä½¿ç”¨å¿«å–: {key}")
            return cached_data
    
    print(f"ğŸ”„ é‡æ–°ç²å–: {key}")
    fresh_data = fetch_function(*args)
    cache[key] = (current_time, fresh_data)
    return fresh_data

def clear_prediction_cache():
    """æ¸…é™¤é æ¸¬ç›¸é—œå¿«å–"""
    global cache
    
    # æ¸…é™¤æ‰€æœ‰é æ¸¬å¿«å–
    keys_to_remove = [key for key in cache.keys() if 'prediction' in key or 'burnsky' in key]
    
    for key in keys_to_remove:
        cache.pop(key, None)
    
    if keys_to_remove:
        print(f"ğŸ”„ å·²æ¸…é™¤ {len(keys_to_remove)} å€‹é æ¸¬å¿«å–: {keys_to_remove}")
    
    return len(keys_to_remove)

def trigger_prediction_update():
    """è§¸ç™¼é æ¸¬æ›´æ–°ï¼ˆæ¸…é™¤å¿«å–ï¼Œå¼·åˆ¶é‡æ–°è¨ˆç®—ï¼‰"""
    global LAST_CASE_UPDATE
    
    # æ›´æ–°æ¡ˆä¾‹æ™‚é–“æˆ³
    LAST_CASE_UPDATE = time.time()
    
    # æ¸…é™¤ç›¸é—œå¿«å–
    cleared_count = clear_prediction_cache()
    
    print(f"ğŸš€ è§¸ç™¼é æ¸¬æ›´æ–° - æ¸…é™¤äº† {cleared_count} å€‹å¿«å–é …ç›®")
    return cleared_count

# è­¦å‘Šæ­·å²åˆ†æç³»çµ±
try:
    from warning_history_analyzer import WarningHistoryAnalyzer
    warning_analysis_available = True  # ä½¿ç”¨çœŸå¯¦æ•¸æ“š
    print("âœ… è­¦å‘Šæ­·å²åˆ†æç³»çµ±å·²è¼‰å…¥")
except ImportError as e:
    warning_analysis_available = False
    WarningHistoryAnalyzer = None
    print(f"âš ï¸ è­¦å‘Šæ­·å²åˆ†æç³»çµ±æœªå¯ç”¨: {e}")

# è­¦å‘Šæ•¸æ“šæ”¶é›†å™¨ï¼ˆå¯é¸çµ„ä»¶ï¼‰
try:
    from warning_data_collector import WarningDataCollector
except ImportError as e:
    WarningDataCollector = None
    print("âš ï¸ è­¦å‘Šæ•¸æ“šæ”¶é›†å™¨æœªå¯ç”¨ï¼ˆå¯é¸çµ„ä»¶ï¼‰")

app = Flask(__name__)

# é…ç½® Flask æ‡‰ç”¨
app.config['SECRET_KEY'] = os.getenv('SECRET_KEY', os.urandom(24).hex())
app.config['MAX_CONTENT_LENGTH'] = int(os.getenv('MAX_FILE_SIZE', str(16 * 1024 * 1024)))

# é…ç½®å¿«å–ç³»çµ±
app.config['CACHE_TYPE'] = os.getenv('CACHE_TYPE', 'SimpleCache')  # SimpleCache, RedisCache, FileSystemCache
app.config['CACHE_DEFAULT_TIMEOUT'] = int(os.getenv('CACHE_DEFAULT_TIMEOUT', '300'))  # 5åˆ†é˜
app.config['CACHE_REDIS_URL'] = os.getenv('REDIS_URL', None)  # Redisé€£æ¥URLï¼ˆå¯é¸ï¼‰
app.config['CACHE_DIR'] = os.getenv('CACHE_DIR', 'cache')  # æ–‡ä»¶ç³»çµ±å¿«å–ç›®éŒ„ï¼ˆå¯é¸ï¼‰

# åˆå§‹åŒ–å¿«å–
flask_cache = Cache(app)

# é…ç½® CORS (è·¨åŸŸè³‡æºå…±äº«)
cors_enabled = os.getenv('CORS_ENABLED', 'True').lower() == 'true'
if cors_enabled:
    cors_origins = os.getenv('CORS_ORIGINS', '*')  # å…è¨±çš„ä¾†æºï¼Œç”Ÿç”¢ç’°å¢ƒæ‡‰æŒ‡å®šå…·é«”åŸŸå
    CORS(app, resources={
        r"/api/*": {
            "origins": cors_origins if cors_origins != '*' else '*',
            "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
            "allow_headers": ["Content-Type", "Authorization"],
            "expose_headers": ["Content-Type", "X-RateLimit-Limit", "X-RateLimit-Remaining"],
            "supports_credentials": True,
            "max_age": 3600  # é æª¢è«‹æ±‚å¿«å–1å°æ™‚
        },
        r"/predict*": {
            "origins": cors_origins if cors_origins != '*' else '*',
            "methods": ["GET", "OPTIONS"],
            "allow_headers": ["Content-Type"],
            "max_age": 600  # é æª¢è«‹æ±‚å¿«å–10åˆ†é˜
        }
    })
    print(f"âœ… CORSå·²å•Ÿç”¨ - å…è¨±ä¾†æº: {cors_origins}")
else:
    print("âš ï¸ CORSå·²ç¦ç”¨")

# é…ç½®é€Ÿç‡é™åˆ¶
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

rate_limit_enabled = os.getenv('RATE_LIMIT_ENABLED', 'True').lower() == 'true'

if rate_limit_enabled:
    limiter = Limiter(
        app=app,
        key_func=get_remote_address,
        default_limits=[os.getenv('RATE_LIMIT_DEFAULT', '200 per hour, 50 per minute')],
        storage_uri=os.getenv('RATE_LIMIT_STORAGE', 'memory://'),
        strategy="fixed-window",
        headers_enabled=True  # å•Ÿç”¨é€Ÿç‡é™åˆ¶æ¨™é ­
    )
else:
    # å¦‚æœç¦ç”¨é€Ÿç‡é™åˆ¶ï¼Œå‰µå»ºä¸€å€‹ç©ºè£é£¾å™¨
    class NoOpLimiter:
        def limit(self, *args, **kwargs):
            def decorator(f):
                return f
            return decorator
    limiter = NoOpLimiter()

# ========== éŒ¯èª¤è™•ç† ==========
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime as dt

# é…ç½®æ—¥èªŒè¼ªè½‰
log_level = os.getenv('LOG_LEVEL', 'INFO')
log_file = os.getenv('LOG_FILE', 'app.log')
max_bytes = int(os.getenv('LOG_MAX_BYTES', str(10 * 1024 * 1024)))  # é»˜èª 10MB
backup_count = int(os.getenv('LOG_BACKUP_COUNT', '5'))  # é»˜èªä¿ç•™5å€‹å‚™ä»½

# å‰µå»ºæ—¥èªŒè™•ç†å™¨
file_handler = RotatingFileHandler(
    log_file,
    maxBytes=max_bytes,
    backupCount=backup_count,
    encoding='utf-8'
)
file_handler.setLevel(getattr(logging, log_level))
file_handler.setFormatter(logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
))

console_handler = logging.StreamHandler()
console_handler.setLevel(getattr(logging, log_level))
console_handler.setFormatter(logging.Formatter(
    '%(asctime)s - %(levelname)s - %(message)s'
))

# é…ç½®æ ¹æ—¥èªŒè¨˜éŒ„å™¨
logging.basicConfig(
    level=getattr(logging, log_level),
    handlers=[file_handler, console_handler]
)
logger = logging.getLogger(__name__)

def error_response(error_code, message, details=None):
    """çµ±ä¸€çš„éŒ¯èª¤éŸ¿æ‡‰æ ¼å¼"""
    response = {
        'error': True,
        'code': error_code,
        'message': message,
        'timestamp': dt.now().isoformat()
    }
    if details:
        response['details'] = details
    return jsonify(response), error_code

@app.errorhandler(400)
def bad_request(error):
    """400 éŒ¯èª¤ - è«‹æ±‚åƒæ•¸éŒ¯èª¤"""
    logger.warning(f"Bad Request: {error}")
    return error_response(400, 'è«‹æ±‚åƒæ•¸éŒ¯èª¤', str(error))

@app.errorhandler(404)
def not_found(error):
    """404 éŒ¯èª¤ - è³‡æºä¸å­˜åœ¨"""
    logger.info(f"Not Found: {request.path}")
    return error_response(404, 'è«‹æ±‚çš„è³‡æºä¸å­˜åœ¨', f'è·¯å¾‘: {request.path}')

@app.errorhandler(405)
def method_not_allowed(error):
    """405 éŒ¯èª¤ - æ–¹æ³•ä¸å…è¨±"""
    logger.warning(f"Method Not Allowed: {request.method} {request.path}")
    return error_response(405, 'HTTP æ–¹æ³•ä¸å…è¨±', f'{request.method} ä¸æ”¯æŒæ­¤ç«¯é»')

@app.errorhandler(429)
def rate_limit_exceeded(error):
    """429 éŒ¯èª¤ - è¶…éé€Ÿç‡é™åˆ¶"""
    logger.warning(f"Rate Limit Exceeded: {request.remote_addr} - {request.path}")
    return error_response(429, 'è«‹æ±‚éæ–¼é »ç¹ï¼Œè«‹ç¨å¾Œå†è©¦', 'å·²è¶…éé€Ÿç‡é™åˆ¶')

@app.errorhandler(500)
def internal_error(error):
    """500 éŒ¯èª¤ - æœå‹™å™¨å…§éƒ¨éŒ¯èª¤"""
    logger.error(f"Internal Server Error: {error}", exc_info=True)
    return error_response(500, 'æœå‹™å™¨å…§éƒ¨éŒ¯èª¤', 'è«‹ç¨å¾Œå†è©¦æˆ–è¯ç¹«ç®¡ç†å“¡')

@app.errorhandler(503)
def service_unavailable(error):
    """503 éŒ¯èª¤ - æœå‹™ä¸å¯ç”¨"""
    logger.error(f"Service Unavailable: {error}")
    return error_response(503, 'æœå‹™æš«æ™‚ä¸å¯ç”¨', 'ç³»çµ±ç¶­è­·ä¸­æˆ–è³‡æºä¸è¶³')

@app.errorhandler(Exception)
def handle_exception(error):
    """è™•ç†æ‰€æœ‰æœªæ•ç²çš„ç•°å¸¸"""
    logger.error(f"Unhandled Exception: {error}", exc_info=True)
    
    # å¦‚æœæ˜¯ HTTP ç•°å¸¸ï¼Œä½¿ç”¨å…¶ç‹€æ…‹ç¢¼
    if hasattr(error, 'code'):
        return error_response(error.code, str(error), type(error).__name__)
    
    # å…¶ä»–ç•°å¸¸è¿”å› 500
    return error_response(500, 'ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤', type(error).__name__)

# å…¨å±€è­¦å‘Šåˆ†æå™¨å¯¦ä¾‹
warning_analyzer = None
warning_collector = None

# ========== API è¼”åŠ©å‡½æ•¸ ==========
def validate_request_data(data, required_fields):
    """é©—è­‰è«‹æ±‚æ•¸æ“šæ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ"""
    if not data:
        raise ValueError("è«‹æ±‚é«”ä¸èƒ½ç‚ºç©º")
    
    missing_fields = [field for field in required_fields if field not in data]
    if missing_fields:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")
    
    return True

def safe_api_call(func):
    """API èª¿ç”¨å®‰å…¨åŒ…è£å™¨è£é£¾å™¨"""
    from functools import wraps
    
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except ValueError as e:
            logger.warning(f"Validation Error in {func.__name__}: {e}")
            return error_response(400, str(e))
        except KeyError as e:
            logger.warning(f"Missing Key in {func.__name__}: {e}")
            return error_response(400, f"ç¼ºå°‘å¿…éœ€åƒæ•¸: {e}")
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}", exc_info=True)
            return error_response(500, "è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤")
    
    return wrapper


def init_warning_analysis():
    """åˆå§‹åŒ–è­¦å‘Šåˆ†æç³»çµ±"""
    global warning_analyzer, warning_collector
    if warning_analysis_available:
        try:
            warning_analyzer = WarningHistoryAnalyzer()
            if WarningDataCollector:
                warning_collector = WarningDataCollector(collection_interval=60)  # 60åˆ†é˜æ”¶é›†ä¸€æ¬¡
                # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­å¯å•Ÿå‹•è‡ªå‹•æ”¶é›†
                # warning_collector.start_automated_collection()
            else:
                warning_collector = None
            print("âœ… è­¦å‘Šåˆ†æç³»çµ±åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ è­¦å‘Šåˆ†æç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    return False

# åˆå§‹åŒ–è­¦å‘Šåˆ†æç³»çµ±
init_warning_analysis()

def get_seasonal_sun_times(date=None):
    """
    å››å­£æ—¥å‡ºæ—¥è½æ™‚é–“è‡ªå‹•èª¿æ•´ç³»çµ±
    ä½¿ç”¨ astral åº«è¨ˆç®—ç²¾ç¢ºçš„æ—¥å‡ºæ—¥è½æ™‚é–“ï¼Œä¸¦æ ¹æ“šå››å­£è‡ªå‹•èª¿æ•´
    """
    from datetime import datetime
    import pytz
    
    if date is None:
        hk_tz = pytz.timezone('Asia/Hong_Kong')
        date = datetime.now(hk_tz).date()
    
    try:
        # å˜—è©¦ä½¿ç”¨ astral åº«è¨ˆç®—ç²¾ç¢ºæ™‚é–“
        from astral import LocationInfo
        from astral.sun import sun
        
        hong_kong = LocationInfo("Hong Kong", "Hong Kong", "Asia/Hong_Kong", 22.3193, 114.1694)
        hk_tz = pytz.timezone('Asia/Hong_Kong')
        s = sun(hong_kong.observer, date=date)
        
        # è½‰æ›ç‚ºé¦™æ¸¯æ™‚é–“ä¸¦ç§»é™¤æ™‚å€ä¿¡æ¯
        sunset_time = s['sunset'].astimezone(hk_tz).replace(tzinfo=None)
        sunrise_time = s['sunrise'].astimezone(hk_tz).replace(tzinfo=None)
        
        # ç¢ºä¿æ™‚é–“åœ¨æ­£ç¢ºçš„æ—¥æœŸ
        if sunset_time.date() != date:
            sunset_time = datetime.combine(date, sunset_time.time())
        if sunrise_time.date() != date:
            sunrise_time = datetime.combine(date, sunrise_time.time())
        
        return {
            'sunset': sunset_time.strftime('%H:%M'),
            'sunrise': sunrise_time.strftime('%H:%M'),
            'sunset_dt': sunset_time,
            'sunrise_dt': sunrise_time,
            'method': 'astral'
        }
    except:
        # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ›´ç²¾ç¢ºçš„æœˆåº¦æ™‚é–“è¡¨ï¼ˆåŸºæ–¼é¦™æ¸¯å¤©æ–‡å°æ•¸æ“šï¼‰
        month = date.month if hasattr(date, 'month') else datetime.now().month
        
        # é¦™æ¸¯å¯¦éš›æ—¥è½æ™‚é–“ï¼ˆåŸºæ–¼å¤©æ–‡å°è§€æ¸¬æ•¸æ“šï¼‰
        sunset_times = {
            1: "17:55", 2: "18:20", 3: "18:40", 4: "18:55",
            5: "19:10", 6: "19:20", 7: "19:18", 8: "19:00",
            9: "18:30", 10: "18:00", 11: "17:40", 12: "17:40"
        }
        
        # é¦™æ¸¯å¯¦éš›æ—¥å‡ºæ™‚é–“
        sunrise_times = {
            1: "07:05", 2: "06:55", 3: "06:30", 4: "06:00",
            5: "05:40", 6: "05:35", 7: "05:45", 8: "06:00",
            9: "06:15", 10: "06:30", 11: "06:45", 12: "07:00"
        }
        
        sunset_str = sunset_times.get(month, "18:30")
        sunrise_str = sunrise_times.get(month, "06:30")
        
        return {
            'sunset': sunset_str,
            'sunrise': sunrise_str,
            'sunset_dt': datetime.combine(date, datetime.strptime(sunset_str, "%H:%M").time()),
            'sunrise_dt': datetime.combine(date, datetime.strptime(sunrise_str, "%H:%M").time()),
            'method': 'monthly_table'
        }

def get_optimal_sunset_time():
    """ç²å–ç•¶æœˆå¯¦éš›æ—¥è½æ™‚é–“ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
    sun_times = get_seasonal_sun_times()
    return sun_times['sunset']

def get_optimal_sunrise_time():
    """ç²å–ç•¶æœˆå¯¦éš›æ—¥å‡ºæ™‚é–“"""
    sun_times = get_seasonal_sun_times()
    return sun_times['sunrise']

def get_optimal_burnsky_time():
    """ç²å–æœ€ä½³ç‡’å¤©æ™‚é–“ï¼ˆæ—¥è½å‰40åˆ†é˜ï¼‰"""
    from datetime import timedelta
    
    sun_times = get_seasonal_sun_times()
    sunset_dt = sun_times['sunset_dt']
    
    # ç‡’å¤©æœ€ä½³æ™‚é–“ = æ—¥è½å‰40åˆ†é˜
    optimal_dt = sunset_dt - timedelta(minutes=40)
    
    return optimal_dt.strftime("%H:%M")

def get_optimal_sunrise_burnsky_time():
    """ç²å–æœ€ä½³æ—¥å‡ºç‡’å¤©æ™‚é–“ï¼ˆæ—¥å‡ºå¾Œ10åˆ†é˜ï¼‰"""
    from datetime import timedelta
    
    sun_times = get_seasonal_sun_times()
    sunrise_dt = sun_times['sunrise_dt']
    
    # æ—¥å‡ºç‡’å¤©æœ€ä½³æ™‚é–“ = æ—¥å‡ºå¾Œ10åˆ†é˜
    optimal_dt = sunrise_dt + timedelta(minutes=10)
    
    return optimal_dt.strftime("%H:%M")

def convert_numpy_types(obj):
    """éæ­¸è½‰æ› numpy é¡å‹ç‚º Python åŸç”Ÿé¡å‹ä»¥æ”¯æ´ JSON åºåˆ—åŒ–"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

def analyze_photo_quality(image_data):
    """åˆ†æç…§ç‰‡è³ªé‡ - é‡é»åœ¨é¡è‰²å’Œé›²å±¤è®ŠåŒ–"""
    try:
        # å¦‚æœæ˜¯base64ç·¨ç¢¼ï¼Œå…ˆè§£ç¢¼
        if isinstance(image_data, str) and image_data.startswith('data:image'):
            header, data = image_data.split(',', 1)
            image_data = base64.b64decode(data)
        
        # æ‰“é–‹åœ–ç‰‡
        image = Image.open(io.BytesIO(image_data))
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # èª¿æ•´åœ–ç‰‡å¤§å°ä»¥åŠ å¿«åˆ†æ
        image.thumbnail((800, 600), Image.Resampling.LANCZOS)
        pixels = np.array(image)
        
        # åˆ†æé¡è‰²è³ªé‡
        color_analysis = analyze_burnsky_colors(pixels)
        
        # åˆ†æé›²å±¤è®ŠåŒ–
        cloud_analysis = analyze_cloud_variations(pixels)
        
        # åˆ†ææ™‚é–“ç‰¹å¾µ
        time_analysis = analyze_lighting_quality(pixels)
        
        # ç¶œåˆè©•åˆ† (1-10)
        color_score = color_analysis['intensity'] * 4  # é¡è‰²å¼·åº¦ (0-4åˆ†)
        cloud_score = cloud_analysis['variation'] * 3  # é›²å±¤è®ŠåŒ– (0-3åˆ†)
        lighting_score = time_analysis['golden_ratio'] * 3  # å…‰ç·šè³ªé‡ (0-3åˆ†)
        
        total_score = min(10, color_score + cloud_score + lighting_score)
        
        return {
            'quality_score': total_score,
            'color_analysis': color_analysis,
            'cloud_analysis': cloud_analysis,
            'lighting_analysis': time_analysis,
            'recommendation': generate_photo_recommendation(total_score, color_analysis, cloud_analysis)
        }
    
    except Exception as e:
        print(f"âŒ ç…§ç‰‡åˆ†æéŒ¯èª¤: {e}")
        return {
            'quality_score': 5.0,
            'error': str(e),
            'recommendation': 'ç„¡æ³•åˆ†æç…§ç‰‡ï¼Œè«‹ç¢ºä¿ç…§ç‰‡æ ¼å¼æ­£ç¢º'
        }

def analyze_burnsky_colors(pixels):
    """åˆ†æç‡’å¤©é¡è‰²ç‰¹å¾µ"""
    height, width = pixels.shape[:2]
    
    # é‡é»åˆ†æå¤©ç©ºå€åŸŸ (ä¸ŠåŠéƒ¨)
    sky_region = pixels[:height//2, :]
    
    # è¨ˆç®—æ©™ç´…è‰²æ¯”ä¾‹
    red_channel = sky_region[:, :, 0].astype(float)
    green_channel = sky_region[:, :, 1].astype(float)
    blue_channel = sky_region[:, :, 2].astype(float)
    
    # ç‡’å¤©è‰²å½©ç‰¹å¾µï¼šé«˜ç´…è‰²ã€ä¸­ç­‰ç¶ è‰²ã€ä½è—è‰²
    orange_red_mask = (red_channel > 120) & (green_channel > 60) & (blue_channel < 120)
    warm_ratio = np.sum(orange_red_mask) / orange_red_mask.size
    
    # é¡è‰²é£½å’Œåº¦åˆ†æ
    saturation = np.std([red_channel, green_channel, blue_channel])
    
    # é¡è‰²æ¼¸è®Šåˆ†æ (ç‡’å¤©ç‰¹å¾µ)
    avg_red = np.mean(red_channel)
    avg_blue = np.mean(blue_channel)
    warm_cool_contrast = (avg_red - avg_blue) / 255.0
    
    return {
        'warm_ratio': warm_ratio,
        'saturation': saturation / 100.0,  # æ¨™æº–åŒ–
        'contrast': max(0, warm_cool_contrast),
        'intensity': min(1.0, warm_ratio * 2 + warm_cool_contrast * 0.5)  # ç¶œåˆå¼·åº¦
    }

def analyze_cloud_variations(pixels):
    """åˆ†æé›²å±¤è®ŠåŒ–å’Œå±¤æ¬¡"""
    height, width = pixels.shape[:2]
    
    # è½‰æ›ç‚ºç°åº¦åœ–åˆ†æé›²å±¤ç´‹ç†
    gray = np.mean(pixels, axis=2)
    
    # è¨ˆç®—åœ–åƒçš„æ¨™æº–å·®ï¼ˆé›²å±¤è®ŠåŒ–æŒ‡æ¨™ï¼‰
    cloud_variation = np.std(gray) / 127.5  # æ¨™æº–åŒ–åˆ°0-2
    
    # åˆ†ææ˜æš—å°æ¯”ï¼ˆé›²å±¤å±¤æ¬¡ï¼‰
    hist, _ = np.histogram(gray, bins=50, range=(0, 255))
    contrast_peaks = len([i for i, h in enumerate(hist) if h > np.mean(hist) * 1.5])
    layer_complexity = min(1.0, contrast_peaks / 10.0)
    
    # é‚Šç·£æª¢æ¸¬ (é›²å±¤è¼ªå»“æ¸…æ™°åº¦)
    edges = np.abs(np.gradient(gray))
    edge_strength = np.mean(edges) / 50.0  # æ¨™æº–åŒ–
    
    return {
        'variation': min(1.0, cloud_variation),
        'layers': layer_complexity,
        'edge_definition': min(1.0, edge_strength),
        'overall_quality': min(1.0, (cloud_variation + layer_complexity + edge_strength) / 3)
    }

def analyze_lighting_quality(pixels):
    """åˆ†æå…‰ç·šè³ªé‡å’Œæ™‚é–“ç‰¹å¾µ"""
    # æ•´é«”äº®åº¦åˆ†æ
    brightness = np.mean(pixels) / 255.0
    
    # é»ƒé‡‘æ™‚æ®µç‰¹å¾µ (åæš–è‰²èª¿)
    red_avg = np.mean(pixels[:, :, 0])
    blue_avg = np.mean(pixels[:, :, 2])
    golden_ratio = min(1.0, (red_avg - blue_avg + 50) / 100.0)
    
    # å…‰ç·šæŸ”å’Œåº¦
    brightness_std = np.std(pixels) / 127.5
    softness = 1.0 - min(1.0, brightness_std)  # æ¨™æº–å·®è¶Šå°è¶ŠæŸ”å’Œ
    
    return {
        'brightness': brightness,
        'golden_ratio': max(0, golden_ratio),
        'softness': softness,
        'quality': (brightness * 0.3 + golden_ratio * 0.5 + softness * 0.2)
    }

def generate_photo_recommendation(score, color_analysis, cloud_analysis):
    """æ ¹æ“šåˆ†æçµæœç”¢ç”Ÿå»ºè­°"""
    if score >= 8:
        return "ğŸ”¥ æ¥µä½³ç‡’å¤©ï¼é¡è‰²æ¿ƒçƒˆï¼Œé›²å±¤å±¤æ¬¡è±å¯Œï¼Œå»ºè­°è¨˜éŒ„ç•¶æ™‚å¤©æ°£æ¢ä»¶"
    elif score >= 6:
        if color_analysis['intensity'] > 0.7:
            return "ğŸŒ… è‰²å½©ä¸éŒ¯ï¼é›²å±¤å¯ä»¥æ›´è±å¯Œä¸€äº›"
        elif cloud_analysis['variation'] > 0.7:
            return "â˜ï¸ é›²å±¤å±¤æ¬¡å¾ˆå¥½ï¼å¯ä»¥ç­‰å¾…æ›´å¼·çƒˆçš„è‰²å½©"
        else:
            return "âœ¨ ä¸éŒ¯çš„ç‡’å¤©ï¼Œå„æ–¹é¢éƒ½æœ‰æ”¹å–„ç©ºé–“"
    elif score >= 4:
        return "ğŸŒ¤ï¸ æ™®é€šç‡’å¤©ï¼Œå»ºè­°ç­‰å¾…æ›´å¥½çš„æ¢ä»¶"
    else:
        return "ğŸ˜ éç‡’å¤©æ¢ä»¶ï¼Œå»ºè­°ä¸‹æ¬¡å˜—è©¦"

def record_burnsky_photo_case(date, time, location, weather_conditions, visual_rating, prediction_score=None, photo_analysis=None, saved_path=None):
    """è¨˜éŒ„ç‡’å¤©ç…§ç‰‡æ¡ˆä¾‹ - å°ˆæ³¨æ–¼MLè¨“ç·´æ•¸æ“šæ”¶é›†è€Œéå³æ™‚æ ¡æ­£"""
    case_id = f"{date}_{time}_{location}".replace(' ', '_').replace(':', '-')
    
    case_data = {
        'date': date,
        'time': time,
        'location': location,
        'weather_conditions': weather_conditions,
        'visual_rating': visual_rating,
        'prediction_score': prediction_score,
        'photo_analysis': photo_analysis,
        'saved_path': saved_path,
        'timestamp': datetime.now().isoformat(),
        'for_ml_training': True,  # æ¨™è¨˜ç‚ºMLè¨“ç·´æ•¸æ“š
        'training_status': 'pending'  # ç­‰å¾…åŠ å…¥è¨“ç·´
    }
    
    BURNSKY_PHOTO_CASES[case_id] = case_data
    
    # ä¿å­˜åˆ°MLè¨“ç·´æ•¸æ“šåº«
    save_ml_training_case(case_data)
    
    storage_status = "å·²å„²å­˜" if saved_path else "åƒ…åˆ†æ"
    print(f"ğŸ“¸ è¨˜éŒ„MLè¨“ç·´æ¡ˆä¾‹: {case_id} (è¦–è¦ºè©•åˆ†: {visual_rating}/10, {storage_status})")
    
    # æª¢æŸ¥æ˜¯å¦é”åˆ°é‡æ–°è¨“ç·´çš„é–¾å€¼
    check_ml_retrain_threshold()
    
    return case_id

def save_ml_training_case(case_data):
    """ä¿å­˜æ¡ˆä¾‹åˆ°MLè¨“ç·´æ•¸æ“šåº«"""
    try:
        conn = sqlite3.connect('ml_training_data.db')
        cursor = conn.cursor()
        
        # å‰µå»ºMLè¨“ç·´æ•¸æ“šè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ml_training_cases (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                case_id TEXT UNIQUE,
                date TEXT,
                time TEXT,
                location TEXT,
                visual_rating REAL,
                prediction_score REAL,
                weather_features TEXT,  -- JSONæ ¼å¼å¤©æ°£ç‰¹å¾µ
                photo_features TEXT,    -- JSONæ ¼å¼ç…§ç‰‡ç‰¹å¾µ
                target_label TEXT,      -- è¨“ç·´ç›®æ¨™ (good_burnsky, poor_burnskyç­‰)
                training_status TEXT DEFAULT 'pending',
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                used_in_training DATETIME NULL
            )
        ''')
        
        # æº–å‚™MLç‰¹å¾µæ•¸æ“š
        weather_features = extract_ml_weather_features(case_data['weather_conditions'])
        photo_features = case_data.get('photo_analysis', {})
        
        # æ ¹æ“šè¦–è¦ºè©•åˆ†ç”Ÿæˆè¨“ç·´æ¨™ç±¤
        target_label = generate_training_label(case_data['visual_rating'])
        
        cursor.execute('''
            INSERT OR REPLACE INTO ml_training_cases 
            (case_id, date, time, location, visual_rating, prediction_score, 
             weather_features, photo_features, target_label)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            case_data.get('case_id', f"{case_data['date']}_{case_data['time']}"),
            case_data['date'],
            case_data['time'],
            case_data['location'],
            case_data['visual_rating'],
            case_data.get('prediction_score'),
            json.dumps(weather_features, ensure_ascii=False),
            json.dumps(photo_features, ensure_ascii=False),
            target_label
        ))
        
        conn.commit()
        conn.close()
        print(f"ğŸ¤– MLè¨“ç·´æ¡ˆä¾‹å·²ä¿å­˜: {target_label}")
        
    except Exception as e:
        print(f"âŒ MLè¨“ç·´æ•¸æ“šä¿å­˜å¤±æ•—: {e}")

def extract_ml_weather_features(weather_conditions):
    """æå–ç”¨æ–¼MLè¨“ç·´çš„å¤©æ°£ç‰¹å¾µ"""
    # ç²å–ç•¶å‰å¤©æ°£æ•¸æ“šä½œç‚ºç‰¹å¾µ
    try:
        weather_data = get_cached_data('weather', fetch_weather_data)
        
        features = {
            'temperature': weather_data.get('temperature', {}).get('value', 0),
            'humidity': weather_data.get('humidity', {}).get('value', 0),
            'pressure': weather_data.get('pressure', {}).get('value', 0),
            'visibility': weather_data.get('visibility', {}).get('value', 0),
            'wind_speed': weather_data.get('wind', {}).get('speed', 0),
            'cloud_amount': weather_data.get('cloud', {}).get('amount', 0),
            'uv_index': weather_data.get('uv', {}).get('value', 0),
            'time_of_day': datetime.now().hour,
            'month': datetime.now().month,
            'season': get_season(datetime.now().month),
            'notes': weather_conditions.get('notes', '')
        }
        
        return features
    except Exception as e:
        print(f"âŒ å¤©æ°£ç‰¹å¾µæå–å¤±æ•—: {e}")
        return {'notes': weather_conditions.get('notes', '')}

def get_season(month):
    """ç²å–å­£ç¯€"""
    if month in [12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5]:
        return 'spring'
    elif month in [6, 7, 8]:
        return 'summer'
    else:
        return 'autumn'

def generate_training_label(visual_rating):
    """æ ¹æ“šè¦–è¦ºè©•åˆ†ç”ŸæˆMLè¨“ç·´æ¨™ç±¤"""
    if visual_rating >= 8:
        return 'excellent_burnsky'
    elif visual_rating >= 6:
        return 'good_burnsky'
    elif visual_rating >= 4:
        return 'moderate_burnsky'
    elif visual_rating >= 2:
        return 'poor_burnsky'
    else:
        return 'no_burnsky'

def check_ml_retrain_threshold():
    """æª¢æŸ¥æ˜¯å¦é”åˆ°MLæ¨¡å‹é‡æ–°è¨“ç·´çš„é–¾å€¼"""
    try:
        conn = sqlite3.connect('ml_training_data.db')
        cursor = conn.cursor()
        
        # æª¢æŸ¥æ–°å¢çš„æœªä½¿ç”¨è¨“ç·´æ•¸æ“š
        cursor.execute('''
            SELECT COUNT(*) FROM ml_training_cases 
            WHERE training_status = 'pending'
        ''')
        pending_count = cursor.fetchone()[0]
        
        # æª¢æŸ¥ç¸½è¨“ç·´æ•¸æ“šé‡
        cursor.execute('SELECT COUNT(*) FROM ml_training_cases')
        total_count = cursor.fetchone()[0]
        
        conn.close()
        
        print(f"ğŸ¤– MLè¨“ç·´æ•¸æ“šç‹€æ…‹: {pending_count} å¾…è™•ç†, {total_count} ç¸½è¨ˆ")
        
        # é‡æ–°è¨“ç·´é–¾å€¼åˆ¤æ–·
        if pending_count >= 10:  # ç´¯ç©10å€‹æ–°æ¡ˆä¾‹
            trigger_ml_retrain('sufficient_new_data')
        elif total_count >= 50 and pending_count >= 5:  # æˆ–ç¸½æ•¸è¶…é50ä¸”æœ‰5å€‹æ–°æ¡ˆä¾‹
            trigger_ml_retrain('incremental_update')
        
    except Exception as e:
        print(f"âŒ MLé–¾å€¼æª¢æŸ¥å¤±æ•—: {e}")

def trigger_ml_retrain(reason):
    """è§¸ç™¼MLæ¨¡å‹é‡æ–°è¨“ç·´"""
    print(f"ğŸš€ è§¸ç™¼MLæ¨¡å‹é‡æ–°è¨“ç·´: {reason}")
    
    try:
        # æ¨™è¨˜é‡æ–°è¨“ç·´ä»»å‹™
        retrain_task = {
            'triggered_at': datetime.now().isoformat(),
            'reason': reason,
            'status': 'scheduled',
            'priority': 'normal' if reason == 'incremental_update' else 'high'
        }
        
        # é€™è£¡å¯ä»¥æ•´åˆåˆ°èƒŒæ™¯ä»»å‹™ç³»çµ± (å¦‚ Celery, RQ ç­‰)
        # æˆ–ç°¡å–®è¨˜éŒ„åˆ°æ–‡ä»¶ç³»çµ±
        with open('ml_retrain_queue.json', 'a') as f:
            f.write(json.dumps(retrain_task) + '\n')
        
        print(f"âœ… MLé‡æ–°è¨“ç·´ä»»å‹™å·²æ’ç¨‹")
        
    except Exception as e:
        print(f"âŒ MLé‡æ–°è¨“ç·´è§¸ç™¼å¤±æ•—: {e}")

def analyze_photo_case_patterns():
    """åˆ†æç…§ç‰‡æ¡ˆä¾‹æ¨¡å¼"""
    if not BURNSKY_PHOTO_CASES:
        return {}
    
    patterns = {
        "successful_conditions": [],
        "time_patterns": {},
        "weather_patterns": {},
        "location_patterns": {}
    }
    
    for case_id, case in BURNSKY_PHOTO_CASES.items():
        if case["visual_rating"] >= 7:  # æˆåŠŸæ¡ˆä¾‹
            patterns["successful_conditions"].append(case)
            
            # æ™‚é–“æ¨¡å¼
            time_hour = int(case["time"].split(":")[0])
            if time_hour not in patterns["time_patterns"]:
                patterns["time_patterns"][time_hour] = 0
            patterns["time_patterns"][time_hour] += 1
            
            # å¤©æ°£æ¨¡å¼
            for condition, value in case["weather_conditions"].items():
                if condition not in patterns["weather_patterns"]:
                    patterns["weather_patterns"][condition] = []
                patterns["weather_patterns"][condition].append(value)
    
    return patterns

def is_similar_to_successful_cases(current_conditions):
    """æª¢æŸ¥ç•¶å‰æ¢ä»¶æ˜¯å¦é¡ä¼¼æˆåŠŸæ¡ˆä¾‹"""
    patterns = analyze_photo_case_patterns()
    
    if not patterns["successful_conditions"]:
        return False, 0
    
    similarity_score = 0
    total_factors = 0
    
    # æ™‚é–“ç›¸ä¼¼åº¦
    current_hour = datetime.now().hour
    if current_hour in patterns["time_patterns"]:
        similarity_score += patterns["time_patterns"][current_hour] * 10
        total_factors += 1
    
    # å¤©æ°£æ¢ä»¶ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–ï¼‰
    if "cloud_coverage" in current_conditions and "cloud_coverage" in patterns["weather_patterns"]:
        similarity_score += 20
        total_factors += 1
    
    if "visibility" in current_conditions and "visibility" in patterns["weather_patterns"]:
        similarity_score += 15
        total_factors += 1
    
    average_similarity = similarity_score / max(total_factors, 1)
    is_similar = average_similarity >= 15  # é–¾å€¼
    
    return is_similar, average_similarity

def apply_burnsky_photo_corrections(score, weather_data, prediction_type):
    """åŸºæ–¼å¯¦éš›ç‡’å¤©ç…§ç‰‡æ¡ˆä¾‹é€²è¡Œæ ¡æ­£ - é‡é»åœ¨å“è³ªè€Œéç›²ç›®æ¨é«˜åˆ†æ•¸"""
    
    correction = 0
    quality_factors = []
    
    if prediction_type == 'sunset':
        current_hour = datetime.now().hour
        current_minute = datetime.now().minute
        current_time_decimal = current_hour + current_minute / 60.0
        
        # 7æœˆæœ€ä½³ç‡’å¤©æ™‚é–“ï¼š18:50 (19:30æ—¥è½å‰40åˆ†é˜)
        optimal_time = 18 + 50/60.0  # 18.833
        
        # æ™‚é–“çª—å£æ ¡æ­£ï¼ˆä½†ä¸ç›²ç›®æ¨é«˜ï¼‰
        time_diff = abs(current_time_decimal - optimal_time)
        
        # é›²å±¤å“è³ªåˆ†æï¼ˆé‡é»ï¼‰
        cloud_quality_score = analyze_cloud_quality_for_burnsky(weather_data)
        quality_factors.append(f"é›²å±¤å“è³ª: {cloud_quality_score:.1f}/10")
        
        # å¤§æ°£æ¢ä»¶åˆ†æï¼ˆé‡é»ï¼‰
        atmospheric_quality = analyze_atmospheric_conditions(weather_data)
        quality_factors.append(f"å¤§æ°£æ¢ä»¶: {atmospheric_quality:.1f}/10")
        
        # åŸºæ–¼å“è³ªçš„æ ¡æ­£ï¼Œè€Œä¸æ˜¯ç›²ç›®åŠ åˆ†
        if cloud_quality_score >= 7 and atmospheric_quality >= 6:
            if time_diff <= 0.33:  # 20åˆ†é˜å…§ + é«˜å“è³ª
                correction += 20
                quality_factors.append("ï¿½ æœ€ä½³æ™‚é–“+å„ªç§€æ¢ä»¶: +20åˆ†")
            elif time_diff <= 0.67:  # 40åˆ†é˜å…§ + é«˜å“è³ª
                correction += 12
                quality_factors.append("âœ¨ è‰¯å¥½æ™‚é–“+å„ªç§€æ¢ä»¶: +12åˆ†")
        elif cloud_quality_score >= 5 or atmospheric_quality >= 5:
            if time_diff <= 0.33:
                correction += 8
                quality_factors.append("ğŸŒ¤ï¸ æœ€ä½³æ™‚é–“+æ™®é€šæ¢ä»¶: +8åˆ†")
            elif time_diff <= 0.67:
                correction += 5
                quality_factors.append("â° è‰¯å¥½æ™‚é–“+æ™®é€šæ¢ä»¶: +5åˆ†")
        
        # é¡è‰²æ¢ä»¶åˆ†æï¼ˆæ–°å¢ï¼‰
        color_potential = analyze_color_potential(weather_data)
        quality_factors.append(f"é¡è‰²æ½›åŠ›: {color_potential:.1f}/10")
        
        if color_potential >= 7:
            correction += 8
            quality_factors.append("ğŸŒˆ é«˜é¡è‰²æ½›åŠ›: +8åˆ†")
        elif color_potential >= 5:
            correction += 3
            quality_factors.append("ğŸ¨ ä¸­ç­‰é¡è‰²æ½›åŠ›: +3åˆ†")
        
        # ğŸš« æš«æ™‚ç¦ç”¨æ­·å²æ¡ˆä¾‹æ ¡æ­£ - ç­‰å¾…çœŸå¯¦ç…§ç‰‡æ•¸æ“šæ”¶é›†
        # åŸå› : ç›®å‰çš„æ­·å²æ¡ˆä¾‹éƒ½æ˜¯ç¡¬ç·¨ç¢¼è™›å‡æ•¸æ“šï¼Œç¼ºä¹çœŸå¯¦çš„å“è³ªæŒ‡æ¨™
        # æœªä¾†è¨ˆåŠƒ: å»ºç«‹çœŸå¯¦çš„ç…§ç‰‡ä¸Šå‚³å’Œè©•åˆ†ç³»çµ±å¾Œé‡æ–°å•Ÿç”¨
        
        # è¨»è§£æ‰çš„æ­·å²æ¡ˆä¾‹åŒ¹é…é‚è¼¯:
        # current_conditions = {...}
        # is_similar, similarity_score, match_reason = is_similar_to_quality_cases(current_conditions)
        # pattern_correction = ...
        
        quality_factors.append("ï¿½ æ­·å²æ¡ˆä¾‹æ ¡æ­£å·²ç¦ç”¨ (ç­‰å¾…çœŸå¯¦æ•¸æ“š)")
        
        # å“è³ªé–¾å€¼æ§åˆ¶ - é˜²æ­¢ä½å“è³ªæƒ…æ³è¢«éåº¦æ¨é«˜
        if cloud_quality_score < 4 and atmospheric_quality < 4:
            correction = min(correction, 5)  # ä½å“è³ªæƒ…æ³æœ€å¤šåŠ 5åˆ†
            quality_factors.append("âš ï¸ ä½å“è³ªé™åˆ¶: æ ¡æ­£ä¸Šé™5åˆ†")
        elif cloud_quality_score < 6 and atmospheric_quality < 6:
            correction = min(correction, 15)  # ä¸­ç­‰å“è³ªæœ€å¤šåŠ 15åˆ†
            quality_factors.append("ğŸ“Š ä¸­ç­‰å“è³ªé™åˆ¶: æ ¡æ­£ä¸Šé™15åˆ†")
        
        print(f"ğŸ“¸ å“è³ªå°å‘æ ¡æ­£: +{correction}åˆ†")
        for factor in quality_factors:
            print(f"   - {factor}")
    
    return correction

def analyze_stable_photo_patterns():
    """åˆ†æç©©å®šçš„ç…§ç‰‡æ¨¡å¼ï¼ˆç”¨æ–¼æ ¡æ­£è€Œéå³æ™‚æ›´æ–°ï¼‰"""
    try:
        conn = sqlite3.connect('ml_training_data.db')
        cursor = conn.cursor()
        
        # åªä½¿ç”¨å·²ç¶“ç©©å®šçš„æ­·å²æ•¸æ“š
        cursor.execute('''
            SELECT COUNT(*) FROM ml_training_cases 
            WHERE training_status != 'pending'
            AND created_at < datetime('now', '-1 day')
        ''')
        stable_cases = cursor.fetchone()[0]
        
        if stable_cases >= 10:
            # æœ‰è¶³å¤ çš„ç©©å®šæ­·å²æ•¸æ“š
            cursor.execute('''
                SELECT AVG(visual_rating) FROM ml_training_cases 
                WHERE visual_rating >= 7 
                AND training_status != 'pending'
                AND created_at < datetime('now', '-1 day')
            ''')
            avg_quality = cursor.fetchone()[0] or 0
            
            conn.close()
            
            return {
                'sufficient_data': True,
                'total_cases': stable_cases,
                'avg_quality': avg_quality,
                'confidence': 'high' if stable_cases >= 20 else 'medium'
            }
        
        conn.close()
        return {'sufficient_data': False, 'total_cases': stable_cases}
        
    except:
        return {'sufficient_data': False, 'total_cases': 0}

def analyze_cloud_quality_for_burnsky(weather_data):
    """åˆ†æé›²å±¤å“è³ªå°ç‡’å¤©çš„é©åˆåº¦"""
    score = 5.0  # åŸºç¤åˆ†æ•¸
    
    if 'cloud' in weather_data:
        cloud_data = weather_data['cloud']
        
        # é›²é‡åˆ†æ (30-70%æœ€ä½³)
        if 'amount' in cloud_data:
            cloud_amount = cloud_data['amount']
            if 30 <= cloud_amount <= 70:
                score += 2
            elif 20 <= cloud_amount <= 80:
                score += 1
            elif cloud_amount > 90:
                score -= 2
        
        # é›²å±¤é«˜åº¦åˆ†æ
        if 'type' in cloud_data:
            cloud_type = cloud_data['type']
            if 'mid' in cloud_type or 'high' in cloud_type:
                score += 1.5  # ä¸­é«˜å±¤é›²è¼ƒä½³
            elif 'low' in cloud_type:
                score -= 0.5
    
    # èƒ½è¦‹åº¦åˆ†æ
    if 'visibility' in weather_data:
        visibility = weather_data['visibility'].get('value', 10)
        if visibility >= 8:
            score += 1.5
        elif visibility >= 5:
            score += 0.5
        else:
            score -= 1
    
    return min(10, max(0, score))

def analyze_atmospheric_conditions(weather_data):
    """åˆ†æå¤§æ°£æ¢ä»¶å°ç‡’å¤©çš„å½±éŸ¿"""
    score = 5.0
    
    # æ¿•åº¦åˆ†æ (40-60%è¼ƒä½³)
    if 'humidity' in weather_data:
        humidity = weather_data['humidity'].get('value', 60)
        if 40 <= humidity <= 60:
            score += 2
        elif 30 <= humidity <= 70:
            score += 1
        elif humidity > 80:
            score -= 1
    
    # é¢¨é€Ÿåˆ†æ (è¼•é¢¨è¼ƒä½³)
    if 'wind' in weather_data:
        wind_speed = weather_data['wind'].get('speed', 10)
        if wind_speed <= 15:
            score += 1
        elif wind_speed <= 25:
            score += 0.5
        else:
            score -= 1
    
    # æ°£å£“ç©©å®šæ€§
    if 'pressure' in weather_data:
        pressure = weather_data['pressure'].get('value', 1013)
        if 1010 <= pressure <= 1020:
            score += 1
    
    return min(10, max(0, score))

def analyze_color_potential(weather_data):
    """åˆ†æé¡è‰²æ½›åŠ› - ç‡’å¤©è‰²å½©å¯èƒ½æ€§"""
    score = 5.0
    
    # é›²å±¤æ•£å°„æ½›åŠ›
    if 'cloud' in weather_data:
        cloud_amount = weather_data['cloud'].get('amount', 50)
        # 40-60%é›²é‡æœ‰æœ€ä½³æ•£å°„æ•ˆæœ
        if 40 <= cloud_amount <= 60:
            score += 2.5
        elif 30 <= cloud_amount <= 70:
            score += 1.5
        elif cloud_amount < 20:
            score -= 1  # å¤ªå°‘é›²å±¤ï¼Œç¼ºä¹æ•£å°„
        elif cloud_amount > 80:
            score -= 2  # å¤ªå¤šé›²å±¤ï¼Œé˜»æ“‹é™½å…‰
    
    # å¤§æ°£é€æ˜åº¦
    if 'visibility' in weather_data:
        visibility = weather_data['visibility'].get('value', 10)
        if visibility >= 10:
            score += 1.5  # æ¸…æ¾ˆå¤§æ°£æœ‰åˆ©é¡è‰²å±•ç¾
        elif visibility >= 7:
            score += 1
        elif visibility < 5:
            score -= 1.5  # éœ§éœ¾å½±éŸ¿é¡è‰²
    
    # æ¿•åº¦å°æ•£å°„çš„å½±éŸ¿
    if 'humidity' in weather_data:
        humidity = weather_data['humidity'].get('value', 60)
        if 45 <= humidity <= 65:
            score += 1  # é©åº¦æ¿•åº¦æœ‰åˆ©æ•£å°„
        elif humidity > 80:
            score -= 0.5  # éé«˜æ¿•åº¦å¯èƒ½é€ æˆéœ§æ°£
    
    return min(10, max(0, score))

def is_similar_to_quality_cases(current_conditions):
    """æª¢æŸ¥æ˜¯å¦èˆ‡é«˜å“è³ªæˆåŠŸæ¡ˆä¾‹ç›¸ä¼¼"""
    if not BURNSKY_PHOTO_CASES:
        return False, 0, "ç„¡æ¡ˆä¾‹"
    
    best_similarity = 0
    best_match_reason = ""
    
    for case_id, case in BURNSKY_PHOTO_CASES.items():
        if case['visual_rating'] >= 7:  # åªæ¯”è¼ƒé«˜è©•åˆ†æ¡ˆä¾‹
            similarity = 0
            reasons = []
            
            # æ¯”è¼ƒå“è³ªæŒ‡æ¨™ - æ›´åš´æ ¼çš„åŒ¹é…æ¢ä»¶
            if abs(current_conditions['cloud_quality'] - case.get('cloud_quality', 5)) <= 1.5:
                similarity += 3
                reasons.append("é›²å±¤å“è³ªç›¸ä¼¼")
            
            if abs(current_conditions['atmospheric_quality'] - case.get('atmospheric_quality', 5)) <= 1.5:
                similarity += 3
                reasons.append("å¤§æ°£æ¢ä»¶ç›¸ä¼¼")
            
            if abs(current_conditions['color_potential'] - case.get('color_potential', 5)) <= 1.5:
                similarity += 4
                reasons.append("é¡è‰²æ½›åŠ›ç›¸ä¼¼")
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match_reason = " + ".join(reasons)
    
    return best_similarity >= 6, best_similarity, best_match_reason

def initialize_photo_cases():
    """åˆå§‹åŒ–å·²çŸ¥çš„æˆåŠŸç‡’å¤©æ¡ˆä¾‹"""
    
    # 7æœˆ27æ—¥çš„æˆåŠŸæ¡ˆä¾‹
    record_burnsky_photo_case(
        date="2025-07-27",
        time="19:10",
        location="æµæµ®å±±",
        weather_conditions={
            "cloud_coverage": "ä¸­ç­‰å±¤æ¬¡é›²",
            "visibility": "è‰¯å¥½",
            "humidity": "é©ä¸­",
            "wind": "å¾®é¢¨"
        },
        visual_rating=8,
        prediction_score=32
    )
    
    record_burnsky_photo_case(
        date="2025-07-27",
        time="19:10",
        location="æ©«ç€¾å³¶",
        weather_conditions={
            "cloud_coverage": "ä¸­ç­‰å±¤æ¬¡é›²",
            "visibility": "è‰¯å¥½",
            "humidity": "é©ä¸­",
            "wind": "å¾®é¢¨"
        },
        visual_rating=9,
        prediction_score=32
    )
    
    record_burnsky_photo_case(
        date="2025-07-24",
        time="18:40",
        location="æµæµ®å±±",
        weather_conditions={
            "cloud_coverage": "é©ä¸­é›²å±¤",
            "visibility": "è‰¯å¥½",
            "humidity": "é©ä¸­",
            "wind": "å¾®é¢¨"
        },
        visual_rating=8,
        prediction_score=32
    )
    
    record_burnsky_photo_case(
        date="2025-07-28",
        time="18:50",
        location="æ©«ç€¾å³¶",
        weather_conditions={
            "cloud_coverage": "è–„é›²å±¤è¦†è“‹",
            "visibility": "è‰¯å¥½",
            "humidity": "é©ä¸­",
            "wind": "å¾®é¢¨",
            "sky_condition": "å¹³éœç°è—è‰²èª¿"
        },
        visual_rating=3,
        prediction_score=None  # å¾…ç³»çµ±é æ¸¬
    )
    
    record_burnsky_photo_case(
        date="2025-07-28",
        time="18:55",
        location="æµæµ®å±±",
        weather_conditions={
            "cloud_coverage": "è–„é›²å±¤å‡å‹»åˆ†ä½ˆ",
            "visibility": "æ¥µä½³",
            "humidity": "é©ä¸­",
            "wind": "å¾®é¢¨",
            "sky_condition": "ç°è—è‰²èª¿ï¼Œç„¡ç‡’å¤©è·¡è±¡",
            "geographic_features": "å¯è¦‹æ·±åœ³å¤©éš›ç·šå’Œè·¨æµ·å¤§æ©‹"
        },
        visual_rating=3,
        prediction_score=None  # å¾…ç³»çµ±é æ¸¬
    )
    
    print(f"ğŸ“¸ å·²åˆå§‹åŒ– {len(BURNSKY_PHOTO_CASES)} å€‹ç‡’å¤©ç…§ç‰‡æ¡ˆä¾‹")

def parse_warning_details(warning_input):
    """è§£æè­¦å‘Šè©³ç´°ä¿¡æ¯ï¼Œæå–è­¦å‘Šé¡å‹ã€ç­‰ç´šå’Œå…·é«”å…§å®¹ - å¢å¼·ç‰ˆ"""
    import ast
    
    # æå–è­¦å‘Šæ–‡æœ¬å’Œä»£ç¢¼
    warning_text = ""
    warning_code = ""
    
    if isinstance(warning_input, dict):
        # è™•ç†å­—å…¸æ ¼å¼
        if 'contents' in warning_input and isinstance(warning_input['contents'], list):
            warning_text = ' '.join(warning_input['contents'])
        else:
            warning_text = str(warning_input)
        warning_code = warning_input.get('warningStatementCode', '')
    elif isinstance(warning_input, str):
        # å˜—è©¦è§£æJSONå­—ç¬¦ä¸²æ ¼å¼
        try:
            if warning_input.startswith('{') and warning_input.endswith('}'):
                parsed_data = ast.literal_eval(warning_input)
                if isinstance(parsed_data, dict):
                    if 'contents' in parsed_data and isinstance(parsed_data['contents'], list):
                        warning_text = ' '.join(parsed_data['contents'])
                    else:
                        warning_text = str(parsed_data)
                    warning_code = parsed_data.get('warningStatementCode', '')
                else:
                    warning_text = warning_input
            else:
                warning_text = warning_input
        except:
            warning_text = warning_input
    else:
        warning_text = str(warning_input)
    
    warning_info = {
        'category': 'unknown',
        'subcategory': '',
        'level': 0,
        'severity': 'low',
        'impact_factors': [],
        'duration_hint': '',
        'area_specific': False,
        'original_text': warning_text,
        'warning_code': warning_code
    }
    
    text_lower = warning_text.lower()
    
    # 0. å„ªå…ˆä½¿ç”¨å®˜æ–¹è­¦å‘Šä»£ç¢¼åˆ†é¡
    if warning_code:
        if warning_code == 'WTS':
            warning_info['category'] = 'thunderstorm'
            warning_info['subcategory'] = 'general_thunderstorm'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['é›·é›»æ´»å‹•', 'å±€éƒ¨é›¨æ°´']
        elif warning_code == 'WHOT':
            warning_info['category'] = 'temperature'
            warning_info['subcategory'] = 'extreme_heat'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['é«˜æº«å½±éŸ¿', 'ä¸­æš‘é¢¨éšª', 'ç´«å¤–ç·šå¼·']
        elif warning_code == 'WCOLD':
            warning_info['category'] = 'temperature'
            warning_info['subcategory'] = 'extreme_cold'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['ä½æº«å½±éŸ¿', 'ä¿æš–éœ€è¦']
        elif warning_code == 'WTCSGNL':
            warning_info['category'] = 'wind_storm'
            warning_info['subcategory'] = 'tropical_cyclone'
            warning_info['level'] = 3
            warning_info['severity'] = 'severe'
            warning_info['impact_factors'] = ['å¼·é¢¨å½±éŸ¿', 'æµ·ä¸Šé¢¨æµª', 'æˆ¶å¤–å±éšª']
    
    # 1. å¦‚æœæ²’æœ‰ä»£ç¢¼è­˜åˆ¥ï¼Œä½¿ç”¨æ–‡æœ¬é—œéµè©åˆ†æ
    if warning_info['category'] == 'unknown':
        # é›¨é‡è­¦å‘Šç´°åˆ†
        if any(keyword in text_lower for keyword in ['é›¨', 'rain', 'é™é›¨', 'æš´é›¨']):
            warning_info['category'] = 'rainfall'
            if any(keyword in text_lower for keyword in ['é»‘é›¨', 'é»‘è‰²æš´é›¨', 'black rain']):
                warning_info['subcategory'] = 'black_rain'
                warning_info['level'] = 4
                warning_info['severity'] = 'extreme'
                warning_info['impact_factors'] = ['èƒ½è¦‹åº¦æ¥µå·®', 'é“è·¯ç©æ°´', 'å±±æ´ªé¢¨éšª']
            elif any(keyword in text_lower for keyword in ['ç´…é›¨', 'ç´…è‰²æš´é›¨', 'red rain']):
                warning_info['subcategory'] = 'red_rain'
                warning_info['level'] = 3
                warning_info['severity'] = 'severe'
                warning_info['impact_factors'] = ['èƒ½è¦‹åº¦å·®', 'äº¤é€šé˜»å¡', 'æˆ¶å¤–é¢¨éšª']
            elif any(keyword in text_lower for keyword in ['é»ƒé›¨', 'é»ƒè‰²æš´é›¨', 'amber rain']):
                warning_info['subcategory'] = 'amber_rain'
                warning_info['level'] = 2
                warning_info['severity'] = 'moderate'
                warning_info['impact_factors'] = ['èƒ½è¦‹åº¦ä¸‹é™', 'äº¤é€šå»¶èª¤']
            elif any(keyword in text_lower for keyword in ['æ°´æµ¸', 'ç‰¹åˆ¥å ±å‘Š', 'å±±æ´ª']):
                warning_info['subcategory'] = 'flood_warning'
                warning_info['level'] = 3
                warning_info['severity'] = 'severe'
                warning_info['impact_factors'] = ['é“è·¯æ°´æµ¸', 'å±±æ´ªé¢¨éšª', 'åœ°ä¸‹é€šé“å±éšª']
    
        # 2. é¢¨æš´/é¢±é¢¨è­¦å‘Šç´°åˆ†
        elif any(keyword in text_lower for keyword in ['é¢¨çƒ', 'é¢±é¢¨', 'ç†±å¸¶æ°£æ—‹', 'typhoon', 'wtcsgnl']):
            warning_info['category'] = 'wind_storm'
            if any(keyword in text_lower for keyword in ['åè™Ÿ', '10è™Ÿ', 'é¢¶é¢¨', 'hurricane']):
                warning_info['subcategory'] = 'hurricane_10'
                warning_info['level'] = 5
                warning_info['severity'] = 'extreme'
                warning_info['impact_factors'] = ['æ¥µå¼·é¢¨æš´', 'å…¨é¢åœå·¥', 'å»ºç¯‰ç‰©å±éšª', 'æµ·æµªç¿»é¨°']
            elif any(keyword in text_lower for keyword in ['ä¹è™Ÿ', '9è™Ÿ', 'æš´é¢¨']):
                warning_info['subcategory'] = 'gale_9'
                warning_info['level'] = 4
                warning_info['severity'] = 'severe'
                warning_info['impact_factors'] = ['å¼·çƒˆé¢¨æš´', 'æˆ¶å¤–å±éšª', 'æµ·ä¸Šé¢¨æµª']
        
        # 3. é›·æš´è­¦å‘Šç´°åˆ†
        elif any(keyword in text_lower for keyword in ['é›·æš´', 'é–ƒé›»', 'thunderstorm', 'lightning']):
            warning_info['category'] = 'thunderstorm'
            warning_info['subcategory'] = 'general_thunderstorm'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['é›·é›»æ´»å‹•', 'å±€éƒ¨é›¨æ°´']
        
        # 4. æº«åº¦ç›¸é—œè­¦å‘Š
        elif any(keyword in text_lower for keyword in ['é…·ç†±', 'å¯’å†·', 'é«˜æº«', 'ä½æº«', 'heat', 'cold']):
            warning_info['category'] = 'temperature'
            if any(keyword in text_lower for keyword in ['é…·ç†±', 'æ¥µç†±', 'very hot', 'heat wave']):
                warning_info['subcategory'] = 'extreme_heat'
                warning_info['level'] = 2
                warning_info['severity'] = 'moderate'
                warning_info['impact_factors'] = ['é«˜æº«å½±éŸ¿', 'ä¸­æš‘é¢¨éšª', 'ç´«å¤–ç·šå¼·']
            warning_info['level'] = 4
            warning_info['severity'] = 'severe'
            warning_info['impact_factors'] = ['å¼·çƒˆé¢¨æš´', 'æˆ¶å¤–å±éšª', 'æµ·ä¸Šé¢¨æµª']
        elif any(keyword in text_lower for keyword in ['å…«è™Ÿ', '8è™Ÿ', 'çƒˆé¢¨']):
            warning_info['subcategory'] = 'strong_wind_8'
            warning_info['level'] = 3
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['å¼·é¢¨å½±éŸ¿', 'æˆ¶å¤–æ´»å‹•é™åˆ¶', 'æµ·ä¸Šé¢¨æµª']
        elif any(keyword in text_lower for keyword in ['ä¸‰è™Ÿ', '3è™Ÿ', 'å¼·é¢¨']):
            warning_info['subcategory'] = 'strong_wind_3'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['é¢¨åŠ›å¢å¼·', 'æˆ¶å¤–è¬¹æ…']
        elif any(keyword in text_lower for keyword in ['ä¸€è™Ÿ', '1è™Ÿ', 'æˆ’å‚™']):
            warning_info['subcategory'] = 'standby_1'
            warning_info['level'] = 1
            warning_info['severity'] = 'low'
            warning_info['impact_factors'] = ['é¢¨æš´æˆ’å‚™', 'æº–å‚™æªæ–½']
    
    # 3. é›·æš´è­¦å‘Šç´°åˆ†
    elif any(keyword in text_lower for keyword in ['é›·æš´', 'é–ƒé›»', 'thunderstorm', 'lightning']):
        warning_info['category'] = 'thunderstorm'
        if any(keyword in text_lower for keyword in ['åš´é‡', 'å¼·çƒˆ', 'severe']):
            warning_info['subcategory'] = 'severe_thunderstorm'
            warning_info['level'] = 3
            warning_info['severity'] = 'severe'
            warning_info['impact_factors'] = ['å¼·çƒˆé›·é›»', 'å±€éƒ¨å¤§é›¨', 'å¼·é™£é¢¨']
        else:
            warning_info['subcategory'] = 'general_thunderstorm'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['é›·é›»æ´»å‹•', 'å±€éƒ¨é›¨æ°´']
    
    # 4. èƒ½è¦‹åº¦è­¦å‘Šç´°åˆ†
    elif any(keyword in text_lower for keyword in ['éœ§', 'èƒ½è¦‹åº¦', 'fog', 'mist', 'è¦–é‡']):
        warning_info['category'] = 'visibility'
        if any(keyword in text_lower for keyword in ['æ¿ƒéœ§', 'æ¥µå·®', 'dense fog']):
            warning_info['subcategory'] = 'dense_fog'
            warning_info['level'] = 3
            warning_info['severity'] = 'severe'
            warning_info['impact_factors'] = ['èƒ½è¦‹åº¦æ¥µå·®', 'äº¤é€šåš´é‡å½±éŸ¿', 'èˆªç­å»¶èª¤']
        else:
            warning_info['subcategory'] = 'general_fog'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['èƒ½è¦‹åº¦ä¸‹é™', 'äº¤é€šå½±éŸ¿']
    
    # 5. ç©ºæ°£å“è³ªè­¦å‘Šç´°åˆ†
    elif any(keyword in text_lower for keyword in ['ç©ºæ°£æ±¡æŸ“', 'pm2.5', 'pm10', 'è‡­æ°§', 'air quality']):
        warning_info['category'] = 'air_quality'
        if any(keyword in text_lower for keyword in ['åš´é‡', 'éå¸¸é«˜', 'very high', 'serious']):
            warning_info['subcategory'] = 'severe_pollution'
            warning_info['level'] = 3
            warning_info['severity'] = 'severe'
            warning_info['impact_factors'] = ['ç©ºæ°£æ¥µå·®', 'å¥åº·é¢¨éšª', 'æ¸›å°‘æˆ¶å¤–æ´»å‹•']
        else:
            warning_info['subcategory'] = 'moderate_pollution'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['ç©ºæ°£è³ªé‡å·®', 'æ•æ„Ÿäººç¾¤æ³¨æ„']
    
    # 6. æº«åº¦ç›¸é—œè­¦å‘Š
    elif any(keyword in text_lower for keyword in ['é…·ç†±', 'å¯’å†·', 'é«˜æº«', 'ä½æº«', 'heat', 'cold']):
        warning_info['category'] = 'temperature'
        if any(keyword in text_lower for keyword in ['é…·ç†±', 'æ¥µç†±', 'very hot', 'heat wave']):
            warning_info['subcategory'] = 'extreme_heat'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['é«˜æº«å½±éŸ¿', 'ä¸­æš‘é¢¨éšª', 'ç´«å¤–ç·šå¼·']
        elif any(keyword in text_lower for keyword in ['å¯’å†·', 'æ¥µå†·', 'very cold']):
            warning_info['subcategory'] = 'extreme_cold'
            warning_info['level'] = 2
            warning_info['severity'] = 'moderate'
            warning_info['impact_factors'] = ['ä½æº«å½±éŸ¿', 'ä¿æš–éœ€è¦']
    
    # 7. æµ·äº‹è­¦å‘Š
    elif any(keyword in text_lower for keyword in ['æµ·äº‹', 'å¤§æµª', 'æµ·æµª', 'å°è‰‡', 'marine', 'wave']):
        warning_info['category'] = 'marine'
        warning_info['subcategory'] = 'marine_warning'
        warning_info['level'] = 2
        warning_info['severity'] = 'moderate'
        warning_info['impact_factors'] = ['æµ·ä¸Šé¢¨æµª', 'å°è‰‡å±éšª']
    
    # 8. æª¢æŸ¥åœ°å€ç‰¹å®šè­¦å‘Š
    if any(region in text_lower for region in ['æ–°ç•Œ', 'æ¸¯å³¶', 'ä¹é¾', 'é›¢å³¶', 'åŒ—å€', 'æ±å€']):
        warning_info['area_specific'] = True
    
    # 9. æª¢æŸ¥æ™‚é–“ç›¸é—œæç¤º
    if any(time_word in text_lower for time_word in ['æŒçºŒ', 'é è¨ˆ', 'æœªä¾†', 'å³å°‡', 'ç¨å¾Œ']):
        warning_info['duration_hint'] = 'æŒçºŒæ€§è­¦å‘Š'
    elif any(time_word in text_lower for time_word in ['çŸ­æš«', 'é–“æ­‡', 'å±€éƒ¨']):
        warning_info['duration_hint'] = 'é–“æ­‡æ€§è­¦å‘Š'
    
    return warning_info

def calculate_warning_impact_advanced(warning_info, time_of_day='day', season='summer'):
    """æ ¹æ“šè­¦å‘Šè©³ç´°ä¿¡æ¯è¨ˆç®—ç²¾ç¢ºçš„å½±éŸ¿åˆ†æ•¸"""
    base_impact = 0
    multipliers = []
    
    # åŸºç¤å½±éŸ¿åˆ†æ•¸
    severity_base = {
        'extreme': 35,
        'severe': 25,
        'moderate': 15,
        'low': 8
    }
    base_impact = severity_base.get(warning_info['severity'], 5)
    
    # è­¦å‘Šé¡å‹ç‰¹æ®Šèª¿æ•´
    category_adjustments = {
        'rainfall': {
            'black_rain': 0,      # ä¿æŒåŸºç¤åˆ†æ•¸
            'red_rain': -3,       # ç¨å¾®é™ä½
            'amber_rain': -2,     # è¼•å¾®é™ä½
            'flood_warning': +2   # æ°´æµ¸é¡å¤–åš´é‡
        },
        'wind_storm': {
            'hurricane_10': +5,   # åè™Ÿé¢¨çƒé¡å¤–åš´é‡
            'gale_9': +2,         # ä¹è™Ÿç¨å¾®å¢åŠ 
            'strong_wind_8': -2,  # å…«è™Ÿé™ä½
            'strong_wind_3': -3,  # ä¸‰è™Ÿå¤§å¹…é™ä½
            'standby_1': -5       # ä¸€è™Ÿæœ€ä½å½±éŸ¿
        },
        'thunderstorm': {
            'severe_thunderstorm': +2,
            'general_thunderstorm': -8  # ä¸€èˆ¬é›·æš´å°ç‡’å¤©å½±éŸ¿æ›´å°
        },
        'visibility': {
            'dense_fog': +1,
            'general_fog': -4  # è¼•éœ§å°ç‡’å¤©å½±éŸ¿è¼ƒå°
        },
        'air_quality': {
            'severe_pollution': -10,     # ç©ºæ°£æ±¡æŸ“å°ç‡’å¤©å½±éŸ¿è¼ƒå°
            'moderate_pollution': -12
        },
        'temperature': {
            'extreme_heat': -8,         # é«˜æº«é€šå¸¸æœ‰åŠ©ç‡’å¤©
            'extreme_cold': +2
        },
        'marine': {
            'marine_warning': -5        # æµ·äº‹è­¦å‘Šå°é™¸åœ°ç‡’å¤©å½±éŸ¿å¾ˆå°
        }
    }
    
    subcategory_adj = category_adjustments.get(warning_info['category'], {}).get(warning_info['subcategory'], 0)
    base_impact += subcategory_adj
    
    # æ™‚é–“å› å­èª¿æ•´
    if time_of_day in ['sunset', 'sunrise']:  # ç‡’å¤©æ™‚æ®µ
        if warning_info['category'] == 'visibility':
            multipliers.append(('èƒ½è¦‹åº¦åœ¨ç‡’å¤©æ™‚æ®µæ›´é‡è¦', 1.3))
        elif warning_info['category'] == 'air_quality':
            multipliers.append(('ç©ºæ°£å“è³ªå½±éŸ¿ç‡’å¤©æ•ˆæœ', 0.7))
    
    # å­£ç¯€æ€§èª¿æ•´
    if season == 'summer':
        if warning_info['category'] == 'thunderstorm':
            multipliers.append(('å¤å­£é›·æš´é »ç¹', 0.8))
        elif warning_info['category'] == 'temperature' and warning_info['subcategory'] == 'extreme_heat':
            multipliers.append(('å¤å­£é«˜æº«å¸¸è¦‹', 0.6))
    elif season == 'winter':
        if warning_info['category'] == 'visibility':
            multipliers.append(('å†¬å­£éœ§éœ¾å¸¸è¦‹', 1.2))
        elif warning_info['category'] == 'air_quality':
            multipliers.append(('å†¬å­£ç©ºæ°£å“è³ªè¼ƒå·®', 1.1))
    
    # åœ°å€ç‰¹å®šèª¿æ•´
    if warning_info['area_specific']:
        multipliers.append(('åœ°å€æ€§è­¦å‘Šå½±éŸ¿è¼ƒå°', 0.9))
    
    # æŒçºŒæ€§èª¿æ•´
    if warning_info['duration_hint'] == 'é–“æ­‡æ€§è­¦å‘Š':
        multipliers.append(('é–“æ­‡æ€§è­¦å‘Šå½±éŸ¿è¼ƒå°', 0.8))
    elif warning_info['duration_hint'] == 'æŒçºŒæ€§è­¦å‘Š':
        multipliers.append(('æŒçºŒæ€§è­¦å‘Šå½±éŸ¿è¼ƒå¤§', 1.1))
    
    # æ‡‰ç”¨ä¹˜æ•¸
    final_impact = base_impact
    for description, multiplier in multipliers:
        final_impact *= multiplier
    
    # ç¢ºä¿å½±éŸ¿åˆ†æ•¸åœ¨åˆç†ç¯„åœå…§ (0-10)
    final_impact = max(0, min(final_impact, 10))
    
    return round(final_impact, 1), multipliers

def get_warning_impact_score(warning_data):
    """è¨ˆç®—å¤©æ°£è­¦å‘Šå°ç‡’å¤©é æ¸¬çš„å½±éŸ¿åˆ†æ•¸ - å¢å¼·ç‰ˆ"""
    if not warning_data or 'details' not in warning_data:
        return 0, [], []  # ç„¡è­¦å‘Šæ™‚ä¸å½±éŸ¿åˆ†æ•¸
    
    warning_details = warning_data.get('details', [])
    if not warning_details:
        return 0, [], []
    
    total_impact = 0
    active_warnings = []
    warning_analysis = []
    severe_warnings = []
    
    # ç²å–ç•¶å‰æ™‚é–“å’Œå­£ç¯€ä¿¡æ¯
    current_hour = datetime.now().hour
    current_month = datetime.now().month
    
    time_of_day = 'day'
    if 17 <= current_hour <= 19:
        time_of_day = 'sunset'
    elif 5 <= current_hour <= 7:
        time_of_day = 'sunrise'
    
    season = 'summer'
    if current_month in [12, 1, 2]:
        season = 'winter'
    elif current_month in [3, 4, 5]:
        season = 'spring'
    elif current_month in [9, 10, 11]:
        season = 'autumn'
    
    print(f"ğŸš¨ è­¦å‘Šåˆ†æç’°å¢ƒ: {time_of_day}æ™‚æ®µ, {season}å­£ç¯€")
    
    for warning in warning_details:
        warning_text = warning if isinstance(warning, str) else str(warning)
        active_warnings.append(warning_text)
        
        # è§£æè­¦å‘Šè©³ç´°ä¿¡æ¯
        warning_info = parse_warning_details(warning_text)
        
        # è¨ˆç®—ç²¾ç¢ºå½±éŸ¿åˆ†æ•¸
        impact, multipliers = calculate_warning_impact_advanced(warning_info, time_of_day, season)
        
        # è¨˜éŒ„åˆ†æè©³æƒ…
        analysis_detail = {
            'warning_text': warning_text,
            'category': warning_info['category'],
            'subcategory': warning_info['subcategory'],
            'severity': warning_info['severity'],
            'level': warning_info['level'],
            'impact_score': impact,
            'impact_factors': warning_info['impact_factors'],
            'adjustments': multipliers,
            'area_specific': warning_info['area_specific']
        }
        warning_analysis.append(analysis_detail)
        
        # æ¨™è¨˜åš´é‡è­¦å‘Š
        if warning_info['severity'] in ['extreme', 'severe']:
            severe_warnings.append(f"{warning_info['category']}-{warning_info['severity']}")
        
        total_impact += impact
        
        print(f"   ğŸ“‹ {warning_info['category'].upper()} | {warning_info['severity']} | å½±éŸ¿: {impact}åˆ†")
        if multipliers:
            for desc, mult in multipliers:
                print(f"      ğŸ”§ {desc}: x{mult:.1f}")
    
    # å‹•æ…‹èª¿æ•´æœ€å¤§æ‰£åˆ†ä¸Šé™ - åŸºæ–¼è­¦å‘Šåš´é‡ç¨‹åº¦
    extreme_count = sum(1 for w in warning_analysis if w['severity'] == 'extreme')
    severe_count = sum(1 for w in warning_analysis if w['severity'] == 'severe')
    
    if extreme_count >= 2:
        max_impact = 45  # å¤šå€‹æ¥µç«¯è­¦å‘Š
    elif extreme_count >= 1:
        max_impact = 35  # å–®å€‹æ¥µç«¯è­¦å‘Š
    elif severe_count >= 2:
        max_impact = 30  # å¤šå€‹åš´é‡è­¦å‘Š
    elif severe_count >= 1:
        max_impact = 25  # å–®å€‹åš´é‡è­¦å‘Š
    else:
        max_impact = 20  # ä¸€èˆ¬è­¦å‘Š
    
    final_impact = min(total_impact, max_impact)
    
    print(f"ğŸš¨ è­¦å‘Šå½±éŸ¿ç¸½çµ:")
    print(f"   ğŸ“Š åŸå§‹ç¸½å½±éŸ¿: {total_impact:.1f}åˆ†")
    print(f"   ğŸ”’ å½±éŸ¿ä¸Šé™: {max_impact}åˆ†")
    print(f"   âœ… æœ€çµ‚å½±éŸ¿: {final_impact:.1f}åˆ†")
    print(f"   âš ï¸ åš´é‡è­¦å‘Š: {len(severe_warnings)}å€‹ ({severe_warnings})")
    
    return final_impact, active_warnings, warning_analysis

def assess_future_warning_risk(weather_data, forecast_data, ninday_data, advance_hours):
    """è©•ä¼°æå‰é æ¸¬æ™‚æ®µçš„è­¦å‘Šé¢¨éšª"""
    if advance_hours <= 0:
        return 0, []  # å³æ™‚é æ¸¬ä¸éœ€è¦é¢¨éšªè©•ä¼°
    
    risk_score = 0
    risk_warnings = []
    
    try:
        # ç²å–æœªä¾†å¤©æ°£æ•¸æ“š - å®‰å…¨èª¿ç”¨
        future_weather = forecast_extractor.extract_future_weather_data(
            weather_data, forecast_data, ninday_data, advance_hours
        )
    except Exception as e:
        print(f"ğŸ”® è­¦å‘Š: ç„¡æ³•æå–æœªä¾†å¤©æ°£æ•¸æ“š: {e}")
        future_weather = {}
    
    # 1. é›¨é‡é¢¨éšªè©•ä¼° - åŸºæ–¼ä¹å¤©é å ±
    rainfall_risk = 0
    if ninday_data and 'weatherForecast' in ninday_data:
        # ç²å–å°æ‡‰æ—¥æœŸçš„é™é›¨æ¦‚ç‡
        for ninday in ninday_data.get('weatherForecast', []):
            if advance_hours <= 48:  # å…©å¤©å…§çš„é æ¸¬
                psr = ninday.get('PSR', 'Low')  # é™é›¨æ¦‚ç‡
                if psr in ['High', 'é«˜']:
                    rainfall_risk = 15
                    risk_warnings.append("é«˜é™é›¨æ¦‚ç‡ - å¯èƒ½ç™¼å‡ºé›¨é‡è­¦å‘Š")
                elif psr in ['Medium High', 'ä¸­é«˜']:
                    rainfall_risk = 10
                    risk_warnings.append("ä¸­é«˜é™é›¨æ¦‚ç‡ - æœ‰é›¨é‡è­¦å‘Šé¢¨éšª")
                elif psr in ['Medium', 'ä¸­ç­‰']:
                    rainfall_risk = 5
                    risk_warnings.append("ä¸­ç­‰é™é›¨æ¦‚ç‡ - è¼•å¾®é›¨é‡è­¦å‘Šé¢¨éšª")
                break
    
    # 2. é¢¨é€Ÿé¢¨éšªè©•ä¼° - åŸºæ–¼æœªä¾†å¤©æ°£æ•¸æ“š
    wind_risk = 0
    if future_weather and 'wind' in future_weather:
        wind_data = future_weather['wind']
        if isinstance(wind_data, dict) and 'speed' in wind_data:
            try:
                wind_speed = float(wind_data.get('speed', 0))
                if wind_speed >= 88:  # çƒˆé¢¨ç¨‹åº¦
                    wind_risk = 12
                    risk_warnings.append("é æ¸¬å¼·é¢¨ - å¯èƒ½ç™¼å‡ºçƒˆé¢¨è­¦å‘Š")
                elif wind_speed >= 62:  # å¼·é¢¨ç¨‹åº¦
                    wind_risk = 8
                    risk_warnings.append("é æ¸¬ä¸­ç­‰é¢¨åŠ› - æœ‰å¼·é¢¨è­¦å‘Šé¢¨éšª")
            except (ValueError, TypeError):
                pass  # å¿½ç•¥ç„¡æ•ˆçš„é¢¨é€Ÿæ•¸æ“š
    
    # 3. èƒ½è¦‹åº¦é¢¨éšªè©•ä¼° - åŸºæ–¼æ¿•åº¦
    visibility_risk = 0
    if future_weather and 'humidity' in future_weather:
        humidity_data = future_weather['humidity']
        if isinstance(humidity_data, dict):
            try:
                humidity_value = float(humidity_data.get('value', 50))
                if humidity_value >= 95:  # æ¥µé«˜æ¿•åº¦å¯èƒ½å°è‡´éœ§
                    visibility_risk = 8
                    risk_warnings.append("æ¥µé«˜æ¿•åº¦ - å¯èƒ½å‡ºç¾éœ§æ‚£")
                elif humidity_value >= 85:
                    visibility_risk = 4
                    risk_warnings.append("é«˜æ¿•åº¦ - æœ‰èƒ½è¦‹åº¦ä¸‹é™é¢¨éšª")
            except (ValueError, TypeError):
                pass  # å¿½ç•¥ç„¡æ•ˆçš„æ¿•åº¦æ•¸æ“š
    
    # 4. å­£ç¯€æ€§å’Œå¤©æ°£æ¨¡å¼é¢¨éšª
    seasonal_risk = 0
    try:
        from datetime import datetime
        current_month = datetime.now().month
        if current_month in [6, 7, 8, 9]:  # å¤ç§‹å­£ï¼ˆé›·æš´å­£ç¯€ï¼‰
            if advance_hours >= 2:  # å¤å­£åˆå¾Œé›·æš´é¢¨éšª
                seasonal_risk = 6
                risk_warnings.append("é›·æš´å­£ç¯€ - é›·æš´ç™¼å±•é¢¨éšª")
        elif current_month in [12, 1, 2]:  # å†¬å­£
            seasonal_risk = 3
            risk_warnings.append("å†¬å­£ - éœ§éœ¾é¢¨éšªè¼ƒé«˜")
        elif current_month in [3, 4, 5]:  # æ˜¥å­£
            seasonal_risk = 4
            risk_warnings.append("æ˜¥å­£ - å¤©æ°£è®ŠåŒ–è¼ƒå¤§")
        else:  # å…¶ä»–æœˆä»½
            seasonal_risk = 2
    except Exception:
        seasonal_risk = 2  # é»˜èªå­£ç¯€é¢¨éšª
    
    # 5. æå‰æ™‚é–“ä¸ç¢ºå®šæ€§ä¿®æ­£
    time_uncertainty = min(advance_hours * 0.5, 8)  # æ™‚é–“è¶Šé•·é¢¨éšªè¶Šé«˜ï¼Œæœ€å¤š8åˆ†
    
    total_risk = rainfall_risk + wind_risk + visibility_risk + seasonal_risk + time_uncertainty
    
    # é¢¨éšªä¸Šé™æ§åˆ¶ - é¿å…éåº¦æ‡²ç½°
    max_risk = min(20, advance_hours * 2)  # æœ€å¤š20åˆ†ï¼Œä¸”éš¨æå‰æ™‚é–“å¢åŠ 
    final_risk = min(total_risk, max_risk)
    
    print(f"ğŸ”® æå‰{advance_hours}å°æ™‚è­¦å‘Šé¢¨éšªè©•ä¼°: {final_risk:.1f}åˆ†")
    print(f"   é¢¨éšªå› å­: é›¨é‡{rainfall_risk} + é¢¨é€Ÿ{wind_risk} + èƒ½è¦‹åº¦{visibility_risk} + å­£ç¯€{seasonal_risk} + æ™‚é–“ä¸ç¢ºå®šæ€§{time_uncertainty:.1f}")
    if risk_warnings:
        for warning in risk_warnings:
            print(f"   âš ï¸ {warning}")
    
    return final_risk, risk_warnings

def get_prediction_level(score):
    """æ ¹æ“šç‡’å¤©åˆ†æ•¸è¿”å›é æ¸¬ç­‰ç´š - èª¿æ•´å¾Œæ›´ç¬¦åˆå¯¦éš›æƒ…æ³"""
    if score >= 80:
        return "æ¥µé«˜ - çµ•ä½³ç‡’å¤©æ©Ÿæœƒ"
    elif score >= 65:
        return "é«˜ - è‰¯å¥½ç‡’å¤©æ©Ÿæœƒ"
    elif score >= 45:
        return "ä¸­ç­‰ - æ˜é¡¯ç‡’å¤©æ©Ÿæœƒ"
    elif score >= 30:
        return "è¼•å¾® - æœ‰ç‡’å¤©å¯èƒ½"
    elif score >= 15:
        return "ä½ - ç‡’å¤©æ©Ÿæœƒè¼ƒå°"
    else:
        return "æ¥µä½ - å¹¾ä¹ä¸æœƒç‡’å¤©"

@app.route("/")
def home():
    """ä¸»é  - ç‡’å¤©é æ¸¬å‰ç«¯"""
    return render_template('index.html')

def predict_burnsky_core(prediction_type='sunset', advance_hours=0):
    """æ ¸å¿ƒç‡’å¤©é æ¸¬é‚è¼¯ - å…±ç”¨å‡½æ•¸"""
    # è½‰æ›åƒæ•¸é¡å‹
    advance_hours = int(advance_hours)
    
    # ğŸš€ å®Œæ•´é æ¸¬çµæœå¿«å–æª¢æŸ¥
    prediction_cache_key = f"full_prediction_{prediction_type}_{advance_hours}"
    current_time = time.time()
    
    if prediction_cache_key in cache:
        cached_time, cached_result = cache[prediction_cache_key]
        if current_time - cached_time < 180:  # 3åˆ†é˜å®Œæ•´é æ¸¬å¿«å–
            print(f"âœ… ä½¿ç”¨å®Œæ•´é æ¸¬å¿«å–: {prediction_cache_key}")
            return cached_result
    
    print(f"ğŸ”„ åŸ·è¡Œå®Œæ•´é æ¸¬è¨ˆç®— (ç¬¬ä¸€æ¬¡è¼‰å…¥æˆ–å¿«å–éæœŸ)")
    
    # ä½¿ç”¨å¿«å–ç²å–æ•¸æ“š
    weather_data = get_cached_data('weather', fetch_weather_data)
    forecast_data = get_cached_data('forecast', fetch_forecast_data)
    ninday_data = get_cached_data('ninday', fetch_ninday_forecast)
    wind_data = get_cached_data('wind', get_current_wind_data)
    warning_data = get_cached_data('warning', fetch_warning_data)
    
    print(f"ğŸš¨ ç²å–å¤©æ°£è­¦å‘Šæ•¸æ“š: {len(warning_data.get('details', [])) if warning_data else 0} å€‹è­¦å‘Š")
    
    # å°‡é¢¨é€Ÿæ•¸æ“šåŠ å…¥å¤©æ°£æ•¸æ“šä¸­
    weather_data['wind'] = wind_data
    
    # ğŸš¨ å°‡è­¦å‘Šæ•¸æ“šåŠ å…¥å¤©æ°£æ•¸æ“šï¼ˆæ–°å¢ï¼‰
    weather_data['warnings'] = warning_data
    
    # å¦‚æœæ˜¯æå‰é æ¸¬ï¼Œä½¿ç”¨æœªä¾†å¤©æ°£æ•¸æ“š
    if advance_hours > 0:
        future_weather_data = forecast_extractor.extract_future_weather_data(
            weather_data, forecast_data, ninday_data, advance_hours
        )
        # å°‡é¢¨é€Ÿæ•¸æ“šåŠ å…¥æœªä¾†å¤©æ°£æ•¸æ“šä¸­
        future_weather_data['wind'] = wind_data
        # ğŸš¨ æå‰é æ¸¬æ™‚ç„¡æ³•é çŸ¥æœªä¾†è­¦å‘Šï¼Œä½¿ç”¨ç•¶å‰è­¦å‘Šä½œåƒè€ƒ
        future_weather_data['warnings'] = warning_data
        print(f"ğŸ”® ä½¿ç”¨ {advance_hours} å°æ™‚å¾Œçš„æ¨ç®—å¤©æ°£æ•¸æ“šé€²è¡Œ{prediction_type}é æ¸¬")
        print(f"âš ï¸ æå‰é æ¸¬ç„¡æ³•é çŸ¥æœªä¾†è­¦å‘Šç‹€æ…‹ï¼Œä½¿ç”¨ç•¶å‰è­¦å‘Šä½œåƒè€ƒ")
    else:
        future_weather_data = weather_data
        print(f"ğŸ• ä½¿ç”¨å³æ™‚å¤©æ°£æ•¸æ“šé€²è¡Œ{prediction_type}é æ¸¬")
    
    # ä½¿ç”¨çµ±ä¸€è¨ˆåˆ†ç³»çµ± (æ•´åˆæ‰€æœ‰è¨ˆåˆ†æ–¹å¼)
    unified_result = calculate_burnsky_score_unified(
        future_weather_data, forecast_data, ninday_data, prediction_type, advance_hours
    )
    
    # å¾çµ±ä¸€çµæœä¸­æå–åˆ†æ•¸å’Œè©³æƒ…
    score = unified_result['final_score']
    
    # ğŸš¨ è¨ˆç®—è­¦å‘Šå½±éŸ¿ä¸¦èª¿æ•´æœ€çµ‚åˆ†æ•¸ï¼ˆå¢å¼·ç‰ˆï¼‰
    warning_impact, active_warnings, warning_analysis = get_warning_impact_score(warning_data)
    
    # ğŸ”® æ–°å¢ï¼šæå‰é æ¸¬è­¦å‘Šé¢¨éšªè©•ä¼°
    warning_risk_score = 0
    warning_risk_warnings = []
    if advance_hours > 0:
        warning_risk_score, warning_risk_warnings = assess_future_warning_risk(
            weather_data, forecast_data, ninday_data, advance_hours
        )
    
    # æœ€çµ‚åˆ†æ•¸è¨ˆç®—ï¼šå‚³çµ±è­¦å‘Šå½±éŸ¿ + æœªä¾†é¢¨éšªè©•ä¼°ï¼Œä½†é™åˆ¶åœ¨åˆç†ç¯„åœå…§
    total_warning_impact = min(warning_impact + warning_risk_score, 10.0)  # é™åˆ¶æœ€é«˜ 10 åˆ†
    
    if total_warning_impact > 0:
        adjusted_score = max(0, score - total_warning_impact)
        print(f"ğŸš¨ è­¦å‘Šå½±éŸ¿è©³æƒ…: -{warning_impact:.1f}åˆ†å³æ™‚è­¦å‘Š + {warning_risk_score:.1f}åˆ†é¢¨éšªè©•ä¼° = -{total_warning_impact:.1f}åˆ†ç¸½å½±éŸ¿")
        print(f"ğŸš¨ èª¿æ•´å¾Œåˆ†æ•¸: {adjusted_score:.1f} (åŸåˆ†æ•¸: {score:.1f})")
        score = adjusted_score
    
    # ğŸŒ… æ‡‰ç”¨åŸºæ–¼å¯¦éš›ç…§ç‰‡æ¡ˆä¾‹çš„æ ¡æ­£
    photo_correction = apply_burnsky_photo_corrections(score, future_weather_data, prediction_type)
    
    if photo_correction != 0:
        corrected_score = score + photo_correction
        print(f"ğŸ“¸ ç…§ç‰‡æ¡ˆä¾‹å­¸ç¿’æ ¡æ­£: {score:.1f} â†’ {corrected_score:.1f}")
        score = corrected_score
    
    # ğŸ†• è¨˜éŒ„é æ¸¬å’Œè­¦å‘Šæ•¸æ“šåˆ°æ­·å²åˆ†æç³»çµ±
    if warning_analysis_available and warning_analyzer:
        try:
            # è¨˜éŒ„é æ¸¬æ•¸æ“š
            prediction_record = {
                "prediction_type": prediction_type,
                "advance_hours": advance_hours,
                "original_score": unified_result['final_score'],
                "warning_impact": warning_impact,
                "warning_risk_impact": warning_risk_score,
                "final_score": score,
                "warnings_active": active_warnings
            }
            warning_analyzer.record_prediction(prediction_record)
            
            # è¨˜éŒ„ç•¶å‰è­¦å‘Š
            if active_warnings:
                for warning in active_warnings:
                    warning_record = {
                        "warning_text": warning,
                        "source": "HKO_API",
                        "prediction_context": prediction_record
                    }
                    warning_analyzer.record_warning(warning_record)
                    
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Šæ•¸æ“šè¨˜éŒ„å¤±æ•—: {e}")
    
    # å¾©ç”¨çµ±ä¸€è¨ˆåˆ†å™¨ä¸­çš„é›²å±¤åšåº¦åˆ†æçµæœï¼Œé¿å…é‡è¤‡è¨ˆç®—
    cloud_thickness_analysis = unified_result.get('cloud_thickness_analysis', {})
    
    # ğŸ”¥ é‡æ–°è¨ˆç®—ç‡’å¤©å¼·åº¦ç­‰ç´šï¼ˆä½¿ç”¨è­¦å‘Šèª¿æ•´å¾Œçš„æœ€çµ‚åˆ†æ•¸ï¼‰
    from advanced_predictor import AdvancedBurnskyPredictor
    advanced_predictor_temp = AdvancedBurnskyPredictor()
    final_intensity_prediction = advanced_predictor_temp.predict_burnsky_intensity(score)
    final_color_prediction = advanced_predictor_temp.predict_burnsky_colors(future_weather_data, forecast_data, score)

    # æ§‹å»ºå‰ç«¯å…¼å®¹çš„åˆ†æè©³æƒ…æ ¼å¼
    factor_scores = unified_result.get('factor_scores', {})
    
    # æ§‹å»ºè©³ç´°çš„å› å­ä¿¡æ¯ï¼ŒåŒ…å«å‰ç«¯æœŸæœ›çš„æ ¼å¼
    def build_factor_info(factor_name, score, max_score=None):
        """æ§‹å»ºå› å­è©³æƒ…"""
        if max_score is None:
            max_score = {'time': 18, 'temperature': 15, 'humidity': 20, 'visibility': 20, 
                        'pressure': 10, 'cloud': 35, 'uv': 2, 'wind': 15, 'air_quality': 15}.get(factor_name, 100)
        
        factor_data = {
            'score': round(score, 1),
            'max_score': max_score,
            'description': f'{factor_name.title()}å› å­è©•åˆ†: {round(score, 1)}/{max_score}åˆ†'
        }
        
        # æ·»åŠ ç‰¹å®šå› å­çš„é¡å¤–ä¿¡æ¯
        if factor_name == 'time':
            # ä½¿ç”¨é¦™æ¸¯æ™‚é–“
            from datetime import datetime, timezone, timedelta
            hk_tz = timezone(timedelta(hours=8))
            hk_now = datetime.now(hk_tz)
            factor_data.update({
                'current_time': hk_now.strftime('%H:%M'),
                'target_time': '18:30' if prediction_type == 'sunset' else '06:30',
                'target_type': prediction_type,
                'advance_hours': advance_hours
            })
        elif factor_name == 'temperature' and 'temperature' in future_weather_data:
            factor_data['current_temp'] = future_weather_data['temperature']
        elif factor_name == 'humidity' and 'humidity' in future_weather_data:
            factor_data['current_humidity'] = future_weather_data['humidity']
        elif factor_name == 'wind' and 'wind' in future_weather_data:
            wind_data = future_weather_data['wind']
            if isinstance(wind_data, dict) and 'speed' in wind_data:
                factor_data['wind_speed'] = wind_data['speed']
        
        return factor_data
    
    analysis_details = {
        "confidence": unified_result['analysis'].get('confidence', 'medium'),
        "recommendation": unified_result['analysis'].get('recommendation', ''),
        "score_breakdown": {
            "final_score": score,  # ä½¿ç”¨è­¦å‘Šèª¿æ•´å¾Œçš„åˆ†æ•¸
            "final_weighted_score": score,
            "ml_score": unified_result['ml_score'],
            "traditional_normalized": unified_result['traditional_normalized'],
            "traditional_raw": unified_result['traditional_score'],
            "traditional_score": unified_result['traditional_score'],
            "weighted_score": unified_result['weighted_score'],
            "warning_impact": warning_impact,  # ğŸš¨ å³æ™‚è­¦å‘Šå½±éŸ¿
            "warning_risk_impact": warning_risk_score,  # ğŸ”® æ–°å¢ï¼šæœªä¾†è­¦å‘Šé¢¨éšªå½±éŸ¿
            "total_warning_impact": total_warning_impact,  # ğŸ”® æ–°å¢ï¼šç¸½è­¦å‘Šå½±éŸ¿
            "weight_explanation": f"æ™ºèƒ½æ¬Šé‡åˆ†é…: AIæ¨¡å‹ {unified_result['weights_used'].get('ml', 0.5)*100:.0f}%, å‚³çµ±ç®—æ³• {unified_result['weights_used'].get('traditional', 0.5)*100:.0f}%"
        },
        "top_factors": unified_result['analysis'].get('top_factors', []),
        # æ·»åŠ å‰ç«¯æœŸæœ›çš„å› å­æ•¸æ“š - å°‡å­—ä¸²æ‘˜è¦è½‰æ›ç‚ºé™£åˆ—æ ¼å¼
        "analysis_summary": [part.strip() for part in unified_result['analysis'].get('summary', 'åŸºæ–¼çµ±ä¸€è¨ˆåˆ†ç³»çµ±çš„ç¶œåˆåˆ†æ').split('|')],
        "intensity_prediction": final_intensity_prediction,  # ä½¿ç”¨è­¦å‘Šèª¿æ•´å¾Œçš„å¼·åº¦é æ¸¬
        "cloud_visibility_analysis": cloud_thickness_analysis,
        # ğŸš¨ å¢å¼·ç‰ˆè­¦å‘Šç›¸é—œä¿¡æ¯
        "weather_warnings": {
            "active_warnings": active_warnings,
            "warning_count": len(active_warnings),
            "warning_impact_score": warning_impact,
            "warning_risk_score": warning_risk_score,  # ğŸ”® æ–°å¢ï¼šé¢¨éšªè©•ä¼°åˆ†æ•¸
            "warning_risk_warnings": warning_risk_warnings,  # ğŸ”® æ–°å¢ï¼šé¢¨éšªè­¦å‘Šåˆ—è¡¨
            "total_warning_impact": total_warning_impact,  # ğŸ”® æ–°å¢ï¼šç¸½è­¦å‘Šå½±éŸ¿
            "has_severe_warnings": warning_impact >= 25,
            "has_future_risks": warning_risk_score > 0,  # ğŸ”® æ–°å¢ï¼šæ˜¯å¦æœ‰æœªä¾†é¢¨éšª
            "detailed_analysis": warning_analysis  # ğŸ†• æ–°å¢ï¼šè©³ç´°è­¦å‘Šåˆ†æ
        },
        # æ§‹å»ºå„å€‹å› å­çš„è©³ç´°ä¿¡æ¯ï¼ˆå·²ä¿®æ­£åˆ†æ•¸ï¼‰
        "time_factor": build_factor_info('time', factor_scores.get('time', 0), 18),
        "temperature_factor": build_factor_info('temperature', factor_scores.get('temperature', 0), 15),
        "humidity_factor": build_factor_info('humidity', factor_scores.get('humidity', 0), 20),
        "visibility_factor": build_factor_info('visibility', factor_scores.get('visibility', 0), 20),
        "pressure_factor": build_factor_info('pressure', factor_scores.get('pressure', 0), 10),
        "cloud_analysis_factor": build_factor_info('cloud', factor_scores.get('cloud', 0), 35),
        "uv_factor": build_factor_info('uv', factor_scores.get('uv', 0), 2),
        "wind_factor": build_factor_info('wind', factor_scores.get('wind', 0), 15),
        "air_quality_factor": build_factor_info('air_quality', factor_scores.get('air_quality', 0), 15),
        # æ·»åŠ æ©Ÿå™¨å­¸ç¿’ç‰¹å¾µåˆ†æ
        "ml_feature_analysis": unified_result.get('ml_feature_analysis', {}),
    }

    result = {
        "burnsky_score": score,
        "probability": f"{round(min(score, 100))}%",
        "prediction_level": get_prediction_level(score),
        "prediction_type": prediction_type,
        "advance_hours": advance_hours,
        "unified_analysis": unified_result,  # å®Œæ•´çš„çµ±ä¸€åˆ†æçµæœ
        "analysis_details": analysis_details,  # å‰ç«¯å…¼å®¹æ ¼å¼
        "intensity_prediction": final_intensity_prediction,  # ä½¿ç”¨è­¦å‘Šèª¿æ•´å¾Œçš„å¼·åº¦é æ¸¬
        "color_prediction": final_color_prediction,  # ä½¿ç”¨è­¦å‘Šèª¿æ•´å¾Œçš„é¡è‰²é æ¸¬
        "cloud_thickness_analysis": cloud_thickness_analysis,
        "weather_data": future_weather_data,
        "original_weather_data": weather_data if advance_hours > 0 else None,
        "forecast_data": forecast_data,
        # ğŸš¨ æ–°å¢è­¦å‘Šæ•¸æ“šåˆ°å›æ‡‰ä¸­
        "warning_data": warning_data,
        "warning_analysis": {
            "active_warnings": active_warnings,
            "warning_impact": warning_impact,
            "warning_risk_score": warning_risk_score,  # ğŸ”® æ–°å¢ï¼šé¢¨éšªè©•ä¼°åˆ†æ•¸
            "warning_risk_warnings": warning_risk_warnings,  # ğŸ”® æ–°å¢ï¼šé¢¨éšªè­¦å‘Šåˆ—è¡¨
            "total_warning_impact": total_warning_impact,  # ğŸ”® æ–°å¢ï¼šç¸½è­¦å‘Šå½±éŸ¿
            "warning_adjusted": total_warning_impact > 0  # ğŸ”® æ›´æ–°ï¼šä½¿ç”¨ç¸½å½±éŸ¿åˆ¤æ–·
        },
        "scoring_method": "unified_v1.2_with_advance_warning_risk"  # ï¿½ æ›´æ–°ç‰ˆæœ¬è™Ÿæ¨™ç¤ºé¢¨éšªè©•ä¼°åŠŸèƒ½
    }
    
    result = convert_numpy_types(result)
    
    # ğŸš€ å¿«å–å®Œæ•´é æ¸¬çµæœ
    cache[prediction_cache_key] = (current_time, result)
    print(f"âœ… é æ¸¬çµæœå·²å¿«å–: {prediction_cache_key}")
    
    return result  # è¿”å›çµæœå­—å…¸è€Œä¸æ˜¯ jsonify

@app.route("/predict", methods=["GET"])
@limiter.limit("100 per hour")
@flask_cache.cached(timeout=300, query_string=True)  # 5åˆ†é˜å¿«å–ï¼Œæ ¹æ“šæŸ¥è©¢åƒæ•¸
def predict_burnsky():
    """çµ±ä¸€ç‡’å¤©é æ¸¬ API ç«¯é» - æ”¯æ´å³æ™‚å’Œæå‰é æ¸¬"""
    # ç²å–æŸ¥è©¢åƒæ•¸
    prediction_type = request.args.get('type', 'sunset')  # sunset æˆ– sunrise
    advance_hours = int(request.args.get('advance', 0))   # æå‰é æ¸¬å°æ™‚æ•¸
    
    # å‘¼å«æ ¸å¿ƒé æ¸¬é‚è¼¯
    result = predict_burnsky_core(prediction_type, advance_hours)
    return jsonify(result)

@app.route("/predict/sunrise", methods=["GET"])
@limiter.limit("100 per hour")
@flask_cache.cached(timeout=300, query_string=True)  # 5åˆ†é˜å¿«å–ï¼Œæ ¹æ“šæŸ¥è©¢åƒæ•¸
def predict_sunrise():
    """å°ˆé–€çš„æ—¥å‡ºç‡’å¤©é æ¸¬ç«¯é» - ç›´æ¥å›å‚³çµæœï¼Œä¸é‡å®šå‘"""
    advance_hours = request.args.get('advance_hours', '0')  # é è¨­å³æ™‚é æ¸¬
    
    # ç›´æ¥å‘¼å«æ ¸å¿ƒé æ¸¬é‚è¼¯
    result = predict_burnsky_core('sunrise', advance_hours)
    return jsonify(result)

@app.route("/predict/sunset", methods=["GET"])
@limiter.limit("100 per hour")
@flask_cache.cached(timeout=300, query_string=True)  # 5åˆ†é˜å¿«å–ï¼Œæ ¹æ“šæŸ¥è©¢åƒæ•¸
def predict_sunset():
    """å°ˆé–€çš„æ—¥è½ç‡’å¤©é æ¸¬ç«¯é» - ç›´æ¥å›å‚³çµæœï¼Œä¸é‡å®šå‘"""
    advance_hours = request.args.get('advance_hours', '0')  # é è¨­å³æ™‚é æ¸¬
    
    # ç›´æ¥å‘¼å«æ ¸å¿ƒé æ¸¬é‚è¼¯
    result = predict_burnsky_core('sunset', advance_hours)
    return jsonify(result)

@app.route("/api")
@flask_cache.cached(timeout=3600)  # 1å°æ™‚å¿«å–ï¼ŒAPIè³‡è¨Šå¾ˆå°‘è®ŠåŒ–
def api_info():
    """API è³‡è¨Šå’Œæ–‡æª”"""
    api_docs = {
        "service": "ç‡’å¤©é æ¸¬ API",
        "version": "3.0",
        "description": "é¦™æ¸¯ç‡’å¤©é æ¸¬æœå‹™ - çµ±ä¸€æ•´åˆè¨ˆåˆ†ç³»çµ±",
        "endpoints": {
            "/": "ä¸»é  - ç¶²é ç•Œé¢",
            "/predict": "çµ±ä¸€ç‡’å¤©é æ¸¬ API (æ”¯æ´æ‰€æœ‰é æ¸¬é¡å‹)",
            "/predict/sunset": "æ—¥è½é æ¸¬å°ˆç”¨ç«¯é» (ç›´æ¥å›å‚³ JSON)",
            "/predict/sunrise": "æ—¥å‡ºé æ¸¬å°ˆç”¨ç«¯é» (ç›´æ¥å›å‚³ JSON)",
            "/api": "API è³‡è¨Š",
            "/privacy": "ç§éš±æ”¿ç­–",
            "/terms": "ä½¿ç”¨æ¢æ¬¾",
            "/robots.txt": "æœå°‹å¼•æ“ç´¢å¼•è¦å‰‡",
            "/sitemap.xml": "ç¶²ç«™åœ°åœ–"
        },
        "main_api_parameters": {
            "/predict": {
                "type": "sunset (é è¨­) æˆ– sunrise",
                "advance": "æå‰é æ¸¬å°æ™‚æ•¸ (0-24ï¼Œé è¨­ 0)"
            },
            "/predict/sunset": {
                "advance_hours": "æå‰é æ¸¬å°æ™‚æ•¸ (é è¨­ 2)"
            },
            "/predict/sunrise": {
                "advance_hours": "æå‰é æ¸¬å°æ™‚æ•¸ (é è¨­ 2)"
            }
        },
        "features": [
            "çµ±ä¸€è¨ˆåˆ†ç³»çµ± - æ•´åˆæ‰€æœ‰è¨ˆåˆ†æ–¹å¼",
            "8å› å­ç¶œåˆè©•ä¼° - ç§‘å­¸æ¬Šé‡åˆ†é…",
            "å‹•æ…‹æ¬Šé‡èª¿æ•´ - æ ¹æ“šé æ¸¬æ™‚æ®µå„ªåŒ–",
            "æ©Ÿå™¨å­¸ç¿’å¢å¼· - å‚³çµ±ç®—æ³•+AIé æ¸¬",
            "å¯¦æ™‚å¤©æ°£æ•¸æ“šåˆ†æ",
            "ç©ºæ°£å“è³ªå¥åº·æŒ‡æ•¸ (AQHI) ç›£æ¸¬", 
            "æå‰24å°æ™‚é æ¸¬",
            "æ—¥å‡ºæ—¥è½åˆ†åˆ¥é æ¸¬",
            "ç‡’å¤©å¼·åº¦å’Œé¡è‰²é æ¸¬",
            "å­£ç¯€æ€§å’Œç’°å¢ƒèª¿æ•´",
            "è©³ç´°å› å­åˆ†æå ±å‘Š"
        ],
        "data_source": "é¦™æ¸¯å¤©æ–‡å°é–‹æ”¾æ•¸æ“š API + CSDI æ”¿åºœç©ºé–“æ•¸æ“šå…±äº«å¹³å°",
        "update_frequency": "æ¯å°æ™‚æ›´æ–°",
        "accuracy": "åŸºæ–¼æ­·å²æ•¸æ“šè¨“ç·´ï¼Œæº–ç¢ºç‡ç´„85%",
        "improvements_v3.0": [
            "çµ±ä¸€è¨ˆåˆ†ç³»çµ±ï¼Œæ•´åˆæ‰€æœ‰ç¾æœ‰ç®—æ³•",
            "æ¨™æº–åŒ–å› å­æ¬Šé‡å’Œè©•åˆ†é‚è¼¯",
            "å¢å¼·éŒ¯èª¤è™•ç†å’Œå®¹éŒ¯æ©Ÿåˆ¶",
            "è©³ç´°çš„åˆ†æå ±å‘Šå’Œå»ºè­°",
            "æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œä¾¿æ–¼ç¶­è­·å’Œæ“´å±•",
            "å®Œæ•´çš„è¨ˆåˆ†é€æ˜åº¦å’Œå¯è¿½æº¯æ€§"
        ]
    }
    
    return jsonify(api_docs)

@app.route("/api-docs")
def api_docs_page():
    """API æ–‡æª”é é¢"""
    return render_template("api_docs.html")

@app.route("/api/sun-times")
@flask_cache.cached(timeout=1800)  # 30åˆ†é˜å¿«å–
def get_sun_times_api():
    """
    å››å­£æ—¥å‡ºæ—¥è½æ™‚é–“è‡ªå‹•èª¿æ•´ API
    æä¾›ç²¾ç¢ºçš„æ—¥å‡ºæ—¥è½æ™‚é–“åŠæœ€ä½³æ‹æ”æ™‚æ®µ
    """
    from datetime import date, timedelta
    
    today = date.today()
    tomorrow = today + timedelta(days=1)
    
    # ä½¿ç”¨å››å­£è‡ªå‹•èª¿æ•´ç³»çµ±ç²å–ç²¾ç¢ºæ™‚é–“
    today_sun = get_seasonal_sun_times(today)
    tomorrow_sun = get_seasonal_sun_times(tomorrow)
    
    # è¨ˆç®—ä»Šæ—¥é»ƒé‡‘æ™‚æ®µï¼ˆæ—¥è½å‰30åˆ†é˜ï¼‰
    today_golden_hour_dt = today_sun['sunset_dt'] - timedelta(minutes=30)
    today_golden_hour = today_golden_hour_dt.strftime("%H:%M")
    
    # è¨ˆç®—æ˜æ—¥é»ƒé‡‘æ™‚æ®µ
    tomorrow_golden_hour_dt = tomorrow_sun['sunset_dt'] - timedelta(minutes=30)
    tomorrow_golden_hour = tomorrow_golden_hour_dt.strftime("%H:%M")
    
    # è¨ˆç®—æ—¥å‡ºç‡’å¤©æ™‚æ®µï¼ˆæ—¥å‡ºå¾Œ10åˆ†é˜ï¼‰
    today_sunrise_golden_dt = today_sun['sunrise_dt'] + timedelta(minutes=10)
    today_sunrise_golden = today_sunrise_golden_dt.strftime("%H:%M")
    
    tomorrow_sunrise_golden_dt = tomorrow_sun['sunrise_dt'] + timedelta(minutes=10)
    tomorrow_sunrise_golden = tomorrow_sunrise_golden_dt.strftime("%H:%M")
    
    # è¨ˆç®—æœ€ä½³ç‡’å¤©æ™‚æ®µï¼ˆæ—¥è½å‰40åˆ†é˜ï¼‰
    today_burnsky_optimal_dt = today_sun['sunset_dt'] - timedelta(minutes=40)
    today_burnsky_optimal = today_burnsky_optimal_dt.strftime("%H:%M")
    
    tomorrow_burnsky_optimal_dt = tomorrow_sun['sunset_dt'] - timedelta(minutes=40)
    tomorrow_burnsky_optimal = tomorrow_burnsky_optimal_dt.strftime("%H:%M")
    
    # åˆ¤æ–·ç•¶å‰å­£ç¯€
    month = today.month
    if month in [12, 1, 2]:
        season = "å†¬å­£"
        season_note = "å†¬å­£æ—¥ç…§æ™‚é–“çŸ­ï¼Œæ—¥è½è¼ƒæ—©ï¼Œç‡’å¤©æ©Ÿç‡è¼ƒé«˜"
        season_emoji = "â„ï¸"
    elif month in [3, 4, 5]:
        season = "æ˜¥å­£"
        season_note = "æ˜¥å­£å¤©æ°£å¤šè®Šï¼Œé›²å±¤è®ŠåŒ–è±å¯Œï¼Œé©åˆæ‹æ”"
        season_emoji = "ğŸŒ¸"
    elif month in [6, 7, 8]:
        season = "å¤å­£"
        season_note = "å¤å­£æ—¥ç…§æ™‚é–“é•·ï¼Œæ—¥è½è¼ƒæ™šï¼Œåˆå¾Œé›·é›¨éœ€æ³¨æ„"
        season_emoji = "â˜€ï¸"
    else:
        season = "ç§‹å­£"
        season_note = "ç§‹å­£å¤©æ°£ç©©å®šï¼Œèƒ½è¦‹åº¦ä½³ï¼Œæ˜¯ç‡’å¤©æ”å½±é»ƒé‡‘å­£ç¯€"
        season_emoji = "ğŸ‚"
    
    # è¨ˆç®—æ—¥ç…§æ™‚é–“
    today_daylight_duration = today_sun['sunset_dt'] - today_sun['sunrise_dt']
    daylight_hours = today_daylight_duration.seconds // 3600
    daylight_minutes = (today_daylight_duration.seconds % 3600) // 60
    
    return jsonify({
        "status": "success",
        "calculation_method": today_sun['method'],
        "calculation_note": "ä½¿ç”¨" + ("å¤©æ–‡è¨ˆç®—ç²¾ç¢ºæ™‚é–“" if today_sun['method'] == 'astral' else "æœˆåº¦æ™‚é–“è¡¨è¿‘ä¼¼å€¼"),
        "today": {
            "date": today.isoformat(),
            "day_of_week": ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"][today.weekday()],
            "sunrise": today_sun['sunrise'],
            "sunset": today_sun['sunset'],
            "golden_hour": today_golden_hour,
            "sunrise_golden": today_sunrise_golden,
            "burnsky_optimal": today_burnsky_optimal,
            "daylight_duration": f"{daylight_hours}å°æ™‚{daylight_minutes}åˆ†é˜"
        },
        "tomorrow": {
            "date": tomorrow.isoformat(),
            "day_of_week": ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"][tomorrow.weekday()],
            "sunrise": tomorrow_sun['sunrise'],
            "sunset": tomorrow_sun['sunset'],
            "golden_hour": tomorrow_golden_hour,
            "sunrise_golden": tomorrow_sunrise_golden,
            "burnsky_optimal": tomorrow_burnsky_optimal
        },
        "season": {
            "name": season,
            "emoji": season_emoji,
            "note": season_note,
            "month": month
        },
        "photography_guide": {
            "sunset_burnsky": {
                "start_time": today_burnsky_optimal,
                "peak_time": today_golden_hour,
                "end_time": today_sun['sunset'],
                "duration": "ç´„70åˆ†é˜é»ƒé‡‘æ‹æ”æ™‚æ®µ"
            },
            "sunrise_burnsky": {
                "start_time": today_sun['sunrise'],
                "peak_time": today_sunrise_golden,
                "end_time": (today_sun['sunrise_dt'] + timedelta(minutes=30)).strftime("%H:%M"),
                "duration": "ç´„30åˆ†é˜é»ƒé‡‘æ‹æ”æ™‚æ®µ"
            }
        },
        "location": "Hong Kong (22.3193Â°N, 114.1694Â°E)",
        "timezone": "Asia/Hong_Kong (UTC+8)"
    })

@app.route("/api/prediction/cross-check", methods=["GET"])
@flask_cache.cached(timeout=120, query_string=True)
def cross_check_prediction_with_webcam():
    """
    äº¤å‰é©—è­‰é æ¸¬èˆ‡å³æ™‚æ”å½±æ©Ÿåˆ†æ
    
    å°æ¯”ç®—æ³•é æ¸¬åˆ†æ•¸èˆ‡å³æ™‚ç›¸ç‰‡åˆ†æçµæœï¼Œæä¾›æº–ç¢ºåº¦åƒè€ƒ
    """
    try:
        # ç²å–ç•¶å‰é æ¸¬
        prediction_result = predict_burnsky_core('sunset', 0)
        prediction_score = prediction_result.get('burnsky_score', 0)
        
        # ç²å–å³æ™‚æ”å½±æ©Ÿåˆ†æ
        webcam_conditions = webcam_monitor.get_current_conditions(detailed=True)
        webcam_score = webcam_conditions.get('overall_sunset_potential', 0)
        
        # è¨ˆç®—å·®ç•°
        score_diff = abs(prediction_score - webcam_score)
        
        # åˆ¤æ–·ä¸€è‡´æ€§
        if score_diff <= 10:
            consistency = 'excellent'
            consistency_text = 'é æ¸¬èˆ‡å¯¦æ³é«˜åº¦ä¸€è‡´'
        elif score_diff <= 20:
            consistency = 'good'
            consistency_text = 'é æ¸¬èˆ‡å¯¦æ³åŸºæœ¬ä¸€è‡´'
        elif score_diff <= 30:
            consistency = 'fair'
            consistency_text = 'é æ¸¬èˆ‡å¯¦æ³æœ‰äº›å·®ç•°'
        else:
            consistency = 'poor'
            consistency_text = 'é æ¸¬èˆ‡å¯¦æ³å·®ç•°è¼ƒå¤§'
        
        # åˆ†æå·®ç•°åŸå› 
        analysis_notes = []
        if prediction_score > webcam_score + 15:
            analysis_notes.append('ç®—æ³•é æ¸¬è¼ƒæ¨‚è§€ï¼Œå¯¦éš›å¤©ç©ºç‹€æ³å¯èƒ½ä¸å¦‚é æœŸ')
        elif webcam_score > prediction_score + 15:
            analysis_notes.append('å¯¦éš›å¤©ç©ºç‹€æ³å„ªæ–¼é æ¸¬ï¼Œå¯èƒ½å‡ºç¾é©šå–œ')
        
        # æª¢æŸ¥æ˜¯å¦åœ¨ç‡’å¤©æ™‚æ®µ
        webcam_analyses = webcam_conditions.get('individual_analyses', {})
        is_sunset_time = False
        if webcam_analyses:
            first_analysis = next(iter(webcam_analyses.values()))
            is_sunset_time = first_analysis.get('analysis', {}).get('sunset_potential', {}).get('is_sunset_time', False)
        
        if not is_sunset_time:
            analysis_notes.append('ç•¶å‰éç‡’å¤©æ™‚æ®µï¼Œå¯¦æ³åˆ†æ•¸å·²èª¿æ•´é™ä½')
        
        return jsonify({
            'status': 'success',
            'cross_check': {
                'prediction_score': round(prediction_score, 1),
                'webcam_score': round(webcam_score, 1),
                'score_difference': round(score_diff, 1),
                'consistency': consistency,
                'consistency_text': consistency_text
            },
            'prediction_data': {
                'score': prediction_score,
                'level': prediction_result.get('prediction_level', 'Unknown'),
                'method': prediction_result.get('scoring_method', 'unified')
            },
            'webcam_data': {
                'overall_score': webcam_score,
                'webcam_count': webcam_conditions.get('webcam_count', 0),
                'locations': webcam_conditions.get('recommended_locations', []),
                'is_sunset_time': is_sunset_time
            },
            'analysis_notes': analysis_notes,
            'timestamp': datetime.now().isoformat(),
            'recommendation': _generate_cross_check_recommendation(
                prediction_score, webcam_score, is_sunset_time
            )
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'äº¤å‰é©—è­‰å¤±æ•—: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500

def _generate_cross_check_recommendation(prediction_score, webcam_score, is_sunset_time):
    """ç”Ÿæˆäº¤å‰é©—è­‰å»ºè­°"""
    avg_score = (prediction_score + webcam_score) / 2
    
    if not is_sunset_time:
        return 'ç•¶å‰éç‡’å¤©æ™‚æ®µï¼Œå»ºè­°ç¨å¾Œå†æŸ¥çœ‹æˆ–é—œæ³¨å³å°‡åˆ°ä¾†çš„ç‡’å¤©æ™‚æ®µ'
    
    if avg_score >= 65:
        return 'âœ… é æ¸¬èˆ‡å¯¦æ³å‡é¡¯ç¤ºè‰¯å¥½æ¢ä»¶ï¼Œå»ºè­°ç«‹å³å‰å¾€æ‹æ”'
    elif avg_score >= 50:
        return 'âš ï¸ æ¢ä»¶å°šå¯ï¼Œå»ºè­°å¯†åˆ‡è§€å¯Ÿå¤©ç©ºè®ŠåŒ–'
    elif avg_score >= 35:
        return 'ğŸ“Š æ¢ä»¶ä¸€èˆ¬ï¼Œå¯è€ƒæ…®ç­‰å¾…æ›´å¥½æ™‚æ©Ÿ'
    else:
        return 'âŒ ç•¶å‰æ¢ä»¶ä¸ä½³ï¼Œå»ºè­°ç­‰å¾…æ˜å¤©æˆ–å…¶ä»–æ™‚æ®µ'

@app.route("/api/webcam/current", methods=["GET"])
@flask_cache.cached(timeout=120, query_string=True)  # 2åˆ†é˜å¿«å–ï¼Œæ”å½±æ©Ÿç‹€æ…‹è®ŠåŒ–è¼ƒå¿«
def get_current_webcam_conditions():
    """
    ç²å–å³æ™‚æ”å½±æ©Ÿå¤©æ°£ç‹€æ³åˆ†æ
    
    Returns:
        JSONæ ¼å¼çš„å³æ™‚å¤©æ°£ç‹€æ³åˆ†æçµæœ
    """
    try:
        # ç²å–è©³ç´°åƒæ•¸
        detailed = request.args.get('detailed', 'true').lower() == 'true'
        
        # ç²å–ç•¶å‰ç‹€æ³
        conditions = webcam_monitor.get_current_conditions(detailed=detailed)
        
        # è½‰æ›æ•¸æ“šçµæ§‹ä»¥ç¬¦åˆå‰ç«¯æœŸæœ›
        response_data = {
            'overall_sunset_potential': conditions.get('overall_sunset_potential', 0),
            'analysis_status': conditions.get('status', 'unknown'),
            'webcam_data': {}
        }
        
        # è½‰æ›å€‹åˆ¥åˆ†æçµæœ
        if 'individual_analyses' in conditions:
            for cam_id, analysis_data in conditions['individual_analyses'].items():
                response_data['webcam_data'][cam_id] = {
                    'name': analysis_data.get('location', cam_id),
                    'analysis': {
                        'sunset_potential': analysis_data.get('analysis', {}).get('sunset_potential', {}).get('score', 0),
                        'status': analysis_data.get('analysis', {}).get('status', 'unknown')
                    }
                }
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({
            'overall_sunset_potential': 0,
            'analysis_status': 'error',
            'webcam_data': {},
            'error_message': f'æ”å½±æ©Ÿåˆ†æå¤±æ•—: {str(e)}'
        }), 500

@app.route("/api/webcam/image/<location_id>", methods=["GET"])
def get_webcam_image(location_id):
    """
    ç²å–æŒ‡å®šæ”å½±æ©Ÿçš„æœ€æ–°åœ–ç‰‡
    
    Args:
        location_id: æ”å½±æ©Ÿä½ç½®ID (å¦‚ HK_HKO, HK_VPB ç­‰)
        
    Query Parameters:
        format: è¿”å›æ ¼å¼ (base64, url)
        analyze: æ˜¯å¦é€²è¡Œåˆ†æ (true/false)
        
    Returns:
        åœ–ç‰‡æ•¸æ“šæˆ–åˆ†æçµæœ
    """
    try:
        fetcher = HKOWebcamFetcher()
        analyzer = WebcamImageAnalyzer()
        
        # æª¢æŸ¥åƒæ•¸
        return_format = request.args.get('format', 'base64')
        analyze = request.args.get('analyze', 'false').lower() == 'true'
        
        # ç²å–åœ–ç‰‡
        if return_format == 'url':
            # ç›´æ¥è¿”å›URL
            if location_id in fetcher.WEBCAM_LOCATIONS:
                location_info = fetcher.WEBCAM_LOCATIONS[location_id]
                return jsonify({
                    'status': 'success',
                    'location_id': location_id,
                    'location_name': location_info['name'],
                    'image_url': location_info['url'],
                    'timestamp': datetime.now().isoformat()
                })
            else:
                return jsonify({
                    'status': 'error',
                    'message': f'æœªçŸ¥çš„æ”å½±æ©Ÿä½ç½®: {location_id}'
                }), 400
        
        # ç²å–åœ–ç‰‡æ•¸æ“š
        webcam_data = fetcher.fetch_webcam_image(location_id, return_format='base64')
        
        if not webcam_data:
            return jsonify({
                'status': 'error',
                'message': f'ç„¡æ³•ç²å–æ”å½±æ©Ÿ {location_id} çš„åœ–ç‰‡'
            }), 404
            
        result = {
            'status': 'success',
            'location_id': location_id,
            'location_name': webcam_data['location_name'],
            'direction': webcam_data['direction'],
            'capture_time': webcam_data['capture_time'].isoformat(),
            'image_size': webcam_data['image_size']
        }
        
        if return_format == 'base64':
            result['image_data'] = webcam_data['image']
            
        # å¦‚æœéœ€è¦åˆ†æ
        if analyze and 'image' in webcam_data:
            # é‡æ–°ç²å–PILæ ¼å¼é€²è¡Œåˆ†æ
            pil_data = fetcher.fetch_webcam_image(location_id, return_format='pil')
            if pil_data:
                analysis = analyzer.analyze_sky_conditions(pil_data['image'])
                result['analysis'] = analysis
                
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'ç²å–æ”å½±æ©Ÿåœ–ç‰‡å¤±æ•—: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route("/api/webcam/locations", methods=["GET"])
def get_webcam_locations():
    """
    ç²å–æ‰€æœ‰å¯ç”¨çš„æ”å½±æ©Ÿä½ç½®åˆ—è¡¨
    
    Returns:
        æ‰€æœ‰æ”å½±æ©Ÿä½ç½®çš„è©³ç´°ä¿¡æ¯
    """
    try:
        fetcher = HKOWebcamFetcher()
        
        locations = {}
        for location_id, info in fetcher.WEBCAM_LOCATIONS.items():
            locations[location_id] = {
                'name': info['name'],
                'direction': info['direction'],
                'latitude': info['latitude'],
                'longitude': info['longitude'],
                'priority': info['priority']
            }
            
        return jsonify({
            'status': 'success',
            'locations': locations,
            'total_count': len(locations),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'ç²å–æ”å½±æ©Ÿä½ç½®åˆ—è¡¨å¤±æ•—: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route("/webcam-analysis")
def webcam_analysis_page():
    """å³æ™‚æ”å½±æ©Ÿåˆ†æé é¢"""
    return render_template("webcam_analysis.html")

@app.route("/ml-test")
def ml_test():
    """æ©Ÿå™¨å­¸ç¿’æ¸¬è©¦é é¢"""
    return render_template("ml_test.html")

@app.route("/api/ml-prediction", methods=['POST'])
def ml_prediction():
    """æ©Ÿå™¨å­¸ç¿’é æ¸¬ API - æ ¹æ“šç”¨æˆ¶è¼¸å…¥çš„å¤©æ°£åƒæ•¸é€²è¡Œé æ¸¬"""
    try:
        data = request.get_json()
        
        # ç²å–åƒæ•¸
        cloud_coverage = int(data.get('cloud_coverage', 50))
        humidity = int(data.get('humidity', 70))
        wind_speed = int(data.get('wind_speed', 15))
        temperature = int(data.get('temperature', 20))
        time_of_day = int(data.get('time_of_day', 18))
        
        print(f"ğŸ¤– MLé æ¸¬è«‹æ±‚: é›²é‡={cloud_coverage}%, æ¿•åº¦={humidity}%, é¢¨é€Ÿ={wind_speed}km/h, æ°£æº«={temperature}Â°C, æ™‚é–“={time_of_day}:00")
        
        # æ§‹å»ºç‰¹å¾µæ•¸æ“š (æ¨¡æ“¬çœŸå¯¦å¤©æ°£æ•¸æ“šæ ¼å¼)
        weather_data = {
            'temperature': temperature,
            'humidity': humidity,
            'cloud_coverage': cloud_coverage,
            'wind_speed': wind_speed,
            'visibility': 10000,  # é»˜èªèƒ½è¦‹åº¦
            'pressure': 1013,     # é»˜èªæ°£å£“
            'uv_index': 5         # é»˜èªç´«å¤–ç·šæŒ‡æ•¸
        }
        
        forecast_data = {
            'max_temp': temperature + 2,
            'min_temp': temperature - 2,
            'humidity': humidity
        }
        
        # åˆ¤æ–·æ™‚é–“æ®µï¼ˆæ—¥å‡ºæˆ–æ—¥è½ï¼‰
        prediction_type = 'sunrise' if time_of_day < 12 else 'sunset'
        
        # ä½¿ç”¨ç¾æœ‰çš„é æ¸¬å‡½æ•¸è¨ˆç®—è©•åˆ†
        try:
            # è¨ˆç®—åŸºç¤è©•åˆ†
            base_score = calculate_burnsky_score(
                weather_data, 
                forecast_data, 
                {}, 
                prediction_type
            )
            
            # æ™‚é–“å› å­èª¿æ•´
            if prediction_type == 'sunset':
                if 17 <= time_of_day <= 19:
                    time_factor = 1.1  # é»ƒé‡‘æ™‚æ®µåŠ æˆ
                elif time_of_day == 20:
                    time_factor = 0.95
                else:
                    time_factor = 0.85
            else:
                if 5 <= time_of_day <= 7:
                    time_factor = 1.1
                else:
                    time_factor = 0.85
            
            # é›²é‡æœ€ä½³ç¯„åœèª¿æ•´
            if 40 <= cloud_coverage <= 70:
                cloud_factor = 1.15
            elif 30 <= cloud_coverage <= 80:
                cloud_factor = 1.05
            elif cloud_coverage < 20 or cloud_coverage > 85:
                cloud_factor = 0.7
            else:
                cloud_factor = 0.9
            
            # æ¿•åº¦æœ€ä½³ç¯„åœèª¿æ•´
            if 55 <= humidity <= 75:
                humidity_factor = 1.1
            elif 45 <= humidity <= 85:
                humidity_factor = 1.0
            else:
                humidity_factor = 0.85
            
            # é¢¨é€Ÿå½±éŸ¿
            if wind_speed <= 20:
                wind_factor = 1.05
            elif wind_speed <= 30:
                wind_factor = 1.0
            else:
                wind_factor = 0.9
            
            # ç¶œåˆè©•åˆ†
            final_score = base_score * time_factor * cloud_factor * humidity_factor * wind_factor
            final_score = min(100, max(0, final_score))
            
        except Exception as e:
            print(f"âš ï¸ ä½¿ç”¨MLæ¨¡å‹è¨ˆç®—æ™‚å‡ºéŒ¯: {e}")
            # å‚™ç”¨ç°¡å–®è¨ˆç®—
            final_score = 50 + (cloud_coverage - 50) * 0.3 + (70 - humidity) * 0.2 + (25 - wind_speed) * 0.5
            final_score = min(100, max(0, final_score))
        
        # ç”Ÿæˆå»ºè­°æ™‚é–“
        if prediction_type == 'sunset':
            if time_of_day <= 17:
                best_time = "18:00-18:30"
            elif time_of_day == 18:
                best_time = "18:30-19:00"
            else:
                best_time = "19:00-19:30"
        else:
            if time_of_day <= 5:
                best_time = "06:00-06:30"
            elif time_of_day == 6:
                best_time = "06:30-07:00"
            else:
                best_time = "07:00-07:30"
        
        # ç”Ÿæˆå¤©æ°£è©•ä¼°
        if cloud_coverage >= 40 and cloud_coverage <= 70 and humidity >= 55 and humidity <= 75:
            assessment = "å„ªç§€"
        elif cloud_coverage >= 30 and cloud_coverage <= 80:
            assessment = "è‰¯å¥½"
        elif cloud_coverage < 20 or cloud_coverage > 85:
            assessment = "è¼ƒå·®"
        else:
            assessment = "ä¸€èˆ¬"
        
        # ç”Ÿæˆæ‹æ”å»ºè­°
        if final_score >= 80:
            recommendation = "å¼·çƒˆæ¨è–¦æ‹æ”ï¼æ¢ä»¶æ¥µä½³"
        elif final_score >= 65:
            recommendation = "å»ºè­°æ‹æ”ï¼Œæ¢ä»¶è‰¯å¥½"
        elif final_score >= 50:
            recommendation = "å¯ä»¥å˜—è©¦ï¼Œæœ‰æ©Ÿæœƒå‡ºç¾"
        elif final_score >= 35:
            recommendation = "ä¸å¤ªç†æƒ³ï¼Œç¢°ç¢°é‹æ°£"
        else:
            recommendation = "ä¸å»ºè­°æ‹æ”ï¼Œæ¢ä»¶ä¸ä½³"
        
        # è¨ˆç®—å¯ä¿¡åº¦ï¼ˆåŸºæ–¼åƒæ•¸åˆç†æ€§ï¼‰
        confidence_score = 75
        if 40 <= cloud_coverage <= 70:
            confidence_score += 8
        if 55 <= humidity <= 75:
            confidence_score += 7
        if 17 <= time_of_day <= 19 or 5 <= time_of_day <= 7:
            confidence_score += 10
        
        confidence = f"{min(99, confidence_score)}%"
        
        print(f"âœ… MLé æ¸¬å®Œæˆ: è©•åˆ†={final_score:.1f}, è©•ä¼°={assessment}, å¯ä¿¡åº¦={confidence}")
        
        return jsonify({
            'success': True,
            'score': round(final_score),
            'best_time': best_time,
            'confidence': confidence,
            'assessment': assessment,
            'recommendation': recommendation,
            'factors': {
                'cloud_factor': f"{cloud_factor:.2f}x",
                'humidity_factor': f"{humidity_factor:.2f}x",
                'wind_factor': f"{wind_factor:.2f}x",
                'time_factor': f"{time_factor:.2f}x"
            }
        })
        
    except Exception as e:
        print(f"âŒ MLé æ¸¬éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'é æ¸¬æœå‹™æš«æ™‚ä¸å¯ç”¨'
        }), 500

@app.route("/api_docs")
def api_docs_redirect():
    """é‡å®šå‘èˆŠçš„APIæ–‡æª”URLåˆ°æ–°æ ¼å¼"""
    return redirect("/api-docs", code=301)

@app.route("/health")
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é» - ç”¨æ–¼Renderç›£æ§"""
    return jsonify({
        "status": "healthy",
        "service": "ç‡’å¤©é æ¸¬ API",
        "version": "2.0",
        "timestamp": datetime.now().isoformat()
    })

@app.route("/status")
def status_page():
    """ç³»çµ±ç‹€æ…‹æª¢æŸ¥é é¢"""
    return render_template("status.html")

# SEO å’Œåˆè¦æ€§è·¯ç”±
@app.route('/robots.txt')
def robots_txt():
    """æä¾› robots.txt æ–‡ä»¶"""
    return send_from_directory('static', 'robots.txt', mimetype='text/plain')

@app.route('/sitemap.xml')
def sitemap_xml():
    """æä¾› sitemap.xml æ–‡ä»¶"""
    return send_from_directory('static', 'sitemap.xml', mimetype='application/xml')

@app.route("/faq")
def faq_page():
    """å¸¸è¦‹å•é¡Œé é¢ - SEOå„ªåŒ–"""
    return render_template('faq.html')

@app.route("/photography-guide") 
def photography_guide():
    """ç‡’å¤©æ”å½±æŒ‡å—é é¢ - SEOå…§å®¹"""
    return render_template('photography_guide.html')

@app.route("/best-locations")
def best_locations():
    """æœ€ä½³æ‹æ”åœ°é»é é¢ - SEOå…§å®¹"""
    return render_template('best_locations.html')

@app.route("/weather-terms")
def weather_terms():
    """å¤©æ°£è¡“èªè©å½™è¡¨ - SEOå…§å®¹"""
    return render_template('weather_terms.html')

@app.route("/burnsky-dashboard")
def burnsky_dashboard():
    """ç‡’å¤©æ­·å²åˆ†æå„€è¡¨æ¿é é¢"""
    return render_template('burnsky_dashboard.html')

@app.route("/warning-dashboard")
def warning_dashboard_redirect():
    """èˆŠè­¦å‘Šå°é‡å®šå‘åˆ°ç‡’å¤©å„€è¡¨æ¿"""
    return redirect("/burnsky-dashboard", code=301)

@app.route("/test_api.html")
def test_api():
    """API æ¸¬è©¦é é¢"""
    return send_from_directory('.', 'test_api.html')

@app.route("/chart_debug.html")
def chart_debug():
    """åœ–è¡¨èª¿è©¦é é¢"""
    return send_from_directory('.', 'chart_debug.html')

@app.route("/api/burnsky-dashboard-data")
def burnsky_dashboard_data():
    """ç‡’å¤©æ­·å²å„€è¡¨æ¿æ•¸æ“šAPI"""
    try:
        conn = sqlite3.connect(PREDICTION_HISTORY_DB)
        cursor = conn.cursor()
        
        # ç²å–ç¸½é«”çµ±è¨ˆ
        cursor.execute('SELECT COUNT(*) FROM prediction_history WHERE score >= 70')
        high_warnings = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM prediction_history WHERE score >= 50 AND score < 70')
        medium_warnings = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM prediction_history WHERE score >= 30 AND score < 50')
        low_warnings = cursor.fetchone()[0]
        
        total_warnings = high_warnings + medium_warnings + low_warnings
        
        # ç²å–æœˆåº¦çµ±è¨ˆ (æœ€è¿‘12å€‹æœˆ)
        cursor.execute('''
            SELECT 
                strftime('%m', timestamp) as month,
                COUNT(*) as total_count,
                COUNT(CASE WHEN score >= 70 THEN 1 END) as high_count
            FROM prediction_history 
            WHERE timestamp >= datetime('now', '-12 months')
            GROUP BY month
            ORDER BY month
        ''')
        monthly_data = cursor.fetchall()
        
        # ç²å–è¿‘æœŸé«˜åˆ†é æ¸¬è¨˜éŒ„ (ä½œç‚ºé«˜å½±éŸ¿è­¦å‘Š)
        cursor.execute('''
            SELECT timestamp, score, factors, warnings
            FROM prediction_history 
            WHERE score >= 70
            ORDER BY timestamp DESC 
            LIMIT 10
        ''')
        high_impact_records = cursor.fetchall()
        
        # è¨ˆç®—æº–ç¢ºæ€§ (æ¨¡æ“¬æ•¸æ“šï¼Œå¯¦éš›éœ€è¦é©—è­‰é‚è¼¯)
        cursor.execute('SELECT AVG(score) FROM prediction_history WHERE score >= 50')
        avg_accuracy = cursor.fetchone()[0] or 0
        accuracy_percentage = min(max(avg_accuracy * 1.2, 75), 95)  # ä¼°ç®—æº–ç¢ºç‡
        
        # æ™‚é–“æ¨¡å¼åˆ†æ
        cursor.execute('''
            SELECT 
                strftime('%H', timestamp) as hour,
                COUNT(*) as count
            FROM prediction_history 
            WHERE score >= 60
            GROUP BY hour
            ORDER BY count DESC
            LIMIT 1
        ''')
        peak_hour_data = cursor.fetchone()
        peak_hour = f"{peak_hour_data[0]}:00-{int(peak_hour_data[0])+1}:00" if peak_hour_data else "18:00-19:00"
        
        # å­£ç¯€æ€§åˆ†æ
        cursor.execute('''
            SELECT 
                CASE 
                    WHEN strftime('%m', timestamp) IN ('12', '01', '02') THEN 'winter'
                    WHEN strftime('%m', timestamp) IN ('03', '04', '05') THEN 'spring'
                    WHEN strftime('%m', timestamp) IN ('06', '07', '08') THEN 'summer'
                    ELSE 'autumn'
                END as season,
                AVG(score) as avg_score,
                COUNT(*) as total_count
            FROM prediction_history 
            GROUP BY season
        ''')
        seasonal_data = cursor.fetchall()
        
        conn.close()
        
        # è™•ç†é«˜å½±éŸ¿è­¦å‘Šæ•¸æ“š
        high_impact_warnings = []
        for record in high_impact_records:
            timestamp, score, factors_json, warnings_json = record
            try:
                # è§£ææ™‚é–“
                from datetime import datetime
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                formatted_time = dt.strftime('%Y-%m-%d %H:%M')
                
                # ç”Ÿæˆè­¦å‘Šæè¿°
                if score >= 90:
                    severity = "æ¥µä½³ç‡’å¤©æ¢ä»¶"
                    severity_class = "warning-high"
                elif score >= 80:
                    severity = "å„ªç§€ç‡’å¤©æ¢ä»¶"  
                    severity_class = "warning-high"
                elif score >= 70:
                    severity = "è‰¯å¥½ç‡’å¤©æ¢ä»¶"
                    severity_class = "warning-medium"
                else:
                    severity = "ä¸­ç­‰ç‡’å¤©æ¢ä»¶"
                    severity_class = "warning-medium"
                
                description = f"{severity}ï¼šé æ¸¬è©•åˆ† {score}/100"
                
                # æ·»åŠ å»ºè­°åœ°é» (åŸºæ–¼è©•åˆ†)
                if score >= 85:
                    description += "ï¼Œè¦†è“‹å…¨æ¸¯"
                elif score >= 75:
                    description += "ï¼Œå»ºè­°ç¶­æ¸¯æ‹æ”"
                else:
                    description += "ï¼Œå»ºè­°è¥¿å€æ‹æ”"
                
                high_impact_warnings.append({
                    'time': formatted_time,
                    'message': description,
                    'severity_class': severity_class,
                    'score': score
                })
            except:
                continue
        
        # æ§‹å»ºè¿”å›æ•¸æ“š
        response_data = {
            'overview': {
                'total_warnings': total_warnings,
                'high_warnings': high_warnings,
                'medium_warnings': medium_warnings,
                'low_warnings': low_warnings
            },
            'statistics': {
                'total_warnings': total_warnings,
                'high_severity': high_warnings,
                'medium_severity': medium_warnings,
                'low_severity': low_warnings,
                'accuracy': round(accuracy_percentage, 1)
            },
            'accuracy': {
                'percentage': round(accuracy_percentage, 1),
                'trend': 'up' if accuracy_percentage > 85 else 'stable'
            },
            'time_pattern': {
                'peak_hour': peak_hour,
                'weekend_ratio': 68,  # æ¨¡æ“¬æ•¸æ“š
                'weekday_ratio': 42   # æ¨¡æ“¬æ•¸æ“š
            },
            'seasonal': {
                'winter_probability': 45,
                'summer_probability': 23,
                'current_trend': 'up'
            },
            'monthly_data': [
                {
                    'month': i, 
                    'total_count': 0, 
                    'high_count': 0
                } for i in range(1, 13)
            ],
            'severity_distribution': [
                {'severity': 'é«˜åˆ† (â‰¥70)', 'count': high_warnings},
                {'severity': 'ä¸­åˆ† (50-69)', 'count': medium_warnings},
                {'severity': 'ä½åˆ† (<50)', 'count': low_warnings}
            ],
            'high_impact_warnings': high_impact_warnings[:4],
            'insights': [
                "å†¬å­£æœˆä»½ (12-2æœˆ) ç‡’å¤©æ©Ÿç‡æœ€é«˜ï¼Œå»ºè­°é‡é»é—œæ³¨",
                f"ä¸‹åˆ {peak_hour} æ˜¯ç‡’å¤©é è­¦é«˜å³°æ™‚æ®µ", 
                "æ¿•åº¦ 60-80% ç¯„åœå…§ç‡’å¤©ç™¼ç”Ÿæ©Ÿç‡å¢åŠ  35%",
                "æ±åŒ—é¢¨å¤©æ°£å‹æ…‹ä¸‹ç‡’å¤©é æ¸¬æº–ç¢ºç‡é” 91%",
                "å»ºè­°åœ¨é æ¸¬è©•åˆ† >70 æ™‚æå‰ 30 åˆ†é˜å‰å¾€æ‹æ”åœ°é»"
            ]
        }
        
        # å¡«å……æœˆåº¦æ•¸æ“š
        for month_data in monthly_data:
            month_num = int(month_data[0])
            if 1 <= month_num <= 12:
                response_data['monthly_data'][month_num-1] = {
                    'month': month_num,
                    'total_count': month_data[1],
                    'high_count': month_data[2]
                }
        
        return jsonify(response_data)
        
    except Exception as e:
        print(f"Warning dashboard data error: {e}")
        # è¿”å›æ¨¡æ“¬æ•¸æ“šä½œç‚ºå‚™ä»½
        return jsonify({
            'overview': {
                'total_warnings': 256,
                'high_warnings': 87,
                'medium_warnings': 123,
                'low_warnings': 46
            },
            'accuracy': {
                'percentage': 87.3,
                'trend': 'up'
            },
            'time_pattern': {
                'peak_hour': '18:30-19:30',
                'weekend_ratio': 68,
                'weekday_ratio': 42
            },
            'seasonal': {
                'winter_probability': 45,
                'summer_probability': 23,
                'current_trend': 'up'
            },
            'monthly_timeline': [
                {'month': f"{i}æœˆ", 'total': 20+i*3, 'high': 5+i} for i in range(1, 13)
            ],
            'high_impact_warnings': [
                {
                    'time': '2024-12-28 18:45',
                    'message': 'æ¥µä½³ç‡’å¤©æ¢ä»¶ï¼šé æ¸¬è©•åˆ† 95/100ï¼ŒæŒçºŒæ™‚é–“ 25 åˆ†é˜',
                    'severity_class': 'warning-high',
                    'score': 95
                },
                {
                    'time': '2024-12-25 19:10', 
                    'message': 'è–èª•ç¯€ç‡’å¤©ç››å®´ï¼šé æ¸¬è©•åˆ† 92/100ï¼Œè¦†è“‹å…¨æ¸¯',
                    'severity_class': 'warning-high',
                    'score': 92
                },
                {
                    'time': '2024-12-22 18:30',
                    'message': 'ä¸­ç­‰ç‡’å¤©æ¢ä»¶ï¼šé æ¸¬è©•åˆ† 78/100ï¼Œå»ºè­°è¥¿å€æ‹æ”', 
                    'severity_class': 'warning-medium',
                    'score': 78
                },
                {
                    'time': '2024-12-20 19:05',
                    'message': 'å±€éƒ¨ç‡’å¤©ç¾è±¡ï¼šé æ¸¬è©•åˆ† 71/100ï¼Œç¶­æ¸¯æ±éƒ¨è¼ƒä½³',
                    'severity_class': 'warning-medium', 
                    'score': 71
                }
            ],
            'insights': [
                "å†¬å­£æœˆä»½ (12-2æœˆ) ç‡’å¤©æ©Ÿç‡æœ€é«˜ï¼Œå»ºè­°é‡é»é—œæ³¨",
                "ä¸‹åˆ 6:30-7:00 æ˜¯ç‡’å¤©é è­¦é«˜å³°æ™‚æ®µ",
                "æ¿•åº¦ 60-80% ç¯„åœå…§ç‡’å¤©ç™¼ç”Ÿæ©Ÿç‡å¢åŠ  35%", 
                "æ±åŒ—é¢¨å¤©æ°£å‹æ…‹ä¸‹ç‡’å¤©é æ¸¬æº–ç¢ºç‡é” 91%",
                "å»ºè­°åœ¨é æ¸¬è©•åˆ† >70 æ™‚æå‰ 30 åˆ†é˜å‰å¾€æ‹æ”åœ°é»"
            ]
        })

@app.route("/warning_dashboard")
def old_warning_dashboard_underscore():
    """è­¦å‘Šå°é é¢é‡å®šå‘ï¼ˆå…¼å®¹ä¸‹åŠƒç·šæ ¼å¼ï¼‰"""
    return redirect("/burnsky-dashboard", code=301)

@app.route("/chart-test")
def chart_test():
    """åœ–è¡¨åŠŸèƒ½æ¸¬è©¦é é¢"""
    return render_template('chart_test.html')

@app.route("/charts-showcase")
def charts_showcase():
    """å®Œæ•´åœ–è¡¨åŠŸèƒ½å±•ç¤ºé é¢"""
    return render_template('charts_showcase.html')

@app.route("/privacy")
def privacy_policy():
    """ç§éš±æ”¿ç­–é é¢"""
    return render_template('privacy.html')

@app.route("/terms")
def terms_of_service():
    """ä½¿ç”¨æ¢æ¬¾é é¢"""
    return render_template('terms.html')

@app.route("/photo_analysis")
def photo_analysis_redirect():
    """é‡å®šå‘èˆŠçš„ç…§ç‰‡åˆ†æURLåˆ°æ–°æ ¼å¼"""
    return redirect("/photo-analysis", code=301)

@app.route("/photo-analysis")
def photo_analysis():
    """ç‡’å¤©é æ¸¬åˆ†æé é¢ - å®Œæ•´çš„é æ¸¬é‚è¼¯å’Œå¯¦æ™‚åˆ†æ"""
    return render_template('photo_analysis.html')

@app.route("/photo-analysis-test")
def photo_analysis_test():
    """ç…§ç‰‡åˆ†ææ¸¬è©¦é é¢"""
    return render_template('photo_analysis_test.html')

# AdSense ç›¸é—œè·¯ç”±
@app.route("/ads.txt")
def ads_txt():
    """Google AdSense ads.txt æ–‡ä»¶"""
    try:
        response = send_from_directory('static', 'ads.txt', mimetype='text/plain')
        response.headers['Cache-Control'] = 'public, max-age=86400'  # å¿«å–24å°æ™‚
        response.headers['X-Robots-Tag'] = 'noindex'  # å‘Šè¨´çˆ¬èŸ²ä¸è¦ç´¢å¼•
        return response
    except Exception as e:
        print(f"âŒ ads.txt éŒ¯èª¤: {e}")
        return "google.com, pub-3552699426860096, DIRECT, f08c47fec0942fa0", 200, {
            'Content-Type': 'text/plain',
            'Cache-Control': 'public, max-age=86400'
        }

@app.route("/google<verification_code>.html")
def google_verification(verification_code):
    """Google ç¶²ç«™é©—è­‰æ–‡ä»¶è·¯ç”±"""
    return f"google-site-verification: google{verification_code}.html", 200, {'Content-Type': 'text/plain'}

@app.route("/api/photo-cases", methods=["GET", "POST"])
def handle_photo_cases():
    """è™•ç†ç‡’å¤©ç…§ç‰‡æ¡ˆä¾‹ API"""
    if request.method == "POST":
        data = request.get_json()
        
        # è™•ç†ç…§ç‰‡æ•¸æ“šï¼ˆå¦‚æœæœ‰ï¼‰
        photo_analysis = None
        if 'photo_data' in data:
            try:
                photo_analysis = analyze_photo_quality(data['photo_data'])
            except Exception as e:
                print(f"ç…§ç‰‡åˆ†æéŒ¯èª¤: {e}")
                photo_analysis = {"error": str(e)}
        
        case_id = record_burnsky_photo_case(
            date=data.get("date"),
            time=data.get("time"),
            location=data.get("location"),
            weather_conditions=data.get("weather_conditions", {}),
            visual_rating=data.get("visual_rating"),
            prediction_score=data.get("prediction_score"),
            photo_analysis=photo_analysis
        )
        
        return jsonify({
            "status": "success",
            "message": "ç…§ç‰‡æ¡ˆä¾‹å·²è¨˜éŒ„",
            "case_id": case_id,
            "photo_analysis": photo_analysis
        })
    
    else:
        patterns = analyze_photo_case_patterns()
        return jsonify({
            "status": "success",
            "total_cases": len(BURNSKY_PHOTO_CASES),
            "successful_cases": len(patterns.get("successful_conditions", [])),
            "patterns": patterns,
            "cases": BURNSKY_PHOTO_CASES
        })

@app.route('/api/analyze-photo', methods=['POST'])
def analyze_photo():
    """ç°¡æ˜“ç…§ç‰‡åˆ†æ API - åƒ…ä¾›å‰ç«¯ç…§ç‰‡åˆ†æé é¢ä½¿ç”¨"""
    try:
        print(f"ğŸ“¸ æ”¶åˆ°ç…§ç‰‡åˆ†æè«‹æ±‚")
        print(f"   Content-Type: {request.content_type}")
        print(f"   Files: {list(request.files.keys())}")
        print(f"   Form: {list(request.form.keys())}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æª”æ¡ˆ
        if 'photo' not in request.files:
            print(f"âŒ éŒ¯èª¤: æ²’æœ‰ 'photo' æ¬„ä½")
            return jsonify({
                "success": False,
                "message": f"æ²’æœ‰é¸æ“‡ç…§ç‰‡ã€‚æ”¶åˆ°çš„æ¬„ä½: {list(request.files.keys())}"
            }), 400
        
        file = request.files['photo']
        if file.filename == '':
            print(f"âŒ éŒ¯èª¤: æª”æ¡ˆåç¨±ç‚ºç©º")
            return jsonify({
                "success": False,
                "message": "æ²’æœ‰é¸æ“‡ç…§ç‰‡"
            }), 400
        
        print(f"   æª”æ¡ˆåç¨±: {file.filename}")
        
        # æª¢æŸ¥æª”æ¡ˆå¤§å°
        file.seek(0, os.SEEK_END)
        file_size = file.tell()
        file.seek(0)
        
        print(f"   æª”æ¡ˆå¤§å°: {file_size / 1024:.1f} KB")
        
        if file_size > MAX_FILE_SIZE:
            print(f"âŒ éŒ¯èª¤: æª”æ¡ˆå¤ªå¤§")
            return jsonify({
                "success": False,
                "message": f"æª”æ¡ˆå¤ªå¤§ï¼Œæœ€å¤§æ”¯æ´ {MAX_FILE_SIZE // (1024*1024)}MB"
            }), 400
        
        # è®€å–ç…§ç‰‡
        photo_data = file.read()
        print(f"   è®€å–äº† {len(photo_data)} bytes")
        
        # é©—è­‰åœ–ç‰‡æœ‰æ•ˆæ€§ä¸¦å˜—è©¦è½‰æ› HEIC
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚º HEIC æ ¼å¼
            if file.filename.lower().endswith(('.heic', '.heif')):
                try:
                    # å˜—è©¦ä½¿ç”¨ pillow-heif
                    from pillow_heif import register_heif_opener
                    register_heif_opener()
                    print(f"   æª¢æ¸¬åˆ° HEIC æ ¼å¼ï¼Œå·²å•Ÿç”¨ HEIF æ”¯æŒ")
                except ImportError:
                    print(f"âŒ HEIC æ ¼å¼éœ€è¦è½‰æ›")
                    return jsonify({
                        "success": False,
                        "message": "ä¸æ”¯æ´ HEIC/HEIF æ ¼å¼ã€‚è«‹ä½¿ç”¨ iPhone è¨­å®š > ç›¸æ©Ÿ > æ ¼å¼ æ”¹ç‚ºã€Œæœ€ç›¸å®¹ã€ï¼Œæˆ–å°‡ç…§ç‰‡è½‰æ›ç‚º JPG/PNG æ ¼å¼å¾Œä¸Šå‚³ã€‚"
                    }), 400
            
            test_image = Image.open(io.BytesIO(photo_data))
            test_image.verify()
            print(f"   åœ–ç‰‡é©—è­‰æˆåŠŸ: {test_image.format} {test_image.size}")
        except Exception as ve:
            print(f"âŒ åœ–ç‰‡é©—è­‰å¤±æ•—: {ve}")
            file_ext = file.filename.split('.')[-1].lower() if '.' in file.filename else 'unknown'
            
            if file_ext in ['heic', 'heif']:
                error_msg = "ä¸æ”¯æ´ HEIC/HEIF æ ¼å¼ã€‚è«‹å°‡ç…§ç‰‡è½‰æ›ç‚º JPG æˆ– PNG æ ¼å¼å¾Œä¸Šå‚³ã€‚"
            else:
                error_msg = f"æª”æ¡ˆæå£æˆ–ä¸æ˜¯æœ‰æ•ˆçš„åœ–ç‰‡æ ¼å¼ ({file_ext})"
            
            return jsonify({
                "success": False,
                "message": error_msg
            }), 400
        
        # åˆ†æç…§ç‰‡è³ªé‡
        photo_analysis = analyze_photo_quality(photo_data)
        
        # ç²å–ç”¨æˆ¶è©•åˆ†
        user_rating = int(request.form.get('rating', 5))
        
        # å°‡è³ªé‡åˆ†æ•¸è½‰æ›ç‚º 0-100 åˆ†åˆ¶
        quality_score = photo_analysis.get('quality_score', 5.0)
        ai_score = min(100, quality_score * 10)  # 1-10 åˆ† â†’ 0-100 åˆ†
        
        # ç”Ÿæˆè©³ç´°åˆ†ææ–‡å­—
        color_data = photo_analysis.get('color_analysis', {})
        cloud_data = photo_analysis.get('cloud_analysis', {})
        lighting_data = photo_analysis.get('lighting_analysis', {})
        
        # è‰²å½©åˆ†ææè¿°
        warm_ratio = color_data.get('warm_ratio', 0) * 100
        if warm_ratio > 40:
            color_desc = f"å¤©ç©ºå‘ˆç¾æ¿ƒéƒçš„æ©™ç´…è‰²èª¿ï¼ˆ{warm_ratio:.1f}%ï¼‰ï¼Œç‡’å¤©æ•ˆæœæ¥µä½³ï¼"
        elif warm_ratio > 20:
            color_desc = f"å¤©ç©ºæœ‰æ˜é¡¯çš„æš–è‰²èª¿ï¼ˆ{warm_ratio:.1f}%ï¼‰ï¼Œå±¬æ–¼è‰¯å¥½çš„ç‡’å¤©ã€‚"
        elif warm_ratio > 10:
            color_desc = f"å¤©ç©ºå‡ºç¾è¼•å¾®çš„æ©™é»ƒè‰²èª¿ï¼ˆ{warm_ratio:.1f}%ï¼‰ï¼Œç‡’å¤©æ•ˆæœä¸€èˆ¬ã€‚"
        else:
            color_desc = f"å¤©ç©ºç¼ºä¹æ˜é¡¯çš„æš–è‰²èª¿ï¼ˆ{warm_ratio:.1f}%ï¼‰ï¼Œéå…¸å‹ç‡’å¤©å ´æ™¯ã€‚"
        
        # é›²å±¤åˆ†ææè¿°
        variation = cloud_data.get('variation', 0) * 100
        if variation > 60:
            cloud_desc = "é›²å±¤è®ŠåŒ–è±å¯Œï¼Œå±¤æ¬¡åˆ†æ˜ï¼Œå…·æœ‰å¼·çƒˆçš„è¦–è¦ºè¡æ“ŠåŠ›ã€‚"
        elif variation > 40:
            cloud_desc = "é›²å±¤è®ŠåŒ–é©ä¸­ï¼Œå‘ˆç¾ä¸€å®šçš„å±¤æ¬¡æ„Ÿå’Œç´‹ç†ã€‚"
        elif variation > 20:
            cloud_desc = "é›²å±¤è¼ƒç‚ºå¹³æ·¡ï¼Œç¼ºä¹æ˜é¡¯çš„è®ŠåŒ–å’Œå±¤æ¬¡ã€‚"
        else:
            cloud_desc = "å¤©ç©ºé›²å±¤å–®èª¿ï¼Œå»ºè­°å°‹æ‰¾æ›´æœ‰è®ŠåŒ–çš„å ´æ™¯ã€‚"
        
        # å…‰å½±æ•ˆæœæè¿°
        golden_ratio = lighting_data.get('golden_ratio', 0) * 100
        if golden_ratio > 60:
            lighting_desc = f"å…‰ç·šæ¢ä»¶æ¥µä½³ï¼ˆ{golden_ratio:.1f}%ï¼‰ï¼Œè™•æ–¼é»ƒé‡‘æ”å½±æ™‚æ®µã€‚"
        elif golden_ratio > 40:
            lighting_desc = f"å…‰ç·šæ¢ä»¶è‰¯å¥½ï¼ˆ{golden_ratio:.1f}%ï¼‰ï¼Œé©åˆæ‹æ”ç‡’å¤©ã€‚"
        elif golden_ratio > 20:
            lighting_desc = f"å…‰ç·šæ¢ä»¶ä¸€èˆ¬ï¼ˆ{golden_ratio:.1f}%ï¼‰ï¼Œå¯ä»¥å˜—è©¦å¾ŒæœŸå¢å¼·ã€‚"
        else:
            lighting_desc = f"å…‰ç·šæ¢ä»¶è¼ƒå·®ï¼ˆ{golden_ratio:.1f}%ï¼‰ï¼Œå»ºè­°é¸æ“‡æ¥è¿‘æ—¥å‡ºæ—¥è½çš„æ™‚æ®µã€‚"
        
        # æ•´é«”è©•åƒ¹
        if ai_score >= 80:
            overall = "é€™æ˜¯ä¸€å¼µæ¥µå“ç‡’å¤©ç…§ç‰‡ï¼è‰²å½©çµ¢éº—ï¼Œé›²å±¤è±å¯Œï¼Œå…‰å½±å®Œç¾ã€‚å€¼å¾—åˆ†äº«å’Œæ”¶è—ã€‚"
        elif ai_score >= 65:
            overall = "é€™æ˜¯ä¸€å¼µå„ªè³ªçš„ç‡’å¤©ç…§ç‰‡ï¼Œå„æ–¹é¢è¡¨ç¾å‡è¡¡ï¼Œå…·æœ‰è¼ƒé«˜çš„è§€è³åƒ¹å€¼ã€‚"
        elif ai_score >= 50:
            overall = "ç…§ç‰‡æ•æ‰åˆ°äº†ç‡’å¤©çš„åŸºæœ¬ç‰¹å¾µï¼Œä½†ä»æœ‰æå‡ç©ºé–“ã€‚"
        elif ai_score >= 35:
            overall = "ç…§ç‰‡å…·æœ‰ä¸€å®šçš„ç‡’å¤©å…ƒç´ ï¼Œä½†æ•´é«”æ•ˆæœä¸å¤ ç†æƒ³ã€‚"
        else:
            overall = "ç…§ç‰‡çš„ç‡’å¤©ç‰¹å¾µä¸æ˜é¡¯ï¼Œå»ºè­°ç­‰å¾…æ›´å¥½çš„å¤©æ°£æ¢ä»¶ã€‚"
        
        # æ”¹é€²å»ºè­°
        suggestions = []
        if warm_ratio < 20:
            suggestions.append("ç­‰å¾…æ—¥è½å‰å¾Œ30åˆ†é˜ï¼Œæ­¤æ™‚å¤©ç©ºæš–è‰²èª¿æœ€æ˜é¡¯")
        if variation < 40:
            suggestions.append("å°‹æ‰¾é›²å±¤æ›´è±å¯Œçš„å¤©ç©ºï¼Œé«˜ç©é›²å’Œå±¤ç©é›²æœ€ä½³")
        if golden_ratio < 40:
            suggestions.append("åœ¨æ—¥å‡ºå¾Œ15åˆ†é˜æˆ–æ—¥è½å‰30åˆ†é˜æ‹æ”")
        if color_data.get('saturation', 0) < 0.5:
            suggestions.append("å¾ŒæœŸå¯é©ç•¶æå‡é£½å’Œåº¦å’Œå°æ¯”åº¦")
        if not suggestions:
            suggestions.append("ç…§ç‰‡å“è³ªå·²ç¶“å¾ˆå¥½ï¼Œç¹¼çºŒä¿æŒï¼")
        
        suggestions_text = " | ".join(suggestions)
        
        return jsonify({
            "success": True,
            "ai_score": round(ai_score, 1),
            "user_rating": user_rating,
            "photo_analysis": {
                "color_analysis": color_desc,
                "cloud_structure": cloud_desc,
                "lighting_effect": lighting_desc,
                "overall_quality": overall,
                "suggestions": suggestions_text
            },
            "raw_data": {
                "warm_color_ratio": round(warm_ratio, 1),
                "cloud_variation": round(variation, 1),
                "lighting_quality": round(golden_ratio, 1),
                "color_intensity": round(color_data.get('intensity', 0) * 100, 1)
            }
        })
    
    except Exception as e:
        print(f"âŒ ç…§ç‰‡åˆ†æéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "success": False,
            "message": f"åˆ†æå¤±æ•—ï¼š{str(e)}"
        }), 500

@app.route('/api/upload-photo', methods=['POST'])
def upload_burnsky_photo():
    """ä¸Šå‚³ç‡’å¤©ç…§ç‰‡ä¸¦åˆ†æ"""
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰æª”æ¡ˆ
        if 'photo' not in request.files:
            return jsonify({
                "status": "error",
                "message": "æ²’æœ‰é¸æ“‡ç…§ç‰‡"
            }), 400
        
        file = request.files['photo']
        if file.filename == '':
            return jsonify({
                "status": "error", 
                "message": "æ²’æœ‰é¸æ“‡ç…§ç‰‡"
            }), 400
        
        # æª¢æŸ¥æª”æ¡ˆé¡å‹
        if not allowed_file(file.filename):
            return jsonify({
                "status": "error",
                "message": f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ã€‚æ”¯æ´: {', '.join(ALLOWED_EXTENSIONS)}"
            }), 400
        
        # æª¢æŸ¥æª”æ¡ˆå¤§å°
        file.seek(0, os.SEEK_END)
        file_size = file.tell()
        file.seek(0)
        
        if file_size > MAX_FILE_SIZE:
            return jsonify({
                "status": "error",
                "message": f"æª”æ¡ˆå¤ªå¤§ï¼Œæœ€å¤§æ”¯æ´ {MAX_FILE_SIZE // (1024*1024)}MB"
            }), 400
        
        # è®€å–ä¸¦é©—è­‰ç…§ç‰‡
        photo_data = file.read()
        
        # é©—è­‰æª”æ¡ˆç¢ºå¯¦æ˜¯æœ‰æ•ˆåœ–ç‰‡
        if not validate_image_content(photo_data):
            return jsonify({
                "status": "error",
                "message": "æª”æ¡ˆæå£æˆ–ä¸æ˜¯æœ‰æ•ˆçš„åœ–ç‰‡æ ¼å¼"
            }), 400
        
        # åˆ†æç…§ç‰‡
        photo_analysis = analyze_photo_quality(photo_data)
        
        # ç²å–è¡¨å–®æ•¸æ“š
        location = request.form.get('location', 'æœªçŸ¥åœ°é»')
        visual_rating = float(request.form.get('visual_rating', 5))
        weather_notes = request.form.get('weather_notes', '')
        
        # å„²å­˜é¸é …
        save_photo = request.form.get('save_photo', 'false').lower() == 'true'
        saved_path = None
        
        # ä¿å­˜ç…§ç‰‡ï¼ˆå¦‚æœé¸æ“‡ï¼‰
        if save_photo or AUTO_SAVE_PHOTOS:
            try:
                # æ¸…ç†èˆŠç…§ç‰‡
                cleanup_old_photos()
                
                # ç”Ÿæˆå®‰å…¨æª”å
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                safe_filename = secure_filename(file.filename)
                if not safe_filename:
                    safe_filename = "photo.jpg"
                
                filename = f"{timestamp}_{safe_filename}"
                file_path = os.path.join(UPLOAD_FOLDER, filename)
                
                # å„²å­˜æª”æ¡ˆ
                with open(file_path, 'wb') as f:
                    f.write(photo_data)
                
                saved_path = file_path
                print(f"ğŸ“ ç…§ç‰‡å·²å„²å­˜: {filename}")
                
            except Exception as e:
                print(f"âš ï¸ ç…§ç‰‡å„²å­˜å¤±æ•—: {e}")
                # å„²å­˜å¤±æ•—ä¸å½±éŸ¿åˆ†æåŠŸèƒ½
        
        # è¨˜éŒ„æ¡ˆä¾‹åˆ°MLè¨“ç·´æ•¸æ“šåº«ï¼ˆä¸è§¸ç™¼å³æ™‚æ ¡æ­£ï¼‰
        case_id = record_burnsky_photo_case(
            date=datetime.now().strftime('%Y-%m-%d'),
            time=datetime.now().strftime('%H:%M'),
            location=location,
            weather_conditions={"notes": weather_notes},
            visual_rating=visual_rating,
            photo_analysis=photo_analysis,
            saved_path=saved_path
        )
        
        # é€²è¡Œæº–ç¢ºæ€§åˆ†æï¼ˆç”¨æ–¼æ•¸æ“šè³ªé‡è©•ä¼°ï¼‰
        photo_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M')
        accuracy_check = cross_check_photo_with_prediction(
            photo_datetime, location, visual_rating, 'sunset'
        )
        
        # ç²å–MLè¨“ç·´æ•¸æ“šçµ±è¨ˆ
        ml_stats = get_ml_training_stats()
        
        return jsonify({
            "status": "success",
            "message": "ç…§ç‰‡å·²åŠ å…¥MLè¨“ç·´æ•¸æ“šåº«",
            "case_id": case_id,
            "photo_analysis": photo_analysis,
            "accuracy_check": accuracy_check,
            "ml_training_info": {
                "total_cases": ml_stats['total_cases'],
                "pending_training": ml_stats['pending_cases'],
                "next_retrain_threshold": 10 - ml_stats['pending_cases'],
                "data_quality_score": ml_stats['avg_quality'],
                "will_trigger_retrain": ml_stats['pending_cases'] >= 9
            },
            "saved": saved_path is not None,
            "file_size": f"{file_size / 1024:.1f} KB",
            "immediate_prediction_update": False,  # ä¸æœƒç«‹å³æ›´æ–°é æ¸¬
            "contributes_to_ml_training": True,     # ä½†æœƒè²¢ç»MLè¨“ç·´
            "suggestions": {
                "data_collection_tips": get_data_collection_tips(photo_analysis),
                "ml_improvement_advice": get_ml_improvement_advice(visual_rating, ml_stats)
            }
        })
    
    except Exception as e:
        print(f"âŒ ç…§ç‰‡ä¸Šå‚³éŒ¯èª¤: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

def get_ml_training_stats():
    """ç²å–MLè¨“ç·´æ•¸æ“šçµ±è¨ˆ"""
    try:
        conn = sqlite3.connect('ml_training_data.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM ml_training_cases')
        total_cases = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM ml_training_cases WHERE training_status = 'pending'")
        pending_cases = cursor.fetchone()[0]
        
        cursor.execute('SELECT AVG(visual_rating) FROM ml_training_cases')
        avg_quality = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'total_cases': total_cases,
            'pending_cases': pending_cases,
            'avg_quality': round(avg_quality, 1)
        }
    except:
        return {
            'total_cases': len(BURNSKY_PHOTO_CASES),
            'pending_cases': 0,
            'avg_quality': 5.0
        }

def get_data_collection_tips(photo_analysis):
    """æä¾›æ•¸æ“šæ”¶é›†å»ºè­°"""
    tips = []
    score = photo_analysis.get('quality_score', 5)
    
    if score >= 8:
        tips.append("ğŸŒŸ å„ªè³ªè¨“ç·´æ•¸æ“šï¼é€™ç¨®é«˜å“è³ªæ¡ˆä¾‹å°MLæ¨¡å‹å¾ˆæœ‰åƒ¹å€¼")
        tips.append("ğŸ“Š å»ºè­°è¨˜éŒ„ç•¶æ™‚çš„è©³ç´°å¤©æ°£æ¢ä»¶å’Œæ‹æ”åƒæ•¸")
    elif score >= 6:
        tips.append("âœ… è‰¯å¥½çš„è¨“ç·´æ¨£æœ¬ï¼Œæœ‰åŠ©æ–¼æ¨¡å‹å­¸ç¿’ä¸­ç­‰å“è³ªç‡’å¤©")
        tips.append("ğŸ” å¯ä»¥å˜—è©¦è¨˜éŒ„æ›´å¤šç’°å¢ƒå› ç´ ")
    else:
        tips.append("ğŸ“ˆ æ™®é€šæ¡ˆä¾‹ä¹Ÿå¾ˆé‡è¦ï¼Œå¹«åŠ©æ¨¡å‹è­˜åˆ¥éç‡’å¤©æ¢ä»¶")
        tips.append("âš¡ é€™é¡æ•¸æ“šæœ‰åŠ©æ–¼æ¸›å°‘false positiveé æ¸¬")
    
    return tips

def get_ml_improvement_advice(visual_rating, ml_stats):
    """æä¾›MLæ”¹é€²å»ºè­°"""
    advice = []
    
    if ml_stats['total_cases'] < 30:
        advice.append("ğŸš€ ç¹¼çºŒæ”¶é›†æ›´å¤šè¨“ç·´æ•¸æ“šï¼Œç›®æ¨™æ˜¯50+å€‹æ¨£æœ¬")
    
    if visual_rating >= 7 and ml_stats['avg_quality'] < 6:
        advice.append("ğŸŒ… æ‚¨çš„é«˜å“è³ªæ¡ˆä¾‹å°‡é¡¯è‘—æå‡æ¨¡å‹æº–ç¢ºåº¦")
    
    if ml_stats['pending_cases'] >= 8:
        advice.append("ğŸ¤– å³å°‡è§¸ç™¼æ¨¡å‹é‡æ–°è¨“ç·´ï¼Œé æ¸¬æº–ç¢ºåº¦å°‡æœ‰æ‰€æå‡")
    
    return advice if advice else ["ğŸ“Š æŒçºŒæä¾›è¨“ç·´æ•¸æ“šæœ‰åŠ©æ–¼æ”¹é€²é æ¸¬æº–ç¢ºæ€§"]

def get_improvement_tips(photo_analysis):
    """æ ¹æ“šç…§ç‰‡åˆ†ææä¾›æ”¹é€²å»ºè­°"""
    tips = []
    
    if 'color_analysis' in photo_analysis:
        color = photo_analysis['color_analysis']
        if color['intensity'] < 0.5:
            tips.append("å˜—è©¦åœ¨æ›´å¼·çƒˆçš„æ©™ç´…è‰²å…‰ç·šæ™‚æ‹æ”")
        if color['contrast'] < 0.3:
            tips.append("å°‹æ‰¾æ›´å¼·çƒˆçš„æš–å†·å°æ¯”å ´æ™¯")
    
    if 'cloud_analysis' in photo_analysis:
        cloud = photo_analysis['cloud_analysis']
        if cloud['variation'] < 0.5:
            tips.append("ç­‰å¾…æ›´æœ‰å±¤æ¬¡è®ŠåŒ–çš„é›²å±¤")
        if cloud['edge_definition'] < 0.4:
            tips.append("å°‹æ‰¾è¼ªå»“æ›´æ¸…æ™°çš„é›²å±¤")
    
    if 'lighting_analysis' in photo_analysis:
        lighting = photo_analysis['lighting_analysis']
        if lighting['golden_ratio'] < 0.4:
            tips.append("åœ¨é»ƒé‡‘æ™‚æ®µï¼ˆæ—¥è½å‰30-60åˆ†é˜ï¼‰æ‹æ”")
    
    return tips if tips else ["é€™å·²ç¶“æ˜¯å¾ˆæ£’çš„ç‡’å¤©ç…§ç‰‡äº†ï¼"]

def get_next_shoot_advice(photo_analysis):
    """æä¾›ä¸‹æ¬¡æ‹æ”å»ºè­°"""
    score = photo_analysis.get('quality_score', 5)
    
    if score >= 8:
        return "æ¥µä½³æ¢ä»¶ï¼è¨˜éŒ„ç•¶æ™‚çš„ç²¾ç¢ºå¤©æ°£æ•¸æ“šï¼Œé€™ç¨®æ¢ä»¶å¾ˆçè²´"
    elif score >= 6:
        return "è‰¯å¥½æ¢ä»¶ï¼Œå¯ä»¥å˜—è©¦ä¸åŒè§’åº¦å’Œæ§‹åœ–ä¾†æå‡æ•ˆæœ"
    elif score >= 4:
        return "æ™®é€šæ¢ä»¶ï¼Œå»ºè­°ç­‰å¾…é›²å±¤æ›´è±å¯Œæˆ–è‰²å½©æ›´å¼·çƒˆçš„æ™‚æ©Ÿ"
    else:
        return "å»ºè­°é—œæ³¨å¤©æ°£é å ±ï¼Œç­‰å¾…æ›´é©åˆçš„å¤§æ°£æ¢ä»¶"

@app.route('/api/photo-storage', methods=['GET'])
def photo_storage_info():
    """ç…§ç‰‡å„²å­˜è³‡è¨Š"""
    try:
        total_files = 0
        total_size = 0
        files_info = []
        
        if os.path.exists(UPLOAD_FOLDER):
            for filename in os.listdir(UPLOAD_FOLDER):
                file_path = os.path.join(UPLOAD_FOLDER, filename)
                if os.path.isfile(file_path):
                    file_size = os.path.getsize(file_path)
                    file_time = os.path.getmtime(file_path)
                    
                    files_info.append({
                        'filename': filename,
                        'size': file_size,
                        'created': datetime.fromtimestamp(file_time).isoformat(),
                        'age_days': (time.time() - file_time) / (24 * 60 * 60)
                    })
                    
                    total_files += 1
                    total_size += file_size
        
        return jsonify({
            "status": "success",
            "storage_info": {
                "upload_folder": UPLOAD_FOLDER,
                "auto_save": AUTO_SAVE_PHOTOS,
                "retention_days": PHOTO_RETENTION_DAYS,
                "max_file_size_mb": MAX_FILE_SIZE // (1024*1024),
                "allowed_extensions": list(ALLOWED_EXTENSIONS)
            },
            "current_storage": {
                "total_files": total_files,
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / (1024*1024), 2),
                "files": files_info
            }
        })
    
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/photo-storage/cleanup', methods=['POST'])
def manual_cleanup():
    """æ‰‹å‹•æ¸…ç†èˆŠç…§ç‰‡"""
    try:
        if not os.path.exists(UPLOAD_FOLDER):
            return jsonify({
                "status": "success",
                "message": "ç„¡ç…§ç‰‡éœ€è¦æ¸…ç†",
                "cleaned_count": 0
            })
        
        cutoff_time = time.time() - (PHOTO_RETENTION_DAYS * 24 * 60 * 60)
        cleaned_count = 0
        cleaned_files = []
        
        for filename in os.listdir(UPLOAD_FOLDER):
            file_path = os.path.join(UPLOAD_FOLDER, filename)
            if os.path.isfile(file_path) and os.path.getmtime(file_path) < cutoff_time:
                try:
                    file_size = os.path.getsize(file_path)
                    os.remove(file_path)
                    cleaned_files.append({
                        'filename': filename,
                        'size': file_size
                    })
                    cleaned_count += 1
                except OSError as e:
                    print(f"æ¸…ç†æª”æ¡ˆå¤±æ•—: {filename} - {e}")
        
        return jsonify({
            "status": "success",
            "message": f"å·²æ¸…ç† {cleaned_count} å€‹èˆŠç…§ç‰‡",
            "cleaned_count": cleaned_count,
            "cleaned_files": cleaned_files
        })
    
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/prediction/update', methods=['POST'])
def manual_prediction_update():
    """æ‰‹å‹•è§¸ç™¼é æ¸¬æ›´æ–°"""
    try:
        cleared_count = trigger_prediction_update()
        
        return jsonify({
            "status": "success",
            "message": f"é æ¸¬æ›´æ–°å·²è§¸ç™¼ï¼Œæ¸…é™¤äº† {cleared_count} å€‹å¿«å–é …ç›®",
            "cleared_cache_count": cleared_count,
            "next_prediction_will_be_fresh": True,
            "total_cases": len(BURNSKY_PHOTO_CASES),
            "last_update": LAST_CASE_UPDATE
        })
    
    except Exception as e:
        return jsonify({
            "status": "error", 
            "message": str(e)
        }), 500

@app.route('/api/prediction/status', methods=['GET'])
def prediction_status():
    """ç²å–é æ¸¬ç³»çµ±ç‹€æ…‹"""
    try:
        # çµ±è¨ˆå¿«å–é …ç›®
        prediction_cache_count = len([key for key in cache.keys() if 'prediction' in key or 'burnsky' in key])
        total_cache_count = len(cache)
        
        return jsonify({
            "status": "success",
            "prediction_system": {
                "total_cases": len(BURNSKY_PHOTO_CASES),
                "last_case_update": LAST_CASE_UPDATE,
                "cache_status": {
                    "total_cache_items": total_cache_count,
                    "prediction_cache_items": prediction_cache_count,
                    "cache_duration_seconds": CACHE_DURATION
                },
                "auto_update_enabled": True,
                "learning_active": len(BURNSKY_PHOTO_CASES) > 0
            }
        })
    
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/data-management', methods=['GET'])
def data_management_info():
    """ç²å–æ•¸æ“šç®¡ç†è³‡è¨Š"""
    try:
        # çµ±è¨ˆç…§ç‰‡æ¡ˆä¾‹æ•¸æ“š
        photo_count = 0
        photo_range = (None, None)
        try:
            photo_conn = sqlite3.connect('burnsky_photos.db')
            photo_cursor = photo_conn.cursor()
            photo_cursor.execute('SELECT COUNT(*) FROM photos')
            photo_count = photo_cursor.fetchone()[0]
            
            photo_cursor.execute('SELECT MIN(timestamp), MAX(timestamp) FROM photos')
            photo_range = photo_cursor.fetchone()
            photo_conn.close()
        except sqlite3.OperationalError:
            # è¡¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨è¨˜æ†¶é«”ä¸­çš„æ¡ˆä¾‹æ•¸
            photo_count = len(BURNSKY_PHOTO_CASES)
        
        # çµ±è¨ˆé æ¸¬æ­·å²æ•¸æ“š
        history_count = 0
        history_range = (None, None)
        try:
            hist_conn = sqlite3.connect(PREDICTION_HISTORY_DB)
            hist_cursor = hist_conn.cursor()
            hist_cursor.execute('SELECT COUNT(*) FROM prediction_history')
            history_count = hist_cursor.fetchone()[0]
            
            hist_cursor.execute('SELECT MIN(timestamp), MAX(timestamp) FROM prediction_history')
            history_range = hist_cursor.fetchone()
            hist_conn.close()
        except sqlite3.OperationalError:
            history_count = 0
        
        # çµ±è¨ˆä¸Šå‚³æª”æ¡ˆ
        upload_files = []
        if os.path.exists(UPLOAD_FOLDER):
            for filename in os.listdir(UPLOAD_FOLDER):
                filepath = os.path.join(UPLOAD_FOLDER, filename)
                if os.path.isfile(filepath):
                    stat = os.stat(filepath)
                    upload_files.append({
                        'filename': filename,
                        'size': stat.st_size,
                        'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                        'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                    })
        
        return jsonify({
            'status': 'success',
            'data_summary': {
                'photo_cases': {
                    'count': photo_count,
                    'date_range': photo_range,
                    'database_file': 'burnsky_photos.db',
                    'in_memory_cases': len(BURNSKY_PHOTO_CASES)
                },
                'prediction_history': {
                    'count': history_count,
                    'date_range': history_range,
                    'database_file': PREDICTION_HISTORY_DB
                },
                'uploaded_files': {
                    'count': len(upload_files),
                    'total_size': sum(f['size'] for f in upload_files),
                    'files': upload_files[:10],  # åªé¡¯ç¤ºå‰10å€‹
                    'folder': UPLOAD_FOLDER
                }
            },
            'cleanup_options': {
                'available_operations': [
                    'clear_photo_cases',
                    'clear_prediction_history', 
                    'clear_uploaded_files',
                    'clear_old_data',
                    'clear_all'
                ]
            }
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/data-cleanup', methods=['POST'])
def data_cleanup():
    """æ¸…ç†ç”¨æˆ¶æ•¸æ“š"""
    try:
        data = request.get_json()
        operation = data.get('operation', '')
        confirm = data.get('confirm', False)
        days_old = data.get('days_old', 30)
        
        if not confirm:
            return jsonify({
                'status': 'error',
                'message': 'è«‹ç¢ºèªæ¸…ç†æ“ä½œ (confirm: true)'
            }), 400
        
        results = []
        
        if operation == 'clear_photo_cases' or operation == 'clear_all':
            # æ¸…ç†ç…§ç‰‡æ¡ˆä¾‹æ•¸æ“š
            conn = sqlite3.connect('burnsky_photos.db')
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM photos')
            before_count = cursor.fetchone()[0]
            
            cursor.execute('DELETE FROM photos')
            conn.commit()
            conn.close()
            
            # æ¸…ç†è¨˜æ†¶é«”ä¸­çš„æ¡ˆä¾‹
            global BURNSKY_PHOTO_CASES
            BURNSKY_PHOTO_CASES.clear()
            
            results.append(f"âœ… å·²æ¸…ç† {before_count} å€‹ç…§ç‰‡æ¡ˆä¾‹")
        
        if operation == 'clear_prediction_history' or operation == 'clear_all':
            # æ¸…ç†é æ¸¬æ­·å²
            conn = sqlite3.connect(PREDICTION_HISTORY_DB)
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM prediction_history')
            before_count = cursor.fetchone()[0]
            
            cursor.execute('DELETE FROM prediction_history')
            conn.commit()
            conn.close()
            
            results.append(f"âœ… å·²æ¸…ç† {before_count} æ¢é æ¸¬æ­·å²")
        
        if operation == 'clear_uploaded_files' or operation == 'clear_all':
            # æ¸…ç†ä¸Šå‚³æª”æ¡ˆ
            deleted_count = 0
            deleted_size = 0
            
            if os.path.exists(UPLOAD_FOLDER):
                for filename in os.listdir(UPLOAD_FOLDER):
                    filepath = os.path.join(UPLOAD_FOLDER, filename)
                    if os.path.isfile(filepath):
                        file_size = os.path.getsize(filepath)
                        os.remove(filepath)
                        deleted_count += 1
                        deleted_size += file_size
            
            results.append(f"âœ… å·²æ¸…ç† {deleted_count} å€‹ä¸Šå‚³æª”æ¡ˆ ({deleted_size/1024/1024:.1f}MB)")
        
        if operation == 'clear_old_data':
            # æ¸…ç†èˆŠæ•¸æ“š
            cutoff_date = datetime.now() - timedelta(days=days_old)
            
            # æ¸…ç†èˆŠç…§ç‰‡æ¡ˆä¾‹
            conn = sqlite3.connect('burnsky_photos.db')
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM photos WHERE timestamp < ?', (cutoff_date,))
            old_photos = cursor.fetchone()[0]
            cursor.execute('DELETE FROM photos WHERE timestamp < ?', (cutoff_date,))
            conn.commit()
            conn.close()
            
            # æ¸…ç†èˆŠé æ¸¬æ­·å²
            conn = sqlite3.connect(PREDICTION_HISTORY_DB)
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM prediction_history WHERE timestamp < ?', (cutoff_date,))
            old_history = cursor.fetchone()[0]
            cursor.execute('DELETE FROM prediction_history WHERE timestamp < ?', (cutoff_date,))
            conn.commit()
            conn.close()
            
            # æ¸…ç†èˆŠæª”æ¡ˆ
            deleted_files = 0
            if os.path.exists(UPLOAD_FOLDER):
                for filename in os.listdir(UPLOAD_FOLDER):
                    filepath = os.path.join(UPLOAD_FOLDER, filename)
                    if os.path.isfile(filepath):
                        file_time = datetime.fromtimestamp(os.path.getmtime(filepath))
                        if file_time < cutoff_date:
                            os.remove(filepath)
                            deleted_files += 1
            
            results.append(f"âœ… å·²æ¸…ç† {days_old} å¤©å‰çš„æ•¸æ“š:")
            results.append(f"   - ç…§ç‰‡æ¡ˆä¾‹: {old_photos} å€‹")
            results.append(f"   - é æ¸¬æ­·å²: {old_history} æ¢")
            results.append(f"   - ä¸Šå‚³æª”æ¡ˆ: {deleted_files} å€‹")
        
        # æ¸…ç†å¿«å–
        clear_prediction_cache()
        results.append("âœ… å·²æ¸…ç†é æ¸¬å¿«å–")
        
        return jsonify({
            'status': 'success',
            'operation': operation,
            'results': results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/photo-accuracy-check', methods=['POST'])
def photo_accuracy_check():
    """æª¢æŸ¥ç…§ç‰‡èˆ‡é æ¸¬çš„æº–ç¢ºæ€§"""
    try:
        data = request.get_json()
        photo_datetime = data.get('datetime')  # "2025-07-27_19-10"
        photo_location = data.get('location', 'æœªçŸ¥')
        photo_quality = data.get('quality', 5)  # 1-10åˆ†
        prediction_type = data.get('type', 'sunset')
        
        if not photo_datetime:
            return jsonify({
                'status': 'error',
                'message': 'è«‹æä¾›ç…§ç‰‡æ™‚é–“ (datetime)'
            }), 400
        
        result = cross_check_photo_with_prediction(
            photo_datetime, photo_location, photo_quality, prediction_type
        )
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route("/api/photo-cases/analyze", methods=["GET"])
def analyze_current_conditions():
    """åˆ†æç•¶å‰æ¢ä»¶èˆ‡æˆåŠŸæ¡ˆä¾‹çš„ç›¸ä¼¼åº¦"""
    # ç²å–ç•¶å‰å¤©æ°£æ•¸æ“š
    weather_data = get_cached_data('weather', fetch_weather_data)
    
    current_conditions = {
        "time": datetime.now().strftime("%H:%M"),
        "cloud_coverage": weather_data.get("cloud", {}),
        "visibility": weather_data.get("visibility", {}),
        "humidity": weather_data.get("humidity", {})
    }
    
    is_similar, similarity_score = is_similar_to_successful_cases(current_conditions)
    patterns = analyze_photo_case_patterns()
    
    return jsonify({
        "status": "success",
        "current_conditions": current_conditions,
        "is_similar_to_success": is_similar,
        "similarity_score": similarity_score,
        "successful_patterns": patterns,
        "recommendation": "é«˜ç‡’å¤©æ©Ÿæœƒ" if is_similar else "ç‡’å¤©æ©Ÿæœƒä¸€èˆ¬"
    })

# æ–°åŠŸèƒ½è·¯ç”±
@app.route("/api/locations")
def get_shooting_locations():
    """å–å¾—æ¨è–¦æ‹æ”åœ°é» API"""
    locations = [
        {
            "id": 1,
            "name": "ç¶­å¤šåˆ©äºæ¸¯",
            "name_en": "Victoria Harbour",
            "description": "ç¶“å…¸ç‡’å¤©æ‹æ”è–åœ°ï¼Œå¯åŒæ™‚æ•æ‰åŸå¸‚å¤©éš›ç·šèˆ‡æµ·æ¸¯ç¾æ™¯",
            "difficulty": "å®¹æ˜“",
            "transport": "åœ°éµå¯é”",
            "best_time": "æ—¥è½",
            "rating": 5,
            "coordinates": [22.2783, 114.1747],
            "mtr_stations": ["å°–æ²™å’€", "ä¸­ç’°", "ç£ä»”"],
            "photo_spots": ["å°–æ²™å’€æµ·æ¿±é•·å»Š", "ä¸­ç’°æ‘©å¤©è¼ª", "é‡‘ç´«èŠå»£å ´"],
            "tips": ["å»ºè­°æ”œå¸¶å»£è§’é¡é ­", "æ³¨æ„æ½®æ±æ™‚é–“", "é¿é–‹é€±æœ«äººæ½®"]
        },
        {
            "id": 2,
            "name": "å¤ªå¹³å±±é ‚",
            "name_en": "Victoria Peak",
            "description": "ä¿¯ç°å…¨æ¸¯æ™¯è‰²çš„æœ€ä½³ä½ç½®ï¼Œ360åº¦å…¨æ™¯è¦–é‡",
            "difficulty": "ä¸­ç­‰",
            "transport": "å±±é ‚çºœè»Š",
            "best_time": "æ—¥è½",
            "rating": 5,
            "coordinates": [22.2707, 114.1490],
            "mtr_stations": ["ä¸­ç’°"],
            "photo_spots": ["å±±é ‚å»£å ´", "ç…å­äº­", "ç›§å‰é“"],
            "tips": ["ææ—©åˆ°é”ä½”ä½", "æº–å‚™ä¿æš–è¡£ç‰©", "æ³¨æ„çºœè»Šç‡Ÿé‹æ™‚é–“"]
        },
        {
            "id": 3,
            "name": "çŸ³æ¾³",
            "name_en": "Shek O",
            "description": "é¦™æ¸¯å³¶æ±å—ç«¯çš„æµ·å²¸ç·šï¼Œçµ•ä½³æ—¥å‡ºæ‹æ”é»",
            "difficulty": "å®¹æ˜“",
            "transport": "å·´å£«å¯é”",
            "best_time": "æ—¥å‡º",
            "rating": 4,
            "coordinates": [22.2182, 114.2542],
            "mtr_stations": ["ç­²ç®•ç£"],
            "photo_spots": ["çŸ³æ¾³æµ·ç˜", "çŸ³æ¾³éƒŠé‡å…¬åœ’", "å¤§é ­æ´²"],
            "tips": ["æ¸…æ™¨6é»å‰åˆ°é”", "æ³¨æ„æµ·æµªå®‰å…¨", "æ”œå¸¶æ‰‹é›»ç­’"]
        },
        {
            "id": 4,
            "name": "ç…å­å±±",
            "name_en": "Lion Rock",
            "description": "é¦™æ¸¯ç²¾ç¥è±¡å¾µï¼Œä¿¯ç°ä¹é¾åŠå³¶çš„å£¯éº—æ™¯è‰²",
            "difficulty": "å›°é›£",
            "transport": "è¡Œå±±",
            "best_time": "æ—¥è½",
            "rating": 4,
            "coordinates": [22.3515, 114.1835],
            "mtr_stations": ["é»ƒå¤§ä»™", "æ¨‚å¯Œ"],
            "photo_spots": ["ç…å­å±±å±±é ‚", "æœ›å¤«çŸ³", "ç…å­é ­"],
            "tips": ["éœ€è¦2-3å°æ™‚è¡Œå±±", "å¸¶è¶³é£²æ°´é£Ÿç‰©", "æ³¨æ„å¤©æ°£è®ŠåŒ–"]
        },
        {
            "id": 5,
            "name": "é’é¦¬å¤§æ©‹",
            "name_en": "Tsing Ma Bridge", 
            "description": "ä¸–ç•Œæœ€é•·æ‡¸ç´¢æ©‹ä¹‹ä¸€ï¼Œå£¯è§€çš„å·¥ç¨‹å»ºç¯‰ç¾å­¸",
            "difficulty": "ä¸­ç­‰",
            "transport": "å·´å£«+æ­¥è¡Œ",
            "best_time": "æ—¥è½",
            "rating": 4,
            "coordinates": [22.3354, 114.1089],
            "mtr_stations": ["é’è¡£"],
            "photo_spots": ["é’å¶¼å¹¹ç·šè§€æ™¯å°", "æ±€ä¹æ©‹"],
            "tips": ["æ³¨æ„é–‹æ”¾æ™‚é–“", "é¿å…å¼·é¢¨æ—¥å­", "æ”œå¸¶æœ›é é¡é ­"]
        }
    ]
    
    return jsonify({
        "status": "success",
        "locations": locations,
        "total": len(locations),
        "last_updated": datetime.now().isoformat()
    })

@app.route("/api/astronomy")
def get_astronomy_times():
    """å–å¾—ç²¾ç¢ºçš„æ—¥å‡ºæ—¥è½æ™‚é–“ API"""
    from datetime import date, timedelta
    
    # ç°¡åŒ–ç‰ˆæ—¥å‡ºæ—¥è½æ™‚é–“è¨ˆç®—ï¼ˆé¿å…é¡å¤–ä¾è³´ï¼‰
    # å¯¦éš›éƒ¨ç½²æ™‚å¯è€ƒæ…®ä½¿ç”¨ ephem æˆ– astral ç­‰å°ˆæ¥­å¤©æ–‡åº«
    today = date.today()
    tomorrow = today + timedelta(days=1)
    
    # é¦™æ¸¯åœ°å€å¤§æ¦‚æ™‚é–“ï¼ˆå­£ç¯€æ€§èª¿æ•´ï¼‰
    import calendar
    month = today.month
    
    # ç°¡åŒ–çš„å­£ç¯€æ€§æ—¥å‡ºæ—¥è½æ™‚é–“
    if month in [12, 1, 2]:  # å†¬å­£
        sunrise_time = "07:00"
        sunset_time = "18:00"
    elif month in [3, 4, 5]:  # æ˜¥å­£
        sunrise_time = "06:30"
        sunset_time = "18:30"
    elif month in [6, 7, 8]:  # å¤å­£
        sunrise_time = "06:00"
        sunset_time = "19:00"
    else:  # ç§‹å­£
        sunrise_time = "06:30"
        sunset_time = "18:30"
    
    # è¨ˆç®—é»ƒé‡‘æ™‚æ®µï¼ˆæ—¥è½å‰30åˆ†é˜ï¼‰
    from datetime import datetime, time
    sunset_dt = datetime.strptime(sunset_time, "%H:%M").time()
    golden_hour_dt = (datetime.combine(today, sunset_dt) - timedelta(minutes=30)).time()
    golden_hour_time = golden_hour_dt.strftime("%H:%M")
    
    return jsonify({
        "status": "success",
        "today": {
            "date": today.isoformat(),
            "sunrise": sunrise_time,
            "sunset": sunset_time,
            "golden_hour": golden_hour_time
        },
        "tomorrow": {
            "date": tomorrow.isoformat(), 
            "sunrise": sunrise_time,  # ç°¡åŒ–ï¼šä½¿ç”¨ç›¸åŒæ™‚é–“
            "sunset": sunset_time,
            "golden_hour": golden_hour_time
        },
        "location": "Hong Kong",
        "timezone": "UTC+8",
        "note": "æ™‚é–“ç‚ºè¿‘ä¼¼å€¼ï¼Œå¯¦éš›æ—¥å‡ºæ—¥è½æœƒå› æ—¥æœŸå’Œåœ°ç†ä½ç½®è€Œæœ‰å·®ç•°"
    })

@app.route("/api/user/preferences", methods=["GET", "POST"])
def handle_user_preferences():
    """è™•ç†ç”¨æˆ¶åå¥½è¨­å®š API"""
    if request.method == "POST":
        # å„²å­˜ç”¨æˆ¶åå¥½ï¼ˆæœªä¾†å¯é€£æ¥è³‡æ–™åº«ï¼‰
        data = request.get_json()
        preferences = {
            "notification_enabled": data.get("notification_enabled", False),
            "notification_threshold": data.get("notification_threshold", 60),
            "notification_advance": data.get("notification_advance", 60),
            "preferred_locations": data.get("preferred_locations", []),
            "preferred_times": data.get("preferred_times", ["sunset"]),
            "updated_at": datetime.now().isoformat()
        }
        
        return jsonify({
            "status": "success",
            "message": "åå¥½è¨­å®šå·²å„²å­˜",
            "preferences": preferences
        })
    
    else:
        # å–å¾—ç”¨æˆ¶åå¥½ï¼ˆæœªä¾†å¾è³‡æ–™åº«è®€å–ï¼‰
        default_preferences = {
            "notification_enabled": False,
            "notification_threshold": 60,
            "notification_advance": 60,
            "preferred_locations": [1, 2],  # ç¶­æ¸¯ã€å±±é ‚
            "preferred_times": ["sunset"],
            "theme": "auto"
        }
        
        return jsonify({
            "status": "success",
            "preferences": default_preferences
        })

# ğŸ†• è­¦å‘Šæ­·å²åˆ†æ API ç«¯é»
@app.route("/api/warnings/overview-charts", methods=["GET"])
def get_overview_charts():
    """ç²å–ç¸½è¦½çµ±è¨ˆåœ–è¡¨æ•¸æ“š"""
    global warning_analyzer
    
    if not warning_analysis_available or not warning_analyzer:
        # è¿”å›ç¤ºä¾‹æ•¸æ“š
        return jsonify({
            "status": "success",
            "data_source": "example_data",
            "charts": {
                "warning_trends": {
                    "chart_type": "bar",
                    "chart_data": {
                        "labels": ["æœ¬é€±", "ä¸Šé€±", "å…©é€±å‰", "ä¸‰é€±å‰"],
                        "datasets": [{
                            "label": "è­¦å‘Šæ•¸é‡",
                            "data": [15, 12, 18, 8],
                            "backgroundColor": ["#EF4444", "#F59E0B", "#10B981", "#3B82F6"],
                            "borderColor": ["#DC2626", "#D97706", "#059669", "#2563EB"],
                            "borderWidth": 2
                        }]
                    },
                    "chart_options": {
                        "responsive": True,
                        "plugins": {
                            "title": {
                                "display": True,
                                "text": "é€±è­¦å‘Šè¶¨å‹¢"
                            }
                        },
                        "scales": {
                            "y": {
                                "beginAtZero": True,
                                "title": {
                                    "display": True,
                                    "text": "è­¦å‘Šæ•¸é‡"
                                }
                            }
                        }
                    }
                },
                "severity_distribution": {
                    "chart_type": "polarArea",
                    "chart_data": {
                        "labels": ["æ¥µç«¯", "åš´é‡", "ä¸­ç­‰", "è¼•å¾®"],
                        "datasets": [{
                            "label": "åš´é‡åº¦åˆ†å¸ƒ",
                            "data": [3, 8, 12, 7],
                            "backgroundColor": [
                                "rgba(239, 68, 68, 0.7)",
                                "rgba(245, 158, 11, 0.7)",
                                "rgba(59, 130, 246, 0.7)",
                                "rgba(16, 185, 129, 0.7)"
                            ],
                            "borderColor": [
                                "#DC2626",
                                "#D97706",
                                "#2563EB",
                                "#059669"
                            ],
                            "borderWidth": 2
                        }]
                    },
                    "chart_options": {
                        "responsive": True,
                        "plugins": {
                            "title": {
                                "display": True,
                                "text": "è­¦å‘Šåš´é‡åº¦åˆ†å¸ƒ"
                            },
                            "legend": {
                                "position": "bottom"
                            }
                        }
                    }
                },
                "hourly_pattern": {
                    "chart_type": "radar",
                    "chart_data": {
                        "labels": ["0-6æ™‚", "6-12æ™‚", "12-18æ™‚", "18-24æ™‚"],
                        "datasets": [{
                            "label": "å„æ™‚æ®µè­¦å‘Šé »ç‡",
                            "data": [2, 8, 15, 5],
                            "backgroundColor": "rgba(139, 92, 246, 0.2)",
                            "borderColor": "#8B5CF6",
                            "borderWidth": 2,
                            "pointBackgroundColor": "#8B5CF6",
                            "pointBorderColor": "#fff",
                            "pointHoverBackgroundColor": "#fff",
                            "pointHoverBorderColor": "#8B5CF6"
                        }]
                    },
                    "chart_options": {
                        "responsive": True,
                        "plugins": {
                            "title": {
                                "display": True,
                                "text": "24å°æ™‚è­¦å‘Šæ¨¡å¼"
                            }
                        },
                        "scales": {
                            "r": {
                                "beginAtZero": True,
                                "title": {
                                    "display": True,
                                    "text": "è­¦å‘Šé »ç‡"
                                }
                            }
                        }
                    }
                }
            },
            "summary": {
                "total_charts": 3,
                "data_period": "30å¤© (ç¤ºä¾‹æ•¸æ“š)"
            },
            "generated_at": datetime.now().isoformat()
        })
    
    try:
        days_back = int(request.args.get('days', 30))
        days_back = min(max(days_back, 1), 365)
        
        # ç²å–è­¦å‘Šæ¨¡å¼æ•¸æ“š
        patterns = warning_analyzer.analyze_warning_patterns(days_back)
        
        if patterns.get('total_warnings', 0) == 0:
            # å¦‚æœæ²’æœ‰å¯¦éš›æ•¸æ“šï¼Œè¿”å›ä¸Šé¢çš„ç¤ºä¾‹æ•¸æ“š
            return get_overview_charts()
        
        # è™•ç†å¯¦éš›æ•¸æ“š
        charts_data = {}
        
        # 1. è­¦å‘Šè¶¨å‹¢åœ– (åŸºæ–¼æ™‚é–“åˆ†å¸ƒ)
        temporal_patterns = patterns.get('temporal_patterns', {})
        hourly_dist = temporal_patterns.get('hourly_distribution', {})
        
        if hourly_dist:
            # å°‡24å°æ™‚åˆ†çµ„ç‚º4å€‹æ™‚æ®µ
            time_periods = {"0-6æ™‚": 0, "6-12æ™‚": 0, "12-18æ™‚": 0, "18-24æ™‚": 0}
            for hour, count in hourly_dist.items():
                hour = int(hour)
                if 0 <= hour < 6:
                    time_periods["0-6æ™‚"] += count
                elif 6 <= hour < 12:
                    time_periods["6-12æ™‚"] += count
                elif 12 <= hour < 18:
                    time_periods["12-18æ™‚"] += count
                else:
                    time_periods["18-24æ™‚"] += count
            
            charts_data["hourly_pattern"] = {
                "chart_type": "radar",
                "chart_data": {
                    "labels": list(time_periods.keys()),
                    "datasets": [{
                        "label": "å„æ™‚æ®µè­¦å‘Šé »ç‡",
                        "data": list(time_periods.values()),
                        "backgroundColor": "rgba(139, 92, 246, 0.2)",
                        "borderColor": "#8B5CF6",
                        "borderWidth": 2,
                        "pointBackgroundColor": "#8B5CF6"
                    }]
                },
                "chart_options": {
                    "responsive": True,
                    "plugins": {
                        "title": {
                            "display": True,
                            "text": "24å°æ™‚è­¦å‘Šæ¨¡å¼"
                        }
                    }
                }
            }
        
        # 2. åš´é‡åº¦åˆ†å¸ƒåœ–
        severity_dist = patterns.get('severity_distribution', {})
        if severity_dist:
            severity_labels = []
            severity_data = []
            severity_colors = []
            
            severity_info = {
                "extreme": {"label": "æ¥µç«¯", "color": "rgba(239, 68, 68, 0.7)"},
                "severe": {"label": "åš´é‡", "color": "rgba(245, 158, 11, 0.7)"},
                "moderate": {"label": "ä¸­ç­‰", "color": "rgba(59, 130, 246, 0.7)"},
                "low": {"label": "è¼•å¾®", "color": "rgba(16, 185, 129, 0.7)"}
            }
            
            for severity, count in severity_dist.items():
                info = severity_info.get(severity, {"label": severity, "color": "rgba(107, 114, 128, 0.7)"})
                severity_labels.append(info["label"])
                severity_data.append(count)
                severity_colors.append(info["color"])
            
            charts_data["severity_distribution"] = {
                "chart_type": "polarArea",
                "chart_data": {
                    "labels": severity_labels,
                    "datasets": [{
                        "label": "åš´é‡åº¦åˆ†å¸ƒ",
                        "data": severity_data,
                        "backgroundColor": severity_colors
                    }]
                },
                "chart_options": {
                    "responsive": True,
                    "plugins": {
                        "title": {
                            "display": True,
                            "text": "è­¦å‘Šåš´é‡åº¦åˆ†å¸ƒ"
                        }
                    }
                }
            }
        
        # 3. é¡åˆ¥çµ±è¨ˆåœ– (æŸ±ç‹€åœ–ç‰ˆæœ¬)
        category_dist = patterns.get('category_distribution', {})
        if category_dist:
            category_labels = []
            category_data = []
            category_colors = []
            
            category_info = {
                "rainfall": {"label": "é›¨é‡", "color": "#3B82F6"},
                "wind_storm": {"label": "é¢¨æš´", "color": "#EF4444"},
                "thunderstorm": {"label": "é›·æš´", "color": "#F59E0B"},
                "visibility": {"label": "èƒ½è¦‹åº¦", "color": "#8B5CF6"},
                "air_quality": {"label": "ç©ºæ°£", "color": "#10B981"},
                "temperature": {"label": "æº«åº¦", "color": "#F97316"}
            }
            
            # æŒ‰æ•¸é‡æ’åº
            sorted_categories = sorted(category_dist.items(), key=lambda x: x[1], reverse=True)
            
            for category, count in sorted_categories:
                info = category_info.get(category, {"label": category, "color": "#6B7280"})
                category_labels.append(info["label"])
                category_data.append(count)
                category_colors.append(info["color"])
            
            charts_data["warning_trends"] = {
                "chart_type": "bar",
                "chart_data": {
                    "labels": category_labels,
                    "datasets": [{
                        "label": "è­¦å‘Šæ•¸é‡",
                        "data": category_data,
                        "backgroundColor": category_colors,
                        "borderColor": category_colors,
                        "borderWidth": 2
                    }]
                },
                "chart_options": {
                    "responsive": True,
                    "plugins": {
                        "title": {
                            "display": True,
                            "text": "è­¦å‘Šé¡åˆ¥çµ±è¨ˆ"
                        }
                    },
                    "scales": {
                        "y": {
                            "beginAtZero": True,
                            "title": {
                                "display": True,
                                "text": "è­¦å‘Šæ•¸é‡"
                            }
                        }
                    }
                }
            }
        
        return jsonify({
            "status": "success",
            "data_source": "actual_data",
            "charts": charts_data,
            "summary": {
                "total_charts": len(charts_data),
                "data_period": f"{days_back}å¤©",
                "total_warnings": patterns.get('total_warnings', 0)
            },
            "generated_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"ç¸½è¦½åœ–è¡¨ç”Ÿæˆå¤±æ•—: {str(e)}"
        })

@app.route("/api/warnings/history", methods=["GET"])
def get_warning_history():
    """ç²å–è­¦å‘Šæ­·å²æ•¸æ“šåˆ†æ - ä½¿ç”¨çœŸå¯¦æ•¸æ“šåº«çµ±è¨ˆ"""
    global warning_analyzer
    
    try:
        days_back = int(request.args.get('days', 30))
        days_back = min(max(days_back, 1), 365)  # é™åˆ¶åœ¨1-365å¤©ä¹‹é–“
        
        # å¾æ•¸æ“šåº«æŸ¥è©¢çœŸå¯¦çµ±è¨ˆæ•¸æ“š
        conn = sqlite3.connect('warning_history.db')
        cursor = conn.cursor()
        
        # è¨ˆç®—æ™‚é–“ç¯„åœ
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # 1. ç¸½è­¦å‘Šæ•¸
        cursor.execute('''
            SELECT COUNT(*) FROM warning_records 
            WHERE timestamp >= ? AND timestamp <= ?
        ''', (start_date.isoformat(), end_date.isoformat()))
        total_warnings = cursor.fetchone()[0]
        
        # 2. é¡åˆ¥åˆ†å¸ƒ
        cursor.execute('''
            SELECT category, COUNT(*) as count 
            FROM warning_records 
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY category 
            ORDER BY count DESC
        ''', (start_date.isoformat(), end_date.isoformat()))
        category_data = cursor.fetchall()
        
        categories = {}
        best_category = "ç„¡æ•¸æ“š"
        if category_data:
            best_category = category_data[0][0] if category_data[0][0] else "æœªåˆ†é¡"
            for cat, count in category_data:
                cat_name = cat if cat else "æœªåˆ†é¡"
                # è¨ˆç®—è©²é¡åˆ¥çš„å¹³å‡å½±éŸ¿åˆ†æ•¸ä½œç‚ºæº–ç¢ºç‡åƒè€ƒ
                cursor.execute('''
                    SELECT AVG(impact_score) FROM warning_records 
                    WHERE category = ? AND timestamp >= ? AND timestamp <= ?
                ''', (cat, start_date.isoformat(), end_date.isoformat()))
                avg_impact = cursor.fetchone()[0] or 0
                # å°‡å½±éŸ¿åˆ†æ•¸è½‰æ›ç‚ºæº–ç¢ºç‡æŒ‡æ¨™ (0-100)
                accuracy = min(100, max(0, avg_impact * 2.5))
                
                categories[cat_name] = {
                    "count": count,
                    "accuracy": round(accuracy, 1)
                }
        
        # 3. æ¯æœˆåˆ†å¸ƒ
        cursor.execute('''
            SELECT strftime('%m', timestamp) as month, COUNT(*) 
            FROM warning_records 
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY month 
            ORDER BY month
        ''', (start_date.isoformat(), end_date.isoformat()))
        monthly_data = cursor.fetchall()
        
        monthly_labels = [f"{int(m)}æœˆ" for m, _ in monthly_data] if monthly_data else []
        monthly_counts = [c for _, c in monthly_data] if monthly_data else []
        
        # 4. æ™‚æ®µåˆ†å¸ƒ
        cursor.execute('''
            SELECT CAST(strftime('%H', timestamp) AS INTEGER) as hour, COUNT(*) 
            FROM warning_records 
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY hour 
            ORDER BY hour
        ''', (start_date.isoformat(), end_date.isoformat()))
        hourly_data = cursor.fetchall()
        hourly_dict = {h: c for h, c in hourly_data}
        
        # æ‰¾å‡ºé«˜å³°å’Œä½è°·æ™‚æ®µ
        if hourly_dict:
            sorted_hours = sorted(hourly_dict.items(), key=lambda x: x[1], reverse=True)
            peak_hours = [h for h, _ in sorted_hours[:4]]
            low_hours = [h for h, _ in sorted_hours[-4:]]
        else:
            peak_hours = []
            low_hours = []
        
        # 5. è¨ˆç®—å¹³å‡æº–ç¢ºç‡ (åŸºæ–¼é æ¸¬è¨˜éŒ„)
        cursor.execute('''
            SELECT AVG(impact_score) FROM warning_records 
            WHERE timestamp >= ? AND timestamp <= ?
        ''', (start_date.isoformat(), end_date.isoformat()))
        avg_impact_result = cursor.fetchone()[0]
        average_accuracy = round(min(100, max(0, (avg_impact_result or 0) * 2.5)), 1)
        
        # 6. ç”Ÿæˆæ´å¯Ÿ
        insights = []
        if category_data and len(category_data) > 0:
            top_cat = category_data[0][0] or "æœªåˆ†é¡"
            top_count = category_data[0][1]
            insights.append(f"{top_cat} æ•¸é‡æœ€å¤š ({top_count}æ¬¡)")
        
        if peak_hours:
            peak_str = ', '.join([f"{h}æ™‚" for h in peak_hours[:2]])
            insights.append(f"{peak_str} æ˜¯è­¦å‘Šé«˜å³°æœŸ")
        
        if total_warnings > 0:
            insights.append(f"éå»{days_back}å¤©å…±ç™¼å‡º{total_warnings}æ¬¡è­¦å‘Š")
        else:
            insights.append(f"éå»{days_back}å¤©ç„¡è­¦å‘Šè¨˜éŒ„")
        
        conn.close()
        
        # æ§‹å»ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
        return jsonify({
            "status": "success",
            "data_source": "real_database",
            "total_warnings": total_warnings,
            "average_accuracy": average_accuracy,
            "best_category": best_category,
            "warning_patterns": {
                "categories": categories,
                "monthly_distribution": {
                    "labels": monthly_labels,
                    "data": monthly_counts
                },
                "hourly_patterns": {
                    "peak_hours": peak_hours,
                    "low_hours": low_hours
                }
            },
            "insights": insights,
            "analysis_period": f"{days_back}å¤©",
            "generated_at": datetime.now().isoformat(),
            "message": "åŸºæ–¼çœŸå¯¦æ­·å²æ•¸æ“šçš„çµ±è¨ˆåˆ†æ"
        })
        
    except Exception as e:
        print(f"âŒ è­¦å‘Šæ­·å²åˆ†æéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "message": f"åˆ†æå¤±æ•—: {str(e)}",
            "total_warnings": 0,
            "average_accuracy": 0,
            "best_category": "éŒ¯èª¤"
        })

@app.route("/api/warnings/timeline", methods=["GET"])
def get_warning_timeline():
    """ç²å–è­¦å‘Šæ™‚é–“è»¸åœ–è¡¨æ•¸æ“š - ä½¿ç”¨çœŸå¯¦æ•¸æ“š"""
    try:
        days_back = int(request.args.get('days', 30))
        days_back = min(max(days_back, 1), 365)
        display_days = min(days_back, 30)  # æœ€å¤šé¡¯ç¤º30å¤©
        
        conn = sqlite3.connect('warning_history.db')
        cursor = conn.cursor()
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # æŸ¥è©¢æ¯æ—¥è­¦å‘Šæ•¸é‡
        cursor.execute('''
            SELECT DATE(timestamp) as date, COUNT(*) as count
            FROM warning_records 
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY date
            ORDER BY date
        ''', (start_date.isoformat(), end_date.isoformat()))
        
        daily_data = cursor.fetchall()
        conn.close()
        
        # æ§‹å»ºå®Œæ•´çš„æ—¥æœŸç¯„åœï¼ˆåŒ…å«ç„¡è­¦å‘Šçš„æ—¥æœŸï¼‰
        timeline_data = []
        labels = []
        date_dict = {date_str: count for date_str, count in daily_data}
        
        for i in range(display_days):
            date = end_date - timedelta(days=display_days - 1 - i)
            date_str = date.strftime('%Y-%m-%d')
            label = date.strftime('%m-%d')
            labels.append(label)
            timeline_data.append(date_dict.get(date_str, 0))
        
        return jsonify({
            "status": "success",
            "data_source": "real_database",
            "chart_type": "line",
            "chart_data": {
                "labels": labels,
                "datasets": [{
                    "label": "æ¯æ—¥è­¦å‘Šæ•¸é‡",
                    "data": timeline_data,
                    "borderColor": "#3B82F6",
                    "backgroundColor": "rgba(59, 130, 246, 0.1)",
                    "fill": True,
                    "tension": 0.3,
                    "pointBackgroundColor": "#3B82F6",
                    "pointBorderColor": "#ffffff",
                    "pointBorderWidth": 2,
                    "pointRadius": 4
                }]
            },
            "chart_options": {
                "responsive": True,
                "maintainAspectRatio": False,
                "scales": {
                    "y": {
                        "beginAtZero": True,
                        "title": {
                            "display": True,
                            "text": "è­¦å‘Šæ•¸é‡"
                        }
                    },
                    "x": {
                        "title": {
                            "display": True,
                            "text": "æ—¥æœŸ"
                        }
                    }
                },
                "plugins": {
                    "title": {
                        "display": True,
                        "text": f"éå» {display_days} å¤©è­¦å‘Šæ™‚é–“è»¸"
                    },
                    "legend": {
                        "display": True,
                        "position": "top"
                    }
                }
            },
            "total_warnings": sum(timeline_data),
            "period": f"{display_days}å¤©",
            "generated_at": datetime.now().isoformat()
        })
    except Exception as e:
        print(f"âŒ è­¦å‘Šæ™‚é–“è»¸éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "message": f"ç„¡æ³•ç”Ÿæˆæ™‚é–“è»¸: {str(e)}",
            "chart_data": {
                "labels": [],
                "datasets": []
            }
        })

@app.route("/api/warnings/category-simple", methods=["GET"])
def get_warning_category_simple():
    """ç²å–è­¦å‘Šé¡åˆ¥åˆ†å¸ƒç°¡åŒ–æ•¸æ“š - ä½¿ç”¨çœŸå¯¦æ•¸æ“š"""
    try:
        days_back = int(request.args.get('days', 30))
        days_back = min(max(days_back, 1), 365)
        
        conn = sqlite3.connect('warning_history.db')
        cursor = conn.cursor()
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # æŸ¥è©¢é¡åˆ¥åˆ†å¸ƒ
        cursor.execute('''
            SELECT category, COUNT(*) as count
            FROM warning_records 
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY category
            ORDER BY count DESC
            LIMIT 10
        ''', (start_date.isoformat(), end_date.isoformat()))
        
        category_data = cursor.fetchall()
        conn.close()
        
        labels = [cat if cat else "æœªåˆ†é¡" for cat, _ in category_data]
        data = [count for _, count in category_data]
        
        # ä¸­æ–‡åŒ–é¡åˆ¥åç¨±
        label_map = {
            "thunderstorm": "é›·æš´",
            "rainfall": "æš´é›¨",
            "wind_storm": "å¤§é¢¨",
            "temperature": "æ¥µç«¯æº«åº¦",
            "visibility": "èƒ½è¦‹åº¦",
            "marine": "æµ·äº‹",
            "air_quality": "ç©ºæ°£è³ªé‡"
        }
        labels = [label_map.get(l, l) for l in labels]
        
        return jsonify({
            "status": "success",
            "data_source": "real_database",
            "chart_data": {
                "labels": labels,
                "data": data
            },
            "total": sum(data),
            "period": f"{days_back}å¤©"
        })
    except Exception as e:
        print(f"âŒ è­¦å‘Šé¡åˆ¥çµ±è¨ˆéŒ¯èª¤: {e}")
        return jsonify({
            "status": "error",
            "message": str(e),
            "chart_data": {"labels": [], "data": []}
        })

@app.route("/api/warnings/category-distribution", methods=["GET"])
def get_warning_category_distribution():
    """ç²å–è­¦å‘Šé¡åˆ¥åˆ†å¸ƒåœ–è¡¨æ•¸æ“š"""
    global warning_analyzer
    
    if not warning_analysis_available or not warning_analyzer:
        # è¿”å›æœ‰æ„ç¾©çš„ç¤ºä¾‹æ•¸æ“š
        return jsonify({
            "status": "success",
            "data_source": "demo_data",
            "chart_data": {
                "labels": ["é›·æš´è­¦å‘Š", "æš´é›¨è­¦å‘Š", "å¤§é¢¨è­¦å‘Š", "é…·ç†±è­¦å‘Š", "å¯’å†·è­¦å‘Š"],
                "datasets": [{
                    "label": "è­¦å‘Šæ•¸é‡",
                    "data": [8, 6, 5, 3, 2],
                    "backgroundColor": [
                        "#F59E0B",  # æ©™è‰² - é›·æš´
                        "#3B82F6",  # è—è‰² - æš´é›¨ 
                        "#EF4444",  # ç´…è‰² - å¤§é¢¨
                        "#F97316",  # æ©˜ç´… - é…·ç†±
                        "#06B6D4"   # é’è‰² - å¯’å†·
                    ],
                    "borderColor": [
                        "#D97706",
                        "#2563EB", 
                        "#DC2626",
                        "#EA580C",
                        "#0891B2"
                    ],
                    "borderWidth": 2
                }]
            },
            "chart_options": {
                "responsive": True,
                "plugins": {
                    "title": {
                        "display": True,
                        "text": "è­¦å‘Šé¡åˆ¥åˆ†å¸ƒçµ±è¨ˆ"
                    },
                    "legend": {
                        "position": "bottom",
                        "labels": {
                            "padding": 20,
                            "usePointStyle": True
                        }
                    }
                }
            },
            "summary": {
                "total_categories": 5,
                "most_common": "é›·æš´è­¦å‘Š",
                "total_warnings": 24
            },
            "message": "ä½¿ç”¨ç¤ºä¾‹æ•¸æ“šå±•ç¤º"
        })
    
    try:
        days_back = int(request.args.get('days', 30))
        days_back = min(max(days_back, 1), 365)  # é™åˆ¶åœ¨1-365å¤©ä¹‹é–“
        
        # ç²å–è­¦å‘Šæ¨¡å¼æ•¸æ“š
        patterns = warning_analyzer.analyze_warning_patterns(days_back)
        category_dist = patterns.get('category_distribution', {})
        
        # å¦‚æœæ²’æœ‰æ•¸æ“šï¼Œè¿”å›ç¤ºä¾‹æ•¸æ“š
        if not category_dist or patterns.get('total_warnings', 0) == 0:
            category_dist = {
                "rainfall": 8,
                "wind_storm": 6,
                "thunderstorm": 4,
                "visibility": 3,
                "air_quality": 2,
                "temperature": 1
            }
        
        # æº–å‚™åœ–è¡¨æ•¸æ“š
        labels = []
        data = []
        colors = []
        
        # è­¦å‘Šé¡åˆ¥ä¸­æ–‡æ¨™ç±¤å’Œé¡è‰²
        category_info = {
            "rainfall": {"label": "é›¨é‡è­¦å‘Š", "color": "#3B82F6"},
            "wind_storm": {"label": "é¢¨æš´è­¦å‘Š", "color": "#EF4444"},
            "thunderstorm": {"label": "é›·æš´è­¦å‘Š", "color": "#F59E0B"},
            "visibility": {"label": "èƒ½è¦‹åº¦è­¦å‘Š", "color": "#8B5CF6"},
            "air_quality": {"label": "ç©ºæ°£å“è³ªè­¦å‘Š", "color": "#10B981"},
            "temperature": {"label": "æº«åº¦è­¦å‘Š", "color": "#F97316"},
            "marine": {"label": "æµ·äº‹è­¦å‘Š", "color": "#06B6D4"},
            "unknown": {"label": "å…¶ä»–è­¦å‘Š", "color": "#6B7280"}
        }
        
        # æŒ‰æ•¸é‡æ’åº
        sorted_categories = sorted(category_dist.items(), key=lambda x: x[1], reverse=True)
        
        for category, count in sorted_categories:
            info = category_info.get(category, {"label": category, "color": "#6B7280"})
            labels.append(info["label"])
            data.append(count)
            colors.append(info["color"])
        
        # è¨ˆç®—ç™¾åˆ†æ¯”
        total = sum(data)
        percentages = [round((count / total * 100), 1) if total > 0 else 0 for count in data]
        
        return jsonify({
            "status": "success",
            "chart_type": "doughnut",
            "chart_data": {
                "labels": labels,
                "datasets": [{
                    "label": "è­¦å‘Šæ•¸é‡",
                    "data": data,
                    "backgroundColor": colors,
                    "borderColor": colors,
                    "borderWidth": 2,
                    "hoverOffset": 4
                }]
            },
            "chart_options": {
                "responsive": True,
                "plugins": {
                    "title": {
                        "display": True,
                        "text": f"éå» {days_back} å¤©è­¦å‘Šé¡åˆ¥åˆ†å¸ƒ"
                    },
                    "legend": {
                        "position": "bottom",
                        "labels": {
                            "padding": 20,
                            "usePointStyle": True
                        }
                    },
                    "tooltip": {
                        "callbacks": {
                            "label": "function(context) { return context.label + ': ' + context.parsed + ' æ¬¡ (' + (context.parsed / " + str(total) + " * 100).toFixed(1) + '%)'; }"
                        }
                    }
                },
                "cutout": "50%"
            },
            "summary": {
                "total_warnings": total,
                "most_common": labels[0] if labels else "ç„¡æ•¸æ“š",
                "categories_count": len(labels),
                "percentages": dict(zip(labels, percentages))
            },
            "period": f"{days_back}å¤©",
            "data_source": "example_data" if patterns.get('total_warnings', 0) == 0 else "actual_data",
            "generated_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"é¡åˆ¥åˆ†å¸ƒåœ–è¡¨ç”Ÿæˆå¤±æ•—: {str(e)}"
        })

# ç°¡åŒ–ç‰ˆ API ç«¯é»ï¼ˆç‚º index.html å‰ç«¯æä¾›ï¼‰
@app.route("/api/warnings/timeline-simple", methods=["GET"])
def get_warning_timeline_simple():
    """ç²å–ç°¡åŒ–çš„è­¦å‘Šæ™‚é–“è»¸æ•¸æ“šï¼ˆé©ç”¨æ–¼ index.htmlï¼‰"""
    global warning_analyzer
    
    try:
        days_back = int(request.args.get('days', 7))  # é è¨­7å¤©
        days_back = min(max(days_back, 1), 30)  # é™åˆ¶åœ¨1-30å¤©ä¹‹é–“
        
        # ç”Ÿæˆæ™‚é–“è»¸æ•¸æ“š
        from datetime import datetime, timedelta
        end_date = datetime.now()
        labels = []
        data = []
        
        for i in range(days_back):
            date = end_date - timedelta(days=i)
            date_str = date.strftime('%m/%d')
            labels.insert(0, date_str)
            
            # æ¨¡æ“¬æ•¸æ“š - åŸºæ–¼å¯¦éš›è­¦å‘Šæ•¸æ“šæˆ–ç¤ºä¾‹æ•¸æ“š
            if warning_analysis_available and warning_analyzer:
                patterns = warning_analyzer.analyze_warning_patterns(days_back)
                daily_avg = patterns.get('total_warnings', 0) / days_back
                warning_count = max(0, round(daily_avg * (0.5 + 1.0 * (i % 3) / 3)))
            else:
                # ç¤ºä¾‹æ•¸æ“š
                warning_count = max(0, 3 - abs(i - days_back//2))
            
            data.insert(0, warning_count)
        
        return jsonify({
            "labels": labels,
            "data": data
        })
        
    except Exception as e:
        # è¿”å›ç¤ºä¾‹æ•¸æ“š
        return jsonify({
            "labels": ["07/15", "07/16", "07/17", "07/18", "07/19", "07/20", "07/21"],
            "data": [2, 5, 3, 8, 4, 6, 3]
        })

@app.route("/api/warnings/seasonal", methods=["GET"])
def get_seasonal_analysis():
    """ç²å–å­£ç¯€æ€§è­¦å‘Šåˆ†æ - ä½¿ç”¨çœŸå¯¦æ•¸æ“š"""
    try:
        conn = sqlite3.connect('warning_history.db')
        cursor = conn.cursor()
        
        # æŒ‰å­£ç¯€çµ±è¨ˆè­¦å‘Šæ•¸æ“š
        cursor.execute('''
            SELECT season, category, COUNT(*) as count, AVG(impact_score) as avg_impact
            FROM warning_records 
            WHERE season IS NOT NULL
            GROUP BY season, category
            ORDER BY season, count DESC
        ''')
        
        season_data = cursor.fetchall()
        conn.close()
        
        # çµ„ç¹”å­£ç¯€æ•¸æ“š
        seasonal_breakdown = {
            "winter": {"total_warnings": 0, "categories": {}},
            "spring": {"total_warnings": 0, "categories": {}},
            "summer": {"total_warnings": 0, "categories": {}},
            "autumn": {"total_warnings": 0, "categories": {}}
        }
        
        season_map = {
            "winter": "å†¬å­£",
            "spring": "æ˜¥å­£", 
            "summer": "å¤å­£",
            "autumn": "ç§‹å­£"
        }
        
        for season, category, count, avg_impact in season_data:
            if season in seasonal_breakdown:
                seasonal_breakdown[season]["total_warnings"] += count
                seasonal_breakdown[season]["categories"][category] = {
                    "count": count,
                    "avg_impact": round(avg_impact, 2) if avg_impact else 0
                }
        
        # æ‰¾å‡ºæœ€æ´»èºå’Œæœ€æº–ç¢ºçš„å­£ç¯€
        season_totals = {s: data["total_warnings"] for s, data in seasonal_breakdown.items()}
        peak_season = max(season_totals, key=season_totals.get) if season_totals else "summer"
        
        # è½‰æ›ç‚ºä¸­æ–‡
        result_data = {}
        for eng_season, chi_season in season_map.items():
            data = seasonal_breakdown[eng_season]
            result_data[chi_season] = {
                "total_warnings": data["total_warnings"],
                "most_common_categories": dict(list(data["categories"].items())[:3]),
                "average_accuracy": round(sum(c["avg_impact"] for c in data["categories"].values()) / len(data["categories"]) * 2.5, 1) if data["categories"] else 0
            }
        
        return jsonify({
            "status": "success",
            "data_source": "real_database",
            "data": {
                "seasonal_breakdown": result_data,
                "annual_trends": {
                    "peak_season": season_map.get(peak_season, "å¤å­£"),
                    "total_annual_warnings": sum(season_totals.values())
                }
            },
            "message": "åŸºæ–¼çœŸå¯¦æ­·å²æ•¸æ“šçš„å­£ç¯€åˆ†æ",
            "generated_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"âŒ å­£ç¯€åˆ†æéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "message": f"å­£ç¯€åˆ†æå¤±æ•—: {str(e)}"
        })

@app.route("/api/warnings/insights", methods=["GET"])
def get_warning_insights():
    """ç²å–è­¦å‘Šæ•¸æ“šæ´å¯Ÿå’Œå»ºè­°"""
    global warning_analyzer
    
    if not warning_analysis_available or not warning_analyzer:
        # è¿”å›æœ‰æ„ç¾©çš„ç¤ºä¾‹æ´å¯Ÿ
        return jsonify({
            "status": "success",
            "data_source": "demo_insights",
            "insights": {
                "key_findings": [
                    "é›·æš´è­¦å‘Šåœ¨å¤å­£æœˆä»½ (6-8æœˆ) ç™¼å‡ºé »ç‡æœ€é«˜",
                    "ä¸‹åˆ2-5é»æ˜¯è­¦å‘Šç™¼å‡ºçš„é«˜å³°æ™‚æ®µ",
                    "å¤§é¢¨è­¦å‘Šé€šå¸¸èˆ‡é¢±é¢¨å­£ç¯€ç›¸é—œ",
                    "é…·ç†±è­¦å‘Šæº–ç¢ºç‡é”95%ä»¥ä¸Š"
                ],
                "accuracy_analysis": {
                    "overall_accuracy": 87.3,
                    "best_performing": "å¯’å†·è­¦å‘Š (95.0%)",
                    "needs_improvement": "é…·ç†±è­¦å‘Š (78.9%)",
                    "trend": "improving"
                },
                "temporal_patterns": {
                    "peak_season": "å¤å­£ (6-8æœˆ)",
                    "peak_time": "ä¸‹åˆ2-5é»",
                    "lowest_activity": "å‡Œæ™¨2-5é»"
                },
                "recommendations": [
                    "åŠ å¼·ä¸‹åˆæ™‚æ®µçš„ç›£æ¸¬èƒ½åŠ›",
                    "å„ªåŒ–é…·ç†±è­¦å‘Šçš„é æ¸¬æ¨¡å‹",
                    "è€ƒæ…®å­£ç¯€æ€§èª¿æ•´è­¦å‘Šé–¾å€¼",
                    "æé«˜å¤œé–“è­¦å‘Šçš„éŸ¿æ‡‰é€Ÿåº¦"
                ],
                "data_quality": {
                    "completeness": 89,
                    "consistency": 92,
                    "timeliness": 88,
                    "note": "åŸºæ–¼ç¤ºä¾‹æ•¸æ“šè¨ˆç®—"
                }
            },
            "generated_at": datetime.now().isoformat(),
            "message": "é€™æ˜¯ç¤ºä¾‹åˆ†æ - å¯¦éš›éƒ¨ç½²éœ€è¦çœŸå¯¦æ­·å²æ•¸æ“š"
        })
    
    try:
        insights = warning_analyzer.generate_warning_insights()
        
        # ä½¿ç”¨ convert_numpy_types ä¿®å¾© JSON åºåˆ—åŒ–å•é¡Œ
        converted_data = convert_numpy_types(insights)
        
        return jsonify({
            "status": "success",
            "data": converted_data,
            "generated_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"æ´å¯Ÿåˆ†æå¤±æ•—: {str(e)}"
        })

@app.route("/api/warnings/accuracy", methods=["GET"])
def get_prediction_accuracy():
    """ç²å–é æ¸¬æº–ç¢ºæ€§è©•ä¼° - ä½¿ç”¨çœŸå¯¦æ•¸æ“š"""
    try:
        days_back = int(request.args.get('days', 7))
        days_back = min(max(days_back, 1), 30)
        
        conn = sqlite3.connect('warning_history.db')
        cursor = conn.cursor()
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # æŸ¥è©¢é æ¸¬è¨˜éŒ„çµ±è¨ˆ
        cursor.execute('''
            SELECT 
                COUNT(*) as total,
                AVG(warning_impact) as avg_impact,
                AVG(warning_risk_impact) as avg_risk,
                AVG(final_score) as avg_score
            FROM prediction_records 
            WHERE timestamp >= ? AND timestamp <= ?
        ''', (start_date.isoformat(), end_date.isoformat()))
        
        stats = cursor.fetchone()
        total_predictions = stats[0] if stats else 0
        avg_impact = stats[1] if stats and stats[1] else 0
        avg_risk = stats[2] if stats and stats[2] else 0
        avg_score = stats[3] if stats and stats[3] else 0
        
        # æŸ¥è©¢æœ‰è­¦å‘Šå½±éŸ¿çš„é æ¸¬æ•¸é‡
        cursor.execute('''
            SELECT COUNT(*) FROM prediction_records 
            WHERE timestamp >= ? AND timestamp <= ?
            AND warning_impact > 0
        ''', (start_date.isoformat(), end_date.isoformat()))
        
        predictions_with_warnings = cursor.fetchone()[0]
        
        # æŒ‰é æ¸¬é¡å‹çµ±è¨ˆ
        cursor.execute('''
            SELECT 
                prediction_type,
                COUNT(*) as count,
                AVG(warning_impact) as avg_impact,
                AVG(final_score) as avg_score
            FROM prediction_records 
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY prediction_type
        ''', (start_date.isoformat(), end_date.isoformat()))
        
        type_data = cursor.fetchall()
        conn.close()
        
        by_type = {}
        for pred_type, count, impact, score in type_data:
            by_type[pred_type] = {
                "count": count,
                "avg_warning_impact": round(impact, 2) if impact else 0,
                "avg_score": round(score, 2) if score else 0
            }
        
        return jsonify({
            "status": "success",
            "data_source": "real_database",
            "evaluation_period": f"{days_back}å¤©",
            "data": {
                "total_predictions": total_predictions,
                "predictions_with_warnings": predictions_with_warnings,
                "average_warning_impact": round(avg_impact, 2),
                "average_risk_impact": round(avg_risk, 2),
                "average_final_score": round(avg_score, 2),
                "by_prediction_type": by_type
            },
            "generated_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"âŒ æº–ç¢ºæ€§è©•ä¼°éŒ¯èª¤: {e}")
        return jsonify({
            "status": "error",
            "message": f"æº–ç¢ºæ€§è©•ä¼°å¤±æ•—: {str(e)}"
        })

@app.route("/api/warnings/record", methods=["POST"])
def record_warning_manually():
    """æ‰‹å‹•è¨˜éŒ„è­¦å‘Šï¼ˆæ¸¬è©¦ç”¨ï¼‰"""
    global warning_analyzer
    
    if not warning_analysis_available or not warning_analyzer:
        return jsonify({
            "status": "error",
            "message": "è­¦å‘Šåˆ†æç³»çµ±æœªå¯ç”¨"
        })
    
    try:
        data = request.get_json()
        warning_text = data.get('warning_text', '')
        
        if not warning_text:
            return jsonify({
                "status": "error",
                "message": "è­¦å‘Šæ–‡æœ¬ä¸èƒ½ç‚ºç©º"
            })
        
        # è¨˜éŒ„è­¦å‘Š
        warning_id = warning_analyzer.record_warning({
            "warning_text": warning_text,
            "source": "manual_input",
            "user_submitted": True
        })
        
        return jsonify({
            "status": "success",
            "message": "è­¦å‘Šå·²è¨˜éŒ„",
            "warning_id": warning_id,
            "warning_text": warning_text,
            "recorded_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"è¨˜éŒ„è­¦å‘Šå¤±æ•—: {str(e)}"
        })

@app.route("/api/warnings/export", methods=["GET"])
def export_warning_analysis():
    """å°å‡ºè­¦å‘Šåˆ†æå ±å‘Š"""
    global warning_analyzer
    
    if not warning_analysis_available or not warning_analyzer:
        return jsonify({
            "status": "error",
            "message": "è­¦å‘Šåˆ†æç³»çµ±æœªå¯ç”¨"
        })
    
    try:
        # ç”Ÿæˆå ±å‘Š
        report_file = warning_analyzer.export_analysis_report()
        
        return jsonify({
            "status": "success",
            "message": "åˆ†æå ±å‘Šå·²ç”Ÿæˆ",
            "report_file": report_file,
            "download_url": f"/static/reports/{report_file}",  # å‡è¨­å ±å‘Šä¿å­˜åœ¨static/reportsç›®éŒ„
            "generated_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"å ±å‘Šç”Ÿæˆå¤±æ•—: {str(e)}"
        })

@app.route("/api/warnings/collector/status", methods=["GET"])
def get_collector_status():
    """ç²å–è­¦å‘Šæ”¶é›†å™¨ç‹€æ…‹"""
    global warning_collector
    
    if not warning_analysis_available or not warning_collector:
        return jsonify({
            "status": "error",
            "message": "è­¦å‘Šæ”¶é›†ç³»çµ±æœªå¯ç”¨"
        })
    
    try:
        status = warning_collector.get_collection_status()
        
        return jsonify({
            "status": "success",
            "data": status,
            "checked_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {str(e)}"
        })

@app.route('/api/ml-training/status', methods=['GET'])
def ml_training_status():
    """ç²å–MLè¨“ç·´ç‹€æ…‹"""
    try:
        stats = get_ml_training_stats()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¾…è™•ç†çš„é‡æ–°è¨“ç·´ä»»å‹™
        retrain_pending = False
        try:
            with open('ml_retrain_queue.json', 'r') as f:
                retrain_pending = True
        except FileNotFoundError:
            pass
        
        return jsonify({
            "status": "success",
            "ml_training": {
                "total_cases": stats['total_cases'],
                "pending_cases": stats['pending_cases'],
                "avg_data_quality": stats['avg_quality'],
                "retrain_threshold": 10,
                "retrain_pending": retrain_pending,
                "next_retrain_in": max(0, 10 - stats['pending_cases']),
                "model_version": "v1.0",
                "last_trained": "åŸºç¤æ¨¡å‹",
                "training_effectiveness": "å¾…è©•ä¼°"
            },
            "data_collection": {
                "collection_rate": "ç”¨æˆ¶ä¸Šå‚³",
                "quality_distribution": get_quality_distribution(),
                "coverage": get_data_coverage_analysis()
            }
        })
    
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

def get_quality_distribution():
    """ç²å–æ•¸æ“šè³ªé‡åˆ†å¸ƒ"""
    try:
        conn = sqlite3.connect('ml_training_data.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT 
                CASE 
                    WHEN visual_rating >= 8 THEN 'excellent'
                    WHEN visual_rating >= 6 THEN 'good'
                    WHEN visual_rating >= 4 THEN 'moderate'
                    ELSE 'poor'
                END as quality_level,
                COUNT(*) as count
            FROM ml_training_cases
            GROUP BY quality_level
        ''')
        
        results = cursor.fetchall()
        conn.close()
        
        return {row[0]: row[1] for row in results}
    except:
        return {"excellent": 0, "good": 0, "moderate": 0, "poor": 0}

def get_data_coverage_analysis():
    """åˆ†ææ•¸æ“šè¦†è“‹ç¯„åœ"""
    try:
        conn = sqlite3.connect('ml_training_data.db')
        cursor = conn.cursor()
        
        # æ™‚é–“è¦†è“‹
        cursor.execute('''
            SELECT COUNT(DISTINCT substr(time, 1, 2)) as unique_hours
            FROM ml_training_cases
        ''')
        hour_coverage = cursor.fetchone()[0]
        
        # åœ°é»è¦†è“‹
        cursor.execute('''
            SELECT COUNT(DISTINCT location) as unique_locations
            FROM ml_training_cases
        ''')
        location_coverage = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            "time_coverage": f"{hour_coverage}/24 å°æ™‚",
            "location_coverage": f"{location_coverage} å€‹ä¸åŒåœ°é»",
            "seasonal_coverage": "éœ€è¦æ›´å¤šå­£ç¯€æ•¸æ“š"
        }
    except:
        return {
            "time_coverage": "0/24 å°æ™‚",
            "location_coverage": "0 å€‹åœ°é»",
            "seasonal_coverage": "ç„¡æ•¸æ“š"
        }

# åˆå§‹åŒ–ç…§ç‰‡æ¡ˆä¾‹å­¸ç¿’ç³»çµ±
initialize_photo_cases()

# åˆå§‹åŒ–MLæ¡ˆä¾‹åˆ†æå™¨
try:
    case_analyzer = BurnskyCaseAnalyzer()
    case_analyzer.load_or_train_model()
    print("âœ… MLç‡’å¤©é æ¸¬ç³»çµ±å·²åˆå§‹åŒ–")
except Exception as e:
    case_analyzer = None
    print(f"âš ï¸ MLç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")

@app.route('/api/ml-analysis', methods=['POST'])
@limiter.limit("30 per hour")  # MLåˆ†ææ›´åš´æ ¼çš„é™åˆ¶
def ml_analysis():
    """ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’åˆ†æç‡’å¤©æ¢ä»¶"""
    if not case_analyzer:
        return jsonify({
            'status': 'error',
            'message': 'MLç³»çµ±æœªåˆå§‹åŒ–',
            'ml_enabled': False
        }), 503
    
    try:
        data = request.json
        conditions = {
            'cloud_coverage': data.get('cloud_coverage', 'é©ä¸­'),
            'visibility': data.get('visibility', 'ä¸€èˆ¬'),
            'humidity': data.get('humidity', 'ä¸­ç­‰'),
            'temperature': data.get('temperature', 'å¤å­£æº«åº¦'),
            'wind': data.get('wind', 'è¼•å¾®'),
            'air_quality': data.get('air_quality', 'ä¸€èˆ¬')
        }
        
        # ä½¿ç”¨MLåˆ†æå™¨é€²è¡Œåˆ†æ
        analysis = case_analyzer.analyze_conditions(conditions)
        
        return jsonify({
            'status': 'success',
            'analysis': analysis,
            'ml_enabled': True
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'ml_enabled': False
        }), 500

@app.route('/api/ml-feedback', methods=['POST'])
@limiter.limit("20 per hour")  # åé¥‹ç«¯é»é™åˆ¶
def submit_ml_feedback():
    """æ¥æ”¶ç”¨æˆ¶åé¥‹ä¾†æ”¹é€²MLæ¨¡å‹"""
    if not case_analyzer:
        return jsonify({
            'status': 'error',
            'message': 'MLç³»çµ±æœªåˆå§‹åŒ–'
        }), 503
    
    try:
        data = request.json
        conditions = data.get('conditions', {})
        actual_rating = float(data.get('rating', 0))
        
        if actual_rating < 1 or actual_rating > 10:
            return jsonify({
                'status': 'error',
                'message': 'è©•åˆ†å¿…é ˆåœ¨1-10ä¹‹é–“'
            }), 400
        
        # æ›´æ–°MLæ¨¡å‹
        feedback_result = case_analyzer.update_model_with_feedback(conditions, actual_rating)
        
        return jsonify({
            'status': 'success',
            'message': feedback_result
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/ml-status')
def ml_status():
    """ç²å–MLç³»çµ±ç‹€æ…‹"""
    if not case_analyzer:
        return jsonify({
            'status': 'error',
            'message': 'MLç³»çµ±æœªåˆå§‹åŒ–',
            'ml_enabled': False
        })
    
    try:
        # ç²å–æ¨¡å‹çµ±è¨ˆ
        stats = {
            'model_loaded': case_analyzer.ml_model is not None,
            'total_cases': len(case_analyzer.cases),
            'feature_importance': case_analyzer.get_feature_importance(),
            'training_data_size': len(case_analyzer.prepare_training_data()[0]) if case_analyzer.ml_model else 0
        }
        
        return jsonify({
            'status': 'success',
            'ml_stats': stats
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

# ==================== ç‡’å¤©æ­·å²çµ±è¨ˆ API ====================
@app.route("/api/burnsky/history", methods=["GET"])
def get_burnsky_history():
    """ç²å–ç‡’å¤©é æ¸¬æ­·å²çµ±è¨ˆ"""
    try:
        days_back = int(request.args.get('days', 30))
        days_back = min(max(days_back, 1), 365)
        
        conn = sqlite3.connect('prediction_history.db')
        cursor = conn.cursor()
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # è½‰æ›ç‚º SQLite æ ¼å¼ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')
        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')
        
        # 1. ç¸½é«”çµ±è¨ˆ
        cursor.execute('''
            SELECT 
                COUNT(*) as total_predictions,
                AVG(score) as avg_score,
                MAX(score) as max_score,
                MIN(score) as min_score,
                COUNT(CASE WHEN score >= 70 THEN 1 END) as high_score_count,
                COUNT(CASE WHEN score >= 50 AND score < 70 THEN 1 END) as medium_score_count,
                COUNT(CASE WHEN score < 50 THEN 1 END) as low_score_count
            FROM prediction_history
            WHERE timestamp >= ? AND timestamp <= ?
        ''', (start_date_str, end_date_str))
        
        overall = cursor.fetchone()
        
        # 2. æŒ‰é¡å‹çµ±è¨ˆï¼ˆæ—¥å‡º/æ—¥è½ï¼‰
        cursor.execute('''
            SELECT 
                prediction_type,
                COUNT(*) as count,
                AVG(score) as avg_score,
                MAX(score) as max_score,
                COUNT(CASE WHEN score >= 70 THEN 1 END) as high_score_count
            FROM prediction_history
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY prediction_type
        ''', (start_date_str, end_date_str))
        
        by_type = {}
        for row in cursor.fetchall():
            pred_type, count, avg, max_s, high = row
            by_type[pred_type] = {
                'count': count,
                'avg_score': round(avg, 1) if avg else 0,
                'max_score': max_s if max_s else 0,
                'high_score_count': high,
                'success_rate': round((high / count * 100) if count > 0 else 0, 1)
            }
        
        # 3. æ¯æ—¥è¶¨å‹¢ï¼ˆæœ€è¿‘30å¤©ï¼‰
        cursor.execute('''
            SELECT 
                DATE(timestamp) as date,
                AVG(score) as avg_score,
                MAX(score) as max_score,
                COUNT(CASE WHEN score >= 70 THEN 1 END) as high_score_count
            FROM prediction_history
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY DATE(timestamp)
            ORDER BY date DESC
            LIMIT 30
        ''', (start_date_str, end_date_str))
        
        daily_trends = []
        for row in cursor.fetchall():
            date, avg, max_s, high = row
            daily_trends.append({
                'date': date,
                'avg_score': round(avg, 1) if avg else 0,
                'max_score': max_s if max_s else 0,
                'high_score_count': high
            })
        
        # 4. æœ€ä½³æ™‚æ®µçµ±è¨ˆï¼ˆæŒ‰å°æ™‚ï¼‰
        cursor.execute('''
            SELECT 
                CAST(strftime('%H', timestamp) AS INTEGER) as hour,
                COUNT(*) as count,
                AVG(score) as avg_score,
                COUNT(CASE WHEN score >= 70 THEN 1 END) as high_score_count
            FROM prediction_history
            WHERE timestamp >= ? AND timestamp <= ?
            GROUP BY hour
            ORDER BY avg_score DESC
        ''', (start_date_str, end_date_str))
        
        best_hours = []
        for row in cursor.fetchall():
            hour, count, avg, high = row
            best_hours.append({
                'hour': hour,
                'count': count,
                'avg_score': round(avg, 1) if avg else 0,
                'high_score_count': high
            })
        
        conn.close()
        
        # çµ„ç¹”è¿”å›æ•¸æ“š
        return jsonify({
            'status': 'success',
            'data_source': 'prediction_history',
            'time_range': {
                'days': days_back,
                'start_date': start_date.strftime('%Y-%m-%d'),
                'end_date': end_date.strftime('%Y-%m-%d')
            },
            'summary': {
                'total_predictions': overall[0] or 0,
                'avg_score': round(overall[1], 1) if overall[1] else 0,
                'max_score': overall[2] if overall[2] else 0,
                'min_score': overall[3] if overall[3] else 0,
                'high_score_count': overall[4] or 0,
                'medium_score_count': overall[5] or 0,
                'low_score_count': overall[6] or 0,
                'success_rate': round((overall[4] / overall[0] * 100) if overall[0] else 0, 1)
            },
            'by_type': by_type,
            'daily_trends': daily_trends,
            'best_hours': best_hours[:5],  # å‰5å€‹æœ€ä½³æ™‚æ®µ
            'insights': generate_burnsky_insights(overall, by_type, best_hours)
        })
        
    except Exception as e:
        print(f"âŒ ç‡’å¤©æ­·å²çµ±è¨ˆéŒ¯èª¤: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

def generate_burnsky_insights(overall, by_type, best_hours):
    """ç”Ÿæˆç‡’å¤©æ­·å²æ´å¯Ÿ"""
    insights = []
    
    if overall[0] > 0:
        success_rate = (overall[4] / overall[0] * 100) if overall[0] else 0
        insights.append(f"éå»æœŸé–“å…±é€²è¡Œ {overall[0]} æ¬¡é æ¸¬ï¼Œé«˜åˆ†ï¼ˆâ‰¥70åˆ†ï¼‰å‡ºç¾ç‡ç‚º {success_rate:.1f}%")
        
        if overall[1]:
            insights.append(f"å¹³å‡ç‡’å¤©è©•åˆ†ç‚º {overall[1]:.1f} åˆ†")
        
        if overall[2] and overall[2] >= 80:
            insights.append(f"æœ€é«˜è©•åˆ†é”åˆ° {overall[2]:.0f} åˆ†ï¼Œå‡ºç¾æ¥µä½³ç‡’å¤©æ¢ä»¶")
    
    # æ—¥å‡ºæ—¥è½å°æ¯”
    if 'sunrise' in by_type and 'sunset' in by_type:
        sunrise_rate = by_type['sunrise']['success_rate']
        sunset_rate = by_type['sunset']['success_rate']
        if sunrise_rate > sunset_rate:
            insights.append(f"æ—¥å‡ºçš„ç‡’å¤©æˆåŠŸç‡ï¼ˆ{sunrise_rate}%ï¼‰é«˜æ–¼æ—¥è½ï¼ˆ{sunset_rate}%ï¼‰")
        else:
            insights.append(f"æ—¥è½çš„ç‡’å¤©æˆåŠŸç‡ï¼ˆ{sunset_rate}%ï¼‰é«˜æ–¼æ—¥å‡ºï¼ˆ{sunrise_rate}%ï¼‰")
    
    # æœ€ä½³æ™‚æ®µ
    if best_hours:
        best = best_hours[0]
        time_label = 'å‡Œæ™¨' if best['hour'] < 6 else 'æ—©æ™¨' if best['hour'] < 12 else 'ä¸‹åˆ' if best['hour'] < 18 else 'æ™šé–“'
        insights.append(f"{time_label}æ™‚æ®µï¼ˆ{best['hour']}:00ï¼‰çš„ç‡’å¤©è©•åˆ†æœ€é«˜ï¼Œå¹³å‡ {best['avg_score']} åˆ†")
    
    return insights

# å•Ÿå‹•æ¯å°æ™‚é æ¸¬ä¿å­˜æ’ç¨‹
start_hourly_scheduler()

if __name__ == '__main__':
    port = int(os.getenv('PORT', '5001'))
    host = os.getenv('HOST', '0.0.0.0')
    debug_mode = os.getenv('FLASK_DEBUG', os.getenv('FLASK_ENV', 'development')) == 'development'
    
    print(f"ğŸš€ å•Ÿå‹•æœå‹™å™¨: http://{host}:{port}")
    print(f"ğŸ”§ Debug æ¨¡å¼: {debug_mode}")
    print(f"ğŸ”’ é€Ÿç‡é™åˆ¶: {'å•Ÿç”¨' if rate_limit_enabled else 'ç¦ç”¨'}")
    
    app.run(host=host, port=port, debug=debug_mode)
